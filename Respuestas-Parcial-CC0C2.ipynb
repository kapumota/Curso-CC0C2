{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c965a90e-8e11-4287-993a-ada1a62e3f80",
   "metadata": {},
   "source": [
    "### **Respuestas del examen parcial CC0C2**\n",
    "\n",
    "#### **Q1 - Tokenización y OOV (2 pts)**\n",
    "\n",
    "**(a) Conceptos (0.8 pt).**\n",
    "La **tokenización word-level** asigna **un token por palabra completa**: si una palabra no está en el vocabulario, cae en **OOV** (fuera de vocabulario). En cambio, los métodos de **subpalabras** (por ejemplo, **BPE** o **WordPiece**) **descomponen** las palabras en piezas frecuentes (prefijos, sufijos o segmentos internos), de modo que palabras raras pueden representarse combinando piezas conocidas.\n",
    "\n",
    "**Cómo reducen OOV.** Si una palabra es desconocida pero sus **subpiezas** sí están en el vocabulario, se evita el token **UNK**: la palabra se expresa como la **secuencia de subpalabras**.\n",
    "\n",
    "**Beneficio y desventaja.** Beneficio: **mucho menos OOV** y mejor manejo de **morfología rica**. Desventaja: **secuencias más largas** $\\Rightarrow$ mayor costo de cómputo (más pasos de atención, mayor latencia).\n",
    "\n",
    "**(b) Ejercicio guiado (1.2 pts).**\n",
    "Vocabulario de subpalabras: ${\\textbf{öğret},\\ \\textbf{men},\\ \\textbf{ler},\\ \\textbf{imiz},\\ \\textbf{den}}\\ +\\ \\textbf{UNK}$.\n",
    "Palabra turca: **öğretmenlerimizden**.\n",
    "Estrategia: avanzar por **máximo emparejamiento** (*greedy* de izquierda a derecha).\n",
    "\n",
    "1. **öğret** $\\mid$ *menlerimizden*  (coincide \"öğret\")\n",
    "2. öğret **men** $\\mid$ *lerimizden*\n",
    "3. öğret men **ler** $\\mid$ *imizden*\n",
    "4. öğret men ler **imiz** $\\mid$ *den*\n",
    "5. öğret men ler imiz **den** $\\mid$ *∅*\n",
    "\n",
    "* **Lista de subpalabras usadas:** $[\\text{öğret},\\ \\text{men},\\ \\text{ler},\\ \\text{imiz},\\ \\text{den}]$\n",
    "* **Número total de tokens:** $5$\n",
    "* **¿Hubo UNK?** No.\n",
    "* **Latencia (frase breve):** **Más tokens $\\Rightarrow$ más tiempo por secuencia**, pues crece la longitud efectiva que atraviesa el codificador/atención.\n",
    "\n",
    "\n",
    "#### **Q2 - Entrenamiento estable y desbalance (2 pts)**\n",
    "\n",
    "**(a) Clipping sencillo (0.8 pt).**\n",
    "\n",
    "Dado $g=[4,,3]$ y umbral $\\tau=5$, la **norma L2** es:\n",
    "$$\n",
    "\\lVert g\\rVert_2=\\sqrt{4^2+3^2}=\\sqrt{16+9}=\\sqrt{25}=5.\n",
    "$$\n",
    "\n",
    "Como $\\lVert g\\rVert_2=\\tau$, **no se clippea** (la regla típica: si $\\lVert g\\rVert_2>\\tau$, se escala).\n",
    "Si hubiese que escalar, sería\n",
    "$$\n",
    "g_{\\text{clipped}}=\\frac{\\tau}{\\lVert g\\rVert_2},g.\n",
    "$$\n",
    "\n",
    "- **Vector final:** $[4,,3]$.\n",
    "- **Intuición:** el **clipping** limita la magnitud de los gradientes y **evita explosión**, estabilizando pasos de optimización y reduciendo variabilidad en la actualización.\n",
    "\n",
    "**(b) Desbalance y F1 (1.2 pts).**\n",
    "\n",
    "Matriz base: $\\mathrm{TP}=12,\\ \\mathrm{FP}=6,\\ \\mathrm{FN}=9,\\ \\mathrm{TN}=73$ (umbral $0.5$).\n",
    "\n",
    "**Precisión** ($P$):\n",
    "$$\n",
    "P=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}=\\frac{12}{12+6}=\\frac{12}{18}=0.6667.\n",
    "$$\n",
    "\n",
    "**Recall** ($R$):\n",
    "$$\n",
    "R=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}=\\frac{12}{12+9}=\\frac{12}{21}=0.5714.\n",
    "$$\n",
    "\n",
    "**F1**:\n",
    "$$\n",
    "F1=\\frac{2PR}{P+R}=\\frac{2\\cdot 0.6667\\cdot 0.5714}{0.6667+0.5714}\\approx 0.6154.\n",
    "$$\n",
    "\n",
    "- **Por qué AUC-PR es más informativa en desbalance.** La **PR** se enfoca en **positivos** (precisión y recall), por lo que **refleja mejor la utilidad** cuando los negativos dominan; **ROC** puede verse \"optimista\" con muchas TN.\n",
    "- **Mitigación de desbalance:** por ejemplo, **pesos de clase** u **oversampling** de la clase minoritaria.\n",
    "- **Mejorar calibración:** por ejemplo, **temperature scaling** (ajusta la \"temperatura\" sobre *logits* para alinear probabilidades con frecuencias reales).\n",
    "\n",
    "\n",
    "#### **Q3 - Embeddings y similitud (2 pts)**\n",
    "\n",
    "**(a) Coseno básico (1.0 pt).**\n",
    "\n",
    "Vectores: $a=[1,0,1],\\ b=[0,1,1],\\ c=[1,1,0]$.\n",
    "\n",
    "**Numeradores (productos punto):**\n",
    "$$\n",
    "a\\cdot b=1\\cdot 0+0\\cdot 1+1\\cdot 1=1,\\qquad\n",
    "a\\cdot c=1\\cdot 1+0\\cdot 1+1\\cdot 0=1.\n",
    "$$\n",
    "\n",
    "**Normas:**\n",
    "$$\n",
    "\\lVert a\\rVert=\\lVert b\\rVert=\\lVert c\\rVert=\\sqrt{1^2+0^2+1^2}=\\sqrt{2}.\n",
    "$$\n",
    "\n",
    "**Similitudes coseno:**\n",
    "$$\n",
    "\\cos(a,b)=\\frac{1}{\\sqrt{2}\\sqrt{2}}=\\frac{1}{2}=0.5,\\qquad\n",
    "\\cos(a,c)=\\frac{1}{\\sqrt{2}\\sqrt{2}}=\\frac{1}{2}=0.5.\n",
    "$$\n",
    "\n",
    "- **Conclusión:** $a$ es **igual de similar** a $b$ y $c$ (ambas $0.5$).\n",
    "- **Utilidad del coseno:** con vectores **normalizados**, el coseno **mide orientación** (contenido semántico relativo) sin depender de magnitudes.\n",
    "\n",
    "**(b) Pipeline de oraciones (1.0 pt).**\n",
    "\n",
    " - **Modelo léxico:** **GloVe** (preentrenado).\n",
    " - **Preprocesamiento mínimo:** minúsculas, quitar signos de puntuación; **stopwords** opcional (según tarea).\n",
    " - **Representación de oración:** **promedio** de *embeddings* de las palabras válidas:\n",
    "$$\n",
    "\\mathbf{s}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{w}_i.\n",
    "$$\n",
    "\n",
    "- **Clasificador:** lineal (por ejemplo, regresión logística) con regularización.\n",
    "\n",
    "- **Mini matriz de confusión 3×3 (ejemplo):**\n",
    "\n",
    "|       | Pred A | Pred B | Pred C |\n",
    "| ----- | ------ | ------ | ------ |\n",
    "| **A** | 7      | 2      | 1      |\n",
    "| **B** | 1      | 6      | 3      |\n",
    "| **C** | 0      | 2      | 7      |\n",
    "\n",
    "Interpretación: **B** se confunde con **C** (por ejemplo, **lematización incompleta** o **entidades compuestas** que el promedio no separa bien).\n",
    "\n",
    "\n",
    "\n",
    "#### **Q4 - Evaluación y despliegue (2 pts)**\n",
    "\n",
    "**(a) Métricas y umbral (1.0 pt).**\n",
    "En binario desbalanceado usaría **$F1$** (balancea precisión/recall) y **AUC-PR** (sensible a la clase positiva).\n",
    "\n",
    "**Regla simple de umbral operativo:**\n",
    "\n",
    "* **Opción 1:** elegir el **umbral que maximiza $F1$** en validación.\n",
    "* **Opción 2 (coste):** fijar umbral que **minimiza**\n",
    "  $$\n",
    "  C=c_{\\mathrm{FP}}\\cdot \\mathrm{FP}+c_{\\mathrm{FN}}\\cdot \\mathrm{FN},\n",
    "  $$\n",
    "  donde $c_{\\mathrm{FP}},c_{\\mathrm{FN}}$ reflejan el impacto de errores.\n",
    "\n",
    "**(b) Reproducibilidad y latencia (1.0 pt).**\n",
    "\n",
    "**Cuatro prácticas de reproducibilidad:**\n",
    "\n",
    "1. **Semillas** fijas (Python/NumPy/torch).\n",
    "2. **Versionado** de datos/código/dependencias (*requirements.lock*).\n",
    "3. **Preprocesamiento** determinista y documentado.\n",
    "4. **Artefactos** guardados (modelo, *tokenizer*, *hashes*) para inferencia repetible.\n",
    "\n",
    "**SLA = $120\\ \\text{ms}$, p99 actual = $150\\ \\text{ms}$.**\n",
    "\n",
    "* **Acción para bajar latencia:** **desactivar ensembling** (un solo modelo), o **cuantizar** a int8, o **acortar secuencia máxima** si no afecta cobertura.\n",
    "* **Métrica de calidad a vigilar:** por ejemplo, **$F1$** (o **recall@k** si *retrieval*), comparando **antes vs. después** para asegurar que el recorte de latencia **no degrada** el desempeño fuera de tolerancia.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
