{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introducción a LangChain**\n",
    "\n",
    "LangChain es una librería de alto nivel diseñada para facilitar la construcción de aplicaciones impulsadas por modelos de lenguaje (LLM), proporcionando abstracciones sobre prompts, cadenas (Chains), agentes y herramientas. \n",
    "\n",
    "Su objetivo es ofrecer componentes reutilizables que orquesten flujos de llamada a LLM, gestión de contexto, almacenamiento de estados conversacionales y encadenamiento de tareas complejas sin tener que lidiar con detalles de bajo nivel de la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCcdzj9WQCOc",
    "outputId": "31c7e834-9438-4509-91c9-e18a9e145923"
   },
   "outputs": [],
   "source": [
    "!pip install transformers langchain accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Agentes LangChain  y herramientas**\n",
    "\n",
    "Los **Agents** en LangChain combinan LLMs con \"herramientas\" externas (por ejemplo, buscadores Web, bases de datos, funciones personalizadas). \n",
    "\n",
    "Funcionan así: el agente decide, a partir de un prompt y su memoria, qué herramienta invocar y con qué argumentos; tras ejecutar la herramienta, incorpora la respuesta al contexto y continúa el flujo. \n",
    "\n",
    "Esto permite construir asistentes que responden de manera dinámica, consultan APIs o ejecutan código.\n",
    "\n",
    "El patrón general para una aplicación con LangChain incluye:\n",
    "\n",
    "1. **PromptTemplate**: definiciones parametrizadas de prompts.\n",
    "2. **LLMChain**: encadena el prompt con la llamada al modelo y procesa la respuesta.\n",
    "3. **Memory** (opcional): guarda fragmentos de la conversación o metadatos.\n",
    "4. **Agents & Tools** (opcional): permite reactividad a eventos o consultas externas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYAn7lQdYpa9"
   },
   "source": [
    "**Ejemplo 1: Generación de una historia interactiva**\n",
    "\n",
    "Este ejemplo genera una historia interactiva donde el lector puede elegir diferentes opciones y el modelo continuará la historia según la elección del lector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bfqyn3fOYvUF",
    "outputId": "17048a3d-8c0d-4302-bce9-2294d44fd6ba"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Detectae el dispositivo y tipo de dato óptimo\n",
    "if torch.cuda.is_available():\n",
    "    dispositivo = 0                     # usa GPU 0\n",
    "    dtype = torch.bfloat16             # bfloat16 en GPU para velocidad y menor uso de memoria\n",
    "else:\n",
    "    dispositivo = -1                   # usa CPU\n",
    "    dtype = torch.float32              # float32 en CPU para evitar conversiones lentas\n",
    "\n",
    "# 2. Crea el pipeline de generación sólo una vez\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",                 \n",
    "    model=\"aisquared/dlite-v1-355m\",   \n",
    "    torch_dtype=dtype,                 \n",
    "    trust_remote_code=True,            \n",
    "    device=dispositivo,                \n",
    ")\n",
    "\n",
    "# 3. Warm-up: primera llamada rápida para cache interno\n",
    "_ = generate_text(\"Hola\")\n",
    "\n",
    "# 4. Función optimizada para continuar la historia\n",
    "def continue_story(prompt: str, choice: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera la siguiente parte de la historia combinando el prompt y la elección del usuario.\n",
    "    - max_new_tokens: límite bajo para tokens nuevos (reduce tiempo de generación)\n",
    "    - do_sample=False: decodificación greedy (más rápida y determinista)\n",
    "    \"\"\"\n",
    "    salida = generate_text(\n",
    "        f\"{prompt} {choice}\",\n",
    "        max_new_tokens=60,\n",
    "        do_sample=False\n",
    "    )\n",
    "    # Algunos pipelines devuelven lista de strings\n",
    "    if isinstance(salida, list) and isinstance(salida[0], str):\n",
    "        return salida[0]\n",
    "    # Otros devuelven lista de dicts con key \"generated_text\" o \"generated_sequence\"\n",
    "    if isinstance(salida, list) and isinstance(salida[0], dict):\n",
    "        return salida[0].get(\"generated_text\") or salida[0].get(\"generated_sequence\", \"\")\n",
    "    # Fallback: cast a str\n",
    "    return str(salida)\n",
    "\n",
    "# 5. Flujo interactivo con comentarios en español\n",
    "story_prompt = (\n",
    "    \"Te encuentras en un bosque oscuro. Delante de ti hay dos caminos. \"\n",
    "    \"¿Tomas el camino izquierdo o el camino derecho?\"\n",
    ")\n",
    "print(story_prompt)\n",
    "\n",
    "choice1 = input(\"Ingresa tu elección (izquierdo/derecho): \").strip().lower()\n",
    "story1 = continue_story(story_prompt, choice1)\n",
    "print(story1)\n",
    "\n",
    "story_prompt2 = (\n",
    "    story1 +\n",
    "    \"\\nTe topas con una figura misteriosa. ¿Te acercas a la figura o te escondes detrás de un árbol?\"\n",
    ")\n",
    "choice2 = input(\"Ingresa tu elección (acercarse/esconderse): \").strip().lower()\n",
    "story2 = continue_story(story1, choice2)\n",
    "print(story2)\n",
    "\n",
    "story_prompt3 = (\n",
    "    story2 +\n",
    "    \"\\nLa figura te ofrece un objeto mágico. ¿Lo aceptas o lo rechazas?\"\n",
    ")\n",
    "choice3 = input(\"Ingresa tu elección (aceptar/rechazar): \").strip().lower()\n",
    "story3 = continue_story(story2, choice3)\n",
    "print(story3)\n",
    "\n",
    "story_prompt4 = (\n",
    "    story3 +\n",
    "    \"\\nCon el objeto mágico en tu mano, sientes un poder. ¿Lo usas ahora o lo guardas para más tarde?\"\n",
    ")\n",
    "choice4 = input(\"Ingresa tu elección (usar/guardar): \").strip().lower()\n",
    "story4 = continue_story(story3, choice4)\n",
    "print(story4)\n",
    "\n",
    "story_prompt5 = (\n",
    "    story4 +\n",
    "    \"\\nTe encuentras con un dragón feroz. ¿Luchas contra él o intentas comunicarte?\"\n",
    ")\n",
    "choice5 = input(\"Ingresa tu elección (luchar/comunicar): \").strip().lower()\n",
    "story5 = continue_story(story4, choice5)\n",
    "print(story5)\n",
    "\n",
    "print(\"\\nLa historia termina aquí. ¡Gracias por jugar!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuoWUFRPtPzX"
   },
   "source": [
    "**Ejemplo 2: Generación de un documento técnico**\n",
    "\n",
    "Este ejemplo utiliza el modelo para generar un documento técnico sobre inteligencia artificial, dividiendo el contenido en secciones específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "id": "7Q7t_a67YzbV",
    "outputId": "19caa089-bad5-440a-ce3b-be0fe8fc127c"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Detecta el dispositivo y tipo de dato óptimo\n",
    "if torch.cuda.is_available():\n",
    "    dispositivo = 0                     # usa GPU 0\n",
    "    dtype = torch.bfloat16             # bfloat16 en GPU para velocidad y menor uso de memoria\n",
    "else:\n",
    "    dispositivo = -1                   # usa CPU\n",
    "    dtype = torch.float32              # float32 en CPU para evitar conversiones lentas\n",
    "\n",
    "# 2. Crea el pipeline de generación de texto una sola vez\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"aisquared/dlite-v1-355m\",\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    device=dispositivo,\n",
    ")\n",
    "\n",
    "# 3. Warm-up: llama brevemente para inicializar caches internas\n",
    "_ = generate_text(\"Hola\")\n",
    "\n",
    "def _extract_text(output) -> str:\n",
    "    \"\"\"\n",
    "    Función interna para extraer el texto generado\n",
    "    Soporta lista de strings, lista de dicts o un string directo.\n",
    "    \"\"\"\n",
    "    if isinstance(output, list):\n",
    "        first = output[0]\n",
    "        if isinstance(first, str):\n",
    "            return first\n",
    "        if isinstance(first, dict):\n",
    "            return first.get(\"generated_text\") or first.get(\"generated_sequence\", \"\")\n",
    "    if isinstance(output, dict):\n",
    "        return output.get(\"generated_text\") or output.get(\"generated_sequence\", \"\")\n",
    "    return str(output)\n",
    "\n",
    "# 4. Función optimizada para generar secciones del documento\n",
    "def generate_section(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera una sección detallada sobre el título dado,\n",
    "    usando un prompt contextualizado en IA.\n",
    "    \"\"\"\n",
    "    prompt = f\"Escriba una sección detallada sobre {title} en el contexto de inteligencia artificial.\"\n",
    "    out = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=100,   # límite de tokens a generar por sección\n",
    "        do_sample=False        # decodificación greedy (más rápida y determinista)\n",
    "    )\n",
    "    return _extract_text(out)\n",
    "\n",
    "# 5. Generación del documento con comentarios en español\n",
    "title = \"Una guía completa sobre la inteligencia artificial\"\n",
    "print(title)\n",
    "print(\"=\" * len(title))\n",
    "\n",
    "for section_title in [\n",
    "    \"Introduction\",\n",
    "    \"History of Artificial Intelligence\",\n",
    "    \"Machine Learning\",\n",
    "    \"Deep Learning\",\n",
    "    \"Applications of AI\",\n",
    "    \"Future of AI\"\n",
    "]:\n",
    "    print(f\"\\n{section_title}\")\n",
    "    print(\"-\" * len(section_title))\n",
    "    texto = generate_section(section_title)\n",
    "    print(texto)\n",
    "\n",
    "# 6. Sección de conclusión\n",
    "print(\"\\nConclusion\")\n",
    "print(\"-\" * len(\"Conclusion\"))\n",
    "out_conclusion = generate_text(\n",
    "    \"Escriba una conclusión para una guía completa sobre inteligencia artificial.\",\n",
    "    max_new_tokens=80,\n",
    "    do_sample=False\n",
    ")\n",
    "print(_extract_text(out_conclusion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_mf8qny4te_"
   },
   "source": [
    "**Ejemplo 3: Explicaciones y ejemplos de funciones matemáticas**\n",
    "\n",
    "Este ejemplo proporcionará una explicación de varios conceptos matemáticos y ejemplos prácticos para cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "4sKf0gGKtlsH",
    "outputId": "3b5ae448-5f72-4b04-89bd-f0c6ebb89d95"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Detecta dispositivo y tipo de dato óptimo\n",
    "if torch.cuda.is_available():\n",
    "    dispositivo = 0                   # usa GPU 0\n",
    "    dtype = torch.bfloat16           # bfloat16 en GPU para velocidad y menor uso de memoria\n",
    "else:\n",
    "    dispositivo = -1                 # usa CPU\n",
    "    dtype = torch.float32            # float32 en CPU para evitar conversiones lentas\n",
    "\n",
    "# 2. Crea el pipeline de generación de texto una sola vez\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"aisquared/dlite-v1-355m\",\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    device=dispositivo,\n",
    ")\n",
    "\n",
    "# 3. Warm-up: inicializar caches internas\n",
    "_ = generate_text(\"Hola\")\n",
    "\n",
    "def _extract_text(output) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el texto generado de distintos formatos de salida:\n",
    "    - lista de strings\n",
    "    - lista de dicts con 'generated_text' o 'generated_sequence'\n",
    "    - dict único\n",
    "    - string directo\n",
    "    \"\"\"\n",
    "    if isinstance(output, list):\n",
    "        first = output[0]\n",
    "        if isinstance(first, str):\n",
    "            return first\n",
    "        if isinstance(first, dict):\n",
    "            return first.get(\"generated_text\") or first.get(\"generated_sequence\", \"\")\n",
    "    if isinstance(output, dict):\n",
    "        return output.get(\"generated_text\") or output.get(\"generated_sequence\", \"\")\n",
    "    return str(output)\n",
    "\n",
    "# 4. Función optimizada para explicar conceptos matemáticos\n",
    "def explain_math_concept(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera una explicación detallada del concepto matemático con ejemplos.\n",
    "    \"\"\"\n",
    "    prompt = f\"Explica el {concept} con ejemplos.\"\n",
    "    out = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=100,    # límite de tokens para la explicación\n",
    "        do_sample=False        # decodificación greedy (más rápida)\n",
    "    )\n",
    "    return _extract_text(out)\n",
    "\n",
    "# 5. Función optimizada para generar ejemplos prácticos\n",
    "def generate_math_examples(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Proporciona ejemplos prácticos y soluciones paso a paso para el concepto dado.\n",
    "    \"\"\"\n",
    "    prompt = f\"Proporciona ejemplo y soluciones paso a paso para el {concept}.\"\n",
    "    out = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=150,    # límite de tokens para los ejemplos\n",
    "        do_sample=False\n",
    "    )\n",
    "    return _extract_text(out)\n",
    "\n",
    "# 6. Lista de conceptos y generación\n",
    "math_concepts = [\"derivatives\", \"integrals\", \"linear algebra\", \"probability\", \"statistics\"]\n",
    "\n",
    "for concept in math_concepts:\n",
    "    print(f\"\\nConcepto: {concept.capitalize()}\")\n",
    "    print(\"=\" * (len(concept) + 10))\n",
    "\n",
    "    # Explicación del concepto\n",
    "    explanation = explain_math_concept(concept)\n",
    "    print(\"\\nExplicación:\")\n",
    "    print(explanation)\n",
    "\n",
    "    # Ejemplos prácticos\n",
    "    examples = generate_math_examples(concept)\n",
    "    print(\"\\nEjemplos prácticos:\")\n",
    "    print(examples)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# 7. Ejemplo adicional: ecuación cuadrática\n",
    "quadratic_prompt = (\n",
    "    \"Resuelve la ecuación cuadrática 3x^2 - 4x - 5 = 0.\"\n",
    "    \"Proporciona una solución y una explicación paso a paso.\"\n",
    ")\n",
    "quadratic_out = generate_text(\n",
    "    quadratic_prompt,\n",
    "    max_new_tokens=120,\n",
    "    do_sample=False\n",
    ")\n",
    "print(\"\\nEjemplo de ecuación cuadrática:\")\n",
    "print(\"-\" * 30)\n",
    "print(_extract_text(quadratic_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompting con LangChain**\n",
    "\n",
    "**1. Prompt Templates: separación de redacción y variables**\n",
    "\n",
    "Las **Prompt Templates** son plantillas parametrizables que distinguen claramente la **estructura fija** de un prompt (instrucción, formatos, estilo) del **contenido dinámico** (variables concretas).\n",
    "\n",
    "* **Desacoplamiento**\n",
    "  El texto base del prompt queda centralizado en una plantilla; las variables se inyectan al ejecutarlo. De este modo no mezclamos la lógica de generación con valores específicos, lo que facilita su lectura y modificación.\n",
    "\n",
    "* **Mantenibilidad**\n",
    "  Al cambiar el tono, corregir errores tipográficos o ajustar el estilo, basta con actualizar la plantilla; todas las invocaciones posteriores heredarán la mejora de forma automática.\n",
    "\n",
    "* **Reutilización**\n",
    "  Una misma plantilla puede servir para diferenciar tareas (resúmenes, explicaciones, preguntas) simplemente variando las variables de entrada. Esto reduce la duplicación de prompts en el código y promueve un diseño más limpio.\n",
    "\n",
    "\n",
    "**2. Few-Shot & ExampleSelectors: ejemplos manuales y automáticos**\n",
    "\n",
    "Los **Few-Shot Prompts** incluyen ejemplos concretos de pares *(entrada -> salida)* dentro del propio prompt, enseñando al modelo \"en vuelo\" el patrón deseado. Sin embargo, elegir manualmente qué ejemplos incluir puede resultar laborioso y no siempre óptimo.\n",
    "\n",
    "* **ExampleSelectors**\n",
    "  Automatizan la selección de los ejemplos más relevantes desde una librería amplia, basándose en:\n",
    "\n",
    "  * **Similitud semántica** (mide la cercanía con embeddings).\n",
    "  * **Coincidencia de palabras clave** o metadatos.\n",
    "\n",
    "* **Beneficios**\n",
    "\n",
    "  1. **Adaptabilidad**: el modelo recibe ejemplos alineados con la consulta actual, mejorando su entendimiento.\n",
    "  2. **Escalabilidad**: permite gestionar grandes bancos de ejemplos sin saturar el contexto.\n",
    "  3. **Calidad constante**: evita sesgos o errores de selección manual, pues el sistema elige según métricas objetivas.\n",
    "\n",
    "**3. Output Parsers: formatos rígidos y fiabilidad**\n",
    "\n",
    "Los **Output Parsers** convierten la respuesta \"libre\" de un LLM en estructuras de datos controladas (JSON, diccionarios tipados, objetos de validación).\n",
    "\n",
    "* **Validación temprana**\n",
    "  Si la salida no encaja en el esquema esperado (por ejemplo, falta un campo obligatorio o el tipo de dato es incorrecto), el parser detecta la anomalía y permite:\n",
    "\n",
    "  * Solicitar al modelo un reintento con instrucciones aclaratorias.\n",
    "  * Aplicar reglas de recuperación o fallback.\n",
    "\n",
    "* **Reducción de errores downstream**\n",
    "  Al garantizar que el formato de salida sea siempre uniforme, evitamos excepciones e inconsistencias en etapas posteriores (almacenamiento, visualización, integración con otras APIs).\n",
    "\n",
    "* **Trazabilidad y observabilidad**\n",
    "  Registrar cuántas veces fallan los parsers o qué validaciones son las más conflictivas ayuda a iterar y optimizar tanto los prompts como los esquemas de datos.\n",
    "\n",
    "\n",
    "**4. Aplicación en un resumidor de noticias**\n",
    "\n",
    "Al incorporar estas tres piezas en un **News Articles Summarizer**, elevamos la calidad y consistencia de los resúmenes:\n",
    "\n",
    "1. **Prompt Templates**\n",
    "\n",
    "   * Definen el estilo editorial (longitud, tono, nivel de detalle) en un único lugar.\n",
    "   * Permiten ajustar rápidamente el \"brief\" que se envía al LLM sin tocar la lógica de orquestación.\n",
    "\n",
    "2. **Few-Shot & ExampleSelectors**\n",
    "\n",
    "   * Recurre a resúmenes humanos previos de artículos similares para guiar el estilo y evitar alucinaciones.\n",
    "   * Selecciona los ejemplos más alineados semánticamente con el tema del artículo, garantizando coherencia temática.\n",
    "\n",
    "3. **Output Parsers**\n",
    "\n",
    "   * Fuerzan un formato JSON fijo:\n",
    "\n",
    "     ```json\n",
    "     {\n",
    "       \"title\": \"...\",\n",
    "       \"summary\": \"...\",\n",
    "       \"keywords\": [\"...\", \"...\"],\n",
    "       \"author\": \"...\",\n",
    "       \"date\": \"YYYY-MM-DD\"\n",
    "     }\n",
    "     ```\n",
    "   * Facilitan la integración con sistemas de publicación o bases de datos, minimizando la limpieza de datos manual.\n",
    "\n",
    "\n",
    "**5. Extensión a Knowledge Graphs**\n",
    "\n",
    "Para convertir los resúmenes en un **recurso navegable y semántico**, podemos construir grafos de conocimiento:\n",
    "\n",
    "1. **Extracción de entidades y relaciones**\n",
    "\n",
    "   * Mediante prompts especializados, pedimos al LLM que identifique nombres de personas, organizaciones, conceptos clave y sus interacciones.\n",
    "\n",
    "2. **Construcción del grafo**\n",
    "\n",
    "   * **Nodos**: las entidades detectadas.\n",
    "   * **Aristas**: relaciones etiquetadas (por ejemplo, \"colaboró con\", \"es parte de\", \"causó\").\n",
    "\n",
    "3. **Consultas semánticas**\n",
    "\n",
    "   * Combinamos un *retriever* para localizar fragmentos relevantes en los resúmenes con prompts que interpretan rutas en el grafo:\n",
    "\n",
    "     > \"¿Cómo se relaciona el concepto X con la organización Y?\"\n",
    "\n",
    "4. **Beneficios**\n",
    "\n",
    "   * Descubrimos conexiones ocultas entre temas.\n",
    "   * Ofrecemos a los usuarios exploración interactiva: navegar de nodo en nodo, profundizar en relaciones, generar nuevos insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ingeniería de prompts**\n",
    "\n",
    "Además de los conceptos básicos (plantillas, few-shot, parsers), en la práctica es útil trabajar con una especie de \"checklist\" de ingeniería de *prompts*. Esta sección resume patrones que usaremos tanto con Transformers \"puros\" como con LangChain y, más adelante, en RAG.  \n",
    "En las **celdas de código siguientes** verás ejemplos prácticos de:\n",
    "\n",
    "- Estructurar prompts con secciones y delimitadores.\n",
    "- Usar few-shot prompting.\n",
    "- Generar salidas estructuradas (JSON).\n",
    "- Hacer prompting \"grounded\" con contexto al estilo RAG y reducir alucinaciones.\n",
    "\n",
    "#### **1. Formular tareas claramente y estructurar *prompts* con secciones**\n",
    "\n",
    "Un *prompt* efectivo suele tener partes bien separadas:\n",
    "\n",
    "- **Contexto**: de qué va la tarea.\n",
    "- **Instrucción**: qué debe hacer el modelo.\n",
    "- **Ejemplos (opcional)**.\n",
    "- **Formato de salida**.\n",
    "\n",
    "Ejemplo de estructura verbal:\n",
    "\n",
    "> **Rol:** Eres un asistente que explica temas de NLP a estudiantes de pregrado.  \n",
    "> **Tarea:** Explica el concepto de *transformer* en lenguaje sencillo.  \n",
    "> **Restricciones:**  \n",
    "> - Máximo 3 párrafos.  \n",
    "> - Incluye un ejemplo concreto.  \n",
    "\n",
    "En los ejemplos de código veremos cómo convertir estas ideas en prompts reutilizables.\n",
    "\n",
    "\n",
    "#### **2. Usar *few-shot prompting*/*in-context learning* con buenos ejemplos**\n",
    "\n",
    "En vez de dar solo una instrucción, añadimos ejemplos bien elegidos:\n",
    "\n",
    "- Muestran el **estilo** deseado.\n",
    "- Fijan el **formato** de entrada y salida.\n",
    "- Guían al modelo cuando la tarea es ambigua.\n",
    "\n",
    "Esquema típico:\n",
    "\n",
    "> **Ejemplo 1**  \n",
    "> Entrada: \"Explica *overfitting* a un niño de 10 años.\"  \n",
    "> Salida: \"Imagina que...\"  \n",
    ">\n",
    "> **Ejemplo 2**  \n",
    "> Entrada: \"Explica *regularización L2* a un estudiante de CS.\"  \n",
    "> Salida: \"En modelos lineales...\"  \n",
    ">\n",
    "> **Ahora tú:**  \n",
    "> Entrada: \"Explica *embeddings* a un estudiante de ingeniería.\"  \n",
    "> Salida: ...\n",
    "\n",
    "En el primer ejemplo de código combinaremos **few-shot** con salidas en formato **JSON**.\n",
    "\n",
    "#### **3. Aplicar *chain-of-thought* y razonamiento paso a paso**\n",
    "\n",
    "Cuando la tarea requiere razonamiento (matemático, lógico, planificación), podemos pedir explícitamente:\n",
    "\n",
    "- \"Razona paso a paso antes de dar la respuesta final.\"\n",
    "- \"Primero explica tu razonamiento y luego da solo la respuesta final.\"\n",
    "\n",
    "Ejemplo de instrucción:\n",
    "\n",
    "> \"Resuelve el problema paso a paso. No saltes pasos.  \n",
    "> Al final, resume la respuesta en una sola línea que empiece con:  \n",
    "> **Respuesta final:** ...\"\n",
    "\n",
    "Esto ayuda al modelo a \"desplegar\" su razonamiento en lugar de intentar adivinar directamente la respuesta final.\n",
    "\n",
    "#### **4. Diseñar *prompts* de auto-verificación y mejora iterativa**\n",
    "\n",
    "Podemos pedir que el modelo:\n",
    "\n",
    "1. Produzca un primer borrador.\n",
    "2. Revise su propia respuesta.\n",
    "3. Corrija o mejore.\n",
    "\n",
    "Patrón típico:\n",
    "\n",
    "1. \"Primero, genera una respuesta inicial.\"  \n",
    "2. \"Luego, revisa tu respuesta buscando errores, omisiones o incoherencias.\"  \n",
    "3. \"Por último, entrega una versión corregida y mejorada.\"  \n",
    "\n",
    "Este patrón es útil para tareas complejas (código, redacción técnica, planes, etc.).\n",
    "\n",
    "#### **5. Controlar estilo, nivel y rol del modelo según la audiencia**\n",
    "\n",
    "Definimos explícitamente:\n",
    "\n",
    "- **Rol:** \"Eres un profesor universitario...\", \"Eres un revisor crítico...\", \"Eres un asistente pedagógico...\"\n",
    "- **Nivel:** \"Dirígete a estudiantes de pregrado...\", \"Nivel técnico básico...\", \"Nivel avanzado...\"\n",
    "- **Estilo:** \"Usa ejemplos concretos\", \"Evita ecuaciones\", \"Sé conciso (máx. 200 palabras)\".\n",
    "\n",
    "Esto permite adaptar el mismo conocimiento a distintas audiencias sin cambiar el modelo ni el código.\n",
    "\n",
    "#### **6. Generar salidas estructuradas (JSON, tablas) de forma fiable**\n",
    "\n",
    "Cuando queremos que la salida se procese con código, necesitamos **formato rígido**:\n",
    "\n",
    "- Especificar el **esquema** (campos, tipos).\n",
    "- Pedir explícitamente: \"No agregues texto fuera del JSON\".\n",
    "\n",
    "Ejemplo de instrucción:\n",
    "\n",
    "> \"Devuelve la respuesta **únicamente** en formato JSON con la forma:  \n",
    "> `{ \"tema\": string, \"nivel\": string, \"explicacion\": string, \"ejemplo\": string }`  \n",
    "> No incluyas texto adicional antes o después del JSON.\"\n",
    "\n",
    "\n",
    "#### **7. *Prompting* con contexto (RAG) y reducción de alucinaciones**\n",
    "\n",
    "En RAG, el modelo debe basarse **solo en los documentos recuperados**. Un patrón típico:\n",
    "\n",
    "1. Pegamos el contexto (chunks recuperados) entre delimitadores claros.\n",
    "2. Damos instrucciones explícitas anti-alucinación.\n",
    "\n",
    "Ejemplo de instrucciones:\n",
    "\n",
    "- \"Usa **exclusivamente** la información del contexto proporcionado.\"  \n",
    "- \"Si la respuesta no está en el contexto, responde: ‘No encuentro esta información en los documentos proporcionados.’\"  \n",
    "- \"No inventes nombres, números ni referencias.\"  \n",
    "  \n",
    "El cuaderno `RAG.ipynb` extenderá esta idea con embeddings, bases de datos vectoriales y métricas de evaluación.\n",
    "\n",
    "#### **8. Diseñar *prompts* que gestionen la incertidumbre**\n",
    "\n",
    "Los modelos tienden a contestar siempre, incluso cuando no saben. Podemos:\n",
    "\n",
    "- Incluir una opción \"no lo sé\" o \"no está en el contexto\".\n",
    "- Pedir que marquen explícitamente cuando especulan.\n",
    "\n",
    "Ejemplo de instrucción:\n",
    "\n",
    "> \"Si no estás seguro de la respuesta, di explícitamente:  \n",
    "> ‘No lo sé con certeza; estoy especulando por estas razones: ...’\"\n",
    "\n",
    "Esto es especialmente importante en dominios sensibles (salud, finanzas, seguridad).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9. Entender el patrón básico de agentes (pensar -> actuar -> observar -> responder)**\n",
    "\n",
    "Los **agentes** combinan prompts + herramientas. El patrón conceptual es:\n",
    "\n",
    "1. **Pensar:** el modelo razona internamente sobre la tarea.  \n",
    "2. **Actuar:** decide usar una herramienta (búsqueda, API, código, etc.).  \n",
    "3. **Observar:** lee la salida de la herramienta.  \n",
    "4. **Responder:** genera la respuesta final al usuario.\n",
    "\n",
    "LangChain formaliza este ciclo con *agents* y *tools*. Más adelante podemos conectar esto con el uso de RAG, bases de datos vectoriales o APIs externas.\n",
    "\n",
    "#### **10. Diseñar *prompts* para evaluación automática (LLM-as-a-judge)**\n",
    "\n",
    "Un LLM también puede evaluar respuestas de otros modelos, pero con cuidado:\n",
    "\n",
    "- Definimos criterios claros (correctitud, claridad, cobertura, estilo).  \n",
    "- Pedimos una salida estructurada (por ejemplo, puntuaciones 0-5 por criterio).  \n",
    "- Recordamos que el juez también puede alucinar o sesgarse.\n",
    "\n",
    "Ejemplo de formato de salida:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"score_correctitud\": 0-5,\n",
    "  \"score_claridad\": 0-5,\n",
    "  \"score_cobertura\": 0-5,\n",
    "  \"comentarios\": \"texto libre\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejemplos (pseudocódigos de técnicas de *prompting)***\n",
    "\n",
    "Los siguientes fragmentos están pensados para **no demorar mucho** al ejecutarse:  \n",
    "solo construyen *prompts* o muestran patrones. Puedes adaptarlos para usarlos con\n",
    "el `pipeline` de Transformers o con LangChain.\n",
    "\n",
    "**1. Prompt estructurado con secciones y delimitadores**\n",
    "\n",
    "```python\n",
    "def build_structured_prompt(tema: str) -> str:\n",
    "    return f\"\"\"\n",
    "[ROL]\n",
    "Eres un profesor de IA que explica conceptos de forma clara y breve.\n",
    "\n",
    "[TAREA]\n",
    "Explica el tema: \"{tema}\" a un estudiante de pregrado.\n",
    "\n",
    "[RESTRICCIONES]\n",
    "- Máximo 2 párrafos.\n",
    "- Incluye un ejemplo sencillo.\n",
    "\n",
    "[FORMATO_SALIDA]\n",
    "Escribe solo el texto de la explicación, sin listas ni viñetas.\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_structured_prompt(\"transformers\")\n",
    "print(prompt)  # Aquí recién podrías pasarlo al modelo si quieres\n",
    "```\n",
    "\n",
    "Este patrón **no llama al modelo**: solo construye el prompt de forma legible y reutilizable.\n",
    "\n",
    "**2. Few-shot/in-context learning con ejemplos breves**\n",
    "\n",
    "```python\n",
    "def build_fewshot_prompt(pregunta: str) -> str:\n",
    "    ejemplos = \"\"\"\n",
    "Ejemplo 1\n",
    "[IN]\n",
    "Explica overfitting en una frase.\n",
    "[OUT]\n",
    "Overfitting es cuando un modelo se ajusta tanto a los datos de entrenamiento\n",
    "que falla al generalizar a datos nuevos.\n",
    "\n",
    "Ejemplo 2\n",
    "[IN]\n",
    "Explica regularización L2 en una frase.\n",
    "[OUT]\n",
    "La regularización L2 penaliza los pesos grandes para evitar que el modelo se\n",
    "vuelva demasiado complejo.\n",
    "\"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "{ejemplos}\n",
    "\n",
    "Ahora responde al siguiente caso:\n",
    "\n",
    "[IN]\n",
    "{pregunta}\n",
    "[OUT]\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_fewshot_prompt(\"Explica embeddings en una frase.\")\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "Puedes usar este prompt con tu `generate_text(prompt, max_new_tokens=60, do_sample=False)` para que siga el estilo de los ejemplos.\n",
    "\n",
    "**3. *Chain-of-thought* (razonamiento paso a paso)**\n",
    "\n",
    "```python\n",
    "def build_cot_prompt(problema: str) -> str:\n",
    "    return f\"\"\"\n",
    "Resuelve el siguiente problema paso a paso.\n",
    "\n",
    "Problema: {problema}\n",
    "\n",
    "Instrucciones:\n",
    "1. Primero, razona paso a paso explicando cada operación.\n",
    "2. Luego, en la última línea, escribe:\n",
    "   Respuesta final: <aquí solo el resultado>\n",
    "\n",
    "Empieza tu razonamiento ahora.\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_cot_prompt(\"Si un modelo acierta 80 de 100 ejemplos, ¿cuál es su accuracy?\")\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "Este patrón insiste en separar el **razonamiento** de la **respuesta final**.\n",
    "\n",
    "**4. Auto-verificación y mejora iterativa**\n",
    "\n",
    "```python\n",
    "def build_self_check_prompt(texto: str) -> str:\n",
    "    return f\"\"\"\n",
    "Tienes el siguiente borrador de explicación:\n",
    "\n",
    "---\n",
    "{texto}\n",
    "---\n",
    "\n",
    "Tarea:\n",
    "1. Identifica posibles errores, ambigüedades o partes poco claras.\n",
    "2. Corrige el texto produciendo una versión mejorada.\n",
    "3. Devuelve SOLO la versión mejorada, sin comentarios adicionales.\n",
    "\"\"\"\n",
    "\n",
    "draft = \"Los transformers siempre usan atención completa y nunca se optimizan con SGD.\"\n",
    "prompt = build_self_check_prompt(draft)\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "La idea es que el modelo actúe como **revisor** de su propio texto u otro texto dado.\n",
    "\n",
    "**5. Salida estructurada (JSON) para evaluación o uso posterior**\n",
    "\n",
    "```python\n",
    "def build_json_judge_prompt(respuesta: str, referencia: str) -> str:\n",
    "    return f\"\"\"\n",
    "Actúa como evaluador de respuestas cortas.\n",
    "\n",
    "[REFERENCIA]\n",
    "{referencia}\n",
    "[/REFERENCIA]\n",
    "\n",
    "[RESPUESTA_ESTUDIANTE]\n",
    "{respuesta}\n",
    "[/RESPUESTA_ESTUDIANTE]\n",
    "\n",
    "Evalúa la respuesta del estudiante respecto a la referencia y\n",
    "devuelve SOLO un JSON con el siguiente formato:\n",
    "\n",
    "{{\n",
    "  \"score_correctitud\": 0-5,\n",
    "  \"score_claridad\": 0-5,\n",
    "  \"score_cobertura\": 0-5,\n",
    "  \"comentarios\": \"texto breve\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_json_judge_prompt(\n",
    "    respuesta=\"Un transformer es una red neuronal que usa atención.\",\n",
    "    referencia=\"Un transformer es una arquitectura basada en atención que reemplazó a las RNN en muchas tareas de NLP.\"\n",
    ")\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "Este patrón se conecta con **LLM-as-a-judge** y fuerza una salida fácil de procesar con código.\n",
    "\n",
    "**6. Prompting \"grounded\" con contexto al estilo RAG**\n",
    "\n",
    "```python\n",
    "def build_grounded_prompt(pregunta: str, contexto: str) -> str:\n",
    "    return f\"\"\"\n",
    "Responde a la pregunta usando EXCLUSIVAMENTE la información del contexto.\n",
    "\n",
    "[CONTEXTO]\n",
    "{contexto}\n",
    "[/CONTEXTO]\n",
    "\n",
    "Instrucciones:\n",
    "- Si la información está en el contexto, responde de forma breve y precisa.\n",
    "- Si NO está, responde exactamente:\n",
    "  \"No encuentro esta información en los documentos proporcionados.\"\n",
    "- No inventes datos que no aparezcan en el contexto.\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "contexto_demo = \"RAG combina un módulo de recuperación de documentos con un modelo generativo para responder con contexto actualizado.\"\n",
    "prompt = build_grounded_prompt(\"¿Qué es RAG?\", contexto_demo)\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "Este patrón es un mini-RAG \"a mano\": el contexto lo pasas tú, sin necesidad de una base vectorial todavía.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "**Ejercicio 1 - Historia interactiva con PromptTemplate + LLMChain**\n",
    "\n",
    "**Basado en:** \"Ejemplo 1: Generación de una historia interactiva\".\n",
    "\n",
    "1. **Analiza el prompt actual** que se pasa a `generate_text` en `continue_story(prompt, choice)`.\n",
    "\n",
    "   * Escribe en una celda *markdown* qué partes corresponden a:\n",
    "\n",
    "     * Contexto de la historia.\n",
    "     * Instrucción al modelo.\n",
    "     * Entrada del usuario (elección).\n",
    "     * Restricciones (si es que hay).\n",
    "\n",
    "2. **Diseña un prompt estructurado** usando el patrón de la sección de prompting:\n",
    "\n",
    "   ```text\n",
    "   [ROL]\n",
    "   [CONTEXTO]\n",
    "   [ELECCIÓN_DEL_USUARIO]\n",
    "   [INSTRUCCIONES]\n",
    "   [RESTRICCIONES]\n",
    "   ```\n",
    "\n",
    "   Escríbelo como una función:\n",
    "\n",
    "   ```python\n",
    "   def build_story_prompt(historia_actual: str, eleccion: str) -> str:\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "3. **Refactoriza `continue_story`** para usar `build_story_prompt` en lugar del prompt \"plano\" actual.\n",
    "\n",
    "4. **Integra LangChain**:\n",
    "\n",
    "   * Envuelve tu `generate_text` en un LLM de LangChain (por ejemplo `HuggingFacePipeline`).\n",
    "   * Crea un `PromptTemplate` con variables `historia_actual` y `eleccion`.\n",
    "   * Crea un `LLMChain` que reciba esas variables y devuelva el siguiente fragmento de historia.\n",
    "\n",
    "5. **Comparación cualitativa**:\n",
    "\n",
    "   * Ejecuta al menos 3 veces la historia con el código original y con la versión LangChain+PromptTemplate.\n",
    "   * Escribe en markdown:\n",
    "\n",
    "     * ¿Mejora la coherencia entre decisiones y resultado?\n",
    "     * ¿Es más fácil ajustar el estilo (por ejemplo, hacerlo más humorístico, más oscuro, etc.)?\n",
    "\n",
    "**Ejercicio 2 - Documento técnico con few-shot prompting**\n",
    "\n",
    "**Basado en:** \"Ejemplo 2: Generación de un documento técnico\".\n",
    "\n",
    "1. Observa cómo se generan actualmente las secciones (`Introducción`, `Aplicaciones`, etc.) con un prompt genérico.\n",
    "\n",
    "2. Diseña un **prompt few-shot** para una sección (por ejemplo, \"Aplicaciones\"):\n",
    "\n",
    "   * Escribe **2 ejemplos cortos** de textos bien escritos para otras secciones (por ejemplo, \"Historia de la IA\", \"Limitaciones actuales\").\n",
    "   * Usa el formato:\n",
    "\n",
    "     ```text\n",
    "     [EJEMPLO 1]\n",
    "     SECCIÓN: Historia de la IA\n",
    "     TEXTO: ...\n",
    "\n",
    "     [EJEMPLO 2]\n",
    "     SECCIÓN: Limitaciones actuales\n",
    "     TEXTO: ...\n",
    "\n",
    "     [NUEVA_TAREA]\n",
    "     SECCIÓN: {nombre_seccion}\n",
    "     INSTRUCCIONES: Mantén el estilo de los ejemplos.\n",
    "     ```\n",
    "\n",
    "3. Implementa una función:\n",
    "\n",
    "   ```python\n",
    "   def build_tech_section_prompt(nombre_seccion: str, descripcion: str) -> str:\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "4. Usa LangChain para crear un `LLMChain` que, dado `nombre_seccion` y `descripcion`, genere el contenido de cada sección con estilo consistente.\n",
    "\n",
    "5. **Discusión corta**:\n",
    "\n",
    "   * ¿Qué diferencias notas entre:\n",
    "\n",
    "     * Prompt sin ejemplos.\n",
    "     * Prompt con pocos ejemplos (few-shot).\n",
    "\n",
    "**Ejercicio 3 - Explicaciones matemáticas con *chain-of-thought***\n",
    "\n",
    "**Basado en:** \"Ejemplo 3: Explicaciones y ejemplos de funciones matemáticas\".\n",
    "\n",
    "1. El código actual pide explicaciones y ejemplos de conceptos matemáticos (funciones lineales, cuadráticas, etc.).\n",
    "\n",
    "2. Diseña un **prompt con chain-of-thought**:\n",
    "\n",
    "   ```text\n",
    "   [ROL]\n",
    "   Eres un profesor de matemáticas.\n",
    "\n",
    "   [TAREA]\n",
    "   Explica el concepto: {concepto}.\n",
    "\n",
    "   [REQUISITOS]\n",
    "   1. Primero, razona paso a paso de forma interna.\n",
    "   2. Luego, entrega al estudiante:\n",
    "      - Una explicación breve (2-3 párrafos).\n",
    "      - Un ejemplo numérico resuelto.\n",
    "\n",
    "   [FORMATO_SALIDA]\n",
    "   Explicación:\n",
    "   ...\n",
    "   Ejemplo:\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "3. Implementa una función `build_math_prompt(concepto: str) -> str` y úsala con el `pipeline` actual.\n",
    "\n",
    "4. Variante avanzada:\n",
    "\n",
    "   * Escribe un prompt que le diga al modelo:\n",
    "\n",
    "     > \"Piensa paso a paso **pero no muestres** todo el razonamiento, solo entrega la explicación final y el ejemplo\".\n",
    "   * Compara las salidas con y sin esta instrucción:\n",
    "\n",
    "     * ¿El modelo tiende a \"pensar más\" (más pasos, más detalles)?\n",
    "     * ¿La explicación es más clara?\n",
    "\n",
    "**Ejercicio 4 - Auto-verificación y mejora iterativa con LangChain**\n",
    "\n",
    "**Idea:** aplicar la técnica de *self-checking* / *self-refinement*.\n",
    "\n",
    "1. Elige una de las tareas anteriores (historia, sección técnica o explicación matemática).\n",
    "\n",
    "2. Crea **dos prompts**:\n",
    "\n",
    "   * `prompt_respuesta`: el modelo da una primera respuesta.\n",
    "   * `prompt_revisor`: el modelo actúa como revisor de su propia respuesta, con instrucciones del tipo:\n",
    "\n",
    "     > \"Dado el enunciado y tu respuesta, detecta errores, mejora la claridad y corrige pasos dudosos. Devuelve una versión mejorada.\"\n",
    "\n",
    "3. Implementa en LangChain un flujo de 2 pasos:\n",
    "\n",
    "   * `LLMChain` 1: genera la respuesta inicial.\n",
    "   * `LLMChain` 2: toma `{enunciado, respuesta_inicial}` y produce una respuesta mejorada.\n",
    "\n",
    "4. **Actividad de análisis**:\n",
    "\n",
    "   * Guarda (`print`) la respuesta inicial y la revisada para 3 entradas distintas.\n",
    "   * Describe en markdown:\n",
    "\n",
    "     * Casos donde la revisión mejora claramente la respuesta.\n",
    "     * Casos donde la revisión introduce errores o cambios innecesarios.\n",
    "\n",
    "**Ejercicio 5 - Prompting *grounded* al estilo mini-RAG**\n",
    "\n",
    "**Basado en:** el pseudocódigo de `build_grounded_prompt` en el cuaderno.\n",
    "\n",
    "1. Implementa por completo `build_grounded_prompt(pregunta: str, contexto: str)` siguiendo la idea del cuaderno:\n",
    "\n",
    "   * Bloques `[ROL]`, `[CONTEXT]`, `[INSTRUCCIONES]`, `[RESPUESTA]`.\n",
    "   * Regla explícita:\n",
    "\n",
    "     > \"Si la respuesta no está en el contexto, responde: 'No encuentro esta información en los documentos proporcionados.'\"\n",
    "\n",
    "2. Construye un pequeño \"dataset\" de contexto y preguntas, por ejemplo:\n",
    "\n",
    "   ```python\n",
    "   contexto_rag = \"\"\"\n",
    "   RAG combina un módulo de recuperación de documentos con un modelo\n",
    "   generativo para responder usando información actualizada...\n",
    "   \"\"\"\n",
    "\n",
    "   preguntas = [\n",
    "       \"¿Qué es RAG?\",\n",
    "       \"¿Cómo se relaciona RAG con bases de datos vectoriales?\",\n",
    "       \"¿Cuál es la capital de Francia?\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. Para cada pregunta:\n",
    "\n",
    "   * Construye el prompt con `build_grounded_prompt`.\n",
    "   * Llama al `pipeline` y guarda la respuesta.\n",
    "\n",
    "4. **Discute**:\n",
    "\n",
    "   * ¿El modelo respeta la regla de \"No encuentro esta información...\" cuando preguntas algo fuera del contexto?\n",
    "   * ¿Qué cambios harías en el prompt para reducir alucinaciones?\n",
    "\n",
    "**Ejercicio 6 - Primer agentecon LangChain (pensar -> actuar -> observar -> responder)**\n",
    "\n",
    "**Relacionado con:** sección \"9. Patrón básico de agentes\".\n",
    "\n",
    "1. Define al menos **dos herramientas** como funciones Python simples:\n",
    "\n",
    "   ```python\n",
    "   def buscar_definicion(concepto: str) -> str:\n",
    "       # Devuelve una \"definición\" desde un diccionario fijo en memoria\n",
    "       ...\n",
    "\n",
    "   def calcular_expresion(expr: str) -> str:\n",
    "       # Evalúa una expresión matemática muy limitada, con seguridad\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "2. Con LangChain, crea:\n",
    "\n",
    "   * Objetos `Tool` para cada función.\n",
    "   * Un LLM (por ejemplo, el mismo HuggingFace o uno remoto tipo ChatOpenAI si lo tienes disponible).\n",
    "\n",
    "3. Diseña un prompt de agente que siga el ciclo:\n",
    "\n",
    "   ```text\n",
    "   [ROL]\n",
    "   Eres un agente que decide cuándo usar herramientas.\n",
    "\n",
    "   [HERRAMIENTAS_DISPONIBLES]\n",
    "   - buscar_definicion\n",
    "   - calcular_expresion\n",
    "\n",
    "   [INSTRUCCIONES]\n",
    "   1. Piensa primero qué necesitas hacer.\n",
    "   2. Si necesitas información externa, invoca una herramienta.\n",
    "   3. Observa la salida.\n",
    "   4. Solo entonces responde al usuario.\n",
    "   ```\n",
    "\n",
    "4. Prueba el agente con al menos 5 consultas:\n",
    "\n",
    "   * Unas que requieran definiciones (\"¿Qué es una función cuadrática?\").\n",
    "   * Otras que requieran cálculo (\"Calcula 3x^2 - 4x - 5 para x=2\").\n",
    "\n",
    "5. Escribe en markdown:\n",
    "\n",
    "   * ¿Cuándo decide usar una herramienta?\n",
    "   * ¿Cuándo responde sin usar herramientas?\n",
    "   * ¿Alguna decisión es claramente equivocada? ¿Cómo ajustarías el prompt del agente?\n",
    "\n",
    "**Ejercicio 7 - LLM-as-a-judge: evaluación automática de respuestas**\n",
    "\n",
    "**Relacionado con:** sección \"10. Diseñar prompts para evaluación automática\".\n",
    "\n",
    "1. Define una tarea base, por ejemplo:\n",
    "\n",
    "   * Explicar el concepto de \"transformer\".\n",
    "   * Explicar qué es \"RAG\".\n",
    "\n",
    "2. Pide al modelo una respuesta a la tarea con el pipeline o con LangChain.\n",
    "\n",
    "3. Diseña un prompt de juez:\n",
    "\n",
    "   ```text\n",
    "   [ROL]\n",
    "   Eres un evaluador estricto de respuestas de estudiantes.\n",
    "\n",
    "   [CRITERIOS]\n",
    "   - correctitud (0-5)\n",
    "   - claridad (0-5)\n",
    "   - cobertura (0-5)\n",
    "\n",
    "   [TAREA]\n",
    "   Evalúa la respuesta del estudiante a la siguiente pregunta:\n",
    "   Pregunta: {pregunta}\n",
    "   Respuesta_estudiante: {respuesta}\n",
    "\n",
    "   [FORMATO_SALIDA]\n",
    "   Devuelve un JSON válido con esta estructura:\n",
    "   {{\n",
    "     \"score_correctitud\": <número de 0 a 5>,\n",
    "     \"score_claridad\": <número de 0 a 5>,\n",
    "     \"score_cobertura\": <número de 0 a 5>,\n",
    "     \"comentarios\": \"texto breve\"\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "4. Implementa una función:\n",
    "\n",
    "   ```python\n",
    "   def evaluar_respuesta(pregunta: str, respuesta: str) -> dict:\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "   que llame al modelo, intente parsear el JSON y devuelva un `dict` de Python.\n",
    "\n",
    "5. Prueba con:\n",
    "\n",
    "   * Una respuesta buena.\n",
    "   * Una respuesta incompleta.\n",
    "   * Una respuesta deliberadamente incorrecta.\n",
    "\n",
    "6. Discusión:\n",
    "\n",
    "   * ¿El juez es consistente?\n",
    "   * ¿Qué riesgos ves al usar LLM-as-a-judge en un entorno de evaluación real?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-proyecto - Comparando \"solo Transformers\" vs \"Transformers + LangChain + buenas técnicas de prompting\"**\n",
    "\n",
    "**Objetivo:** integrar todo.\n",
    "\n",
    "1. Define un menú simple (puede ser con `input()` o en celdas separadas) con 3 \"modos\":\n",
    "\n",
    "   * Modo A: historia interactiva.\n",
    "   * Modo B: documento técnico corto.\n",
    "   * Modo C: explicación matemática.\n",
    "\n",
    "2. Para cada modo, implementa **dos versiones**:\n",
    "\n",
    "   * Versión 1: usando solo el `pipeline` de `transformers` con prompts simples.\n",
    "   * Versión 2: usando LangChain (PromptTemplate, LLMChain, quizá alguna `Tool`) y al menos **una técnica de prompting avanzada**:\n",
    "\n",
    "     * few-shot,\n",
    "     * chain-of-thought,\n",
    "     * grounding con contexto,\n",
    "     * auto-verificación, etc.\n",
    "\n",
    "3. Pide a 2-3 compañeros (o tú mismo en distintos momentos) que:\n",
    "\n",
    "   * Usen la versión 1 y la versión 2.\n",
    "   * Evalúen subjetivamente: claridad, coherencia, utilidad.\n",
    "\n",
    "4. Resume en markdown:\n",
    "\n",
    "   * ¿Qué aporta LangChain en términos de organización del código y reusabilidad?\n",
    "   * ¿Qué aportan las técnicas de prompting frente a prompts \"ad-hoc\"?\n",
    "   * ¿En qué casos el *over-engineering* no se justifica (tarea demasiado simple)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
