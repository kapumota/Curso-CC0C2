{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bed2cd",
   "metadata": {},
   "source": [
    "### **Workflows y Pipelines de LLM + Agentes**\n",
    "\n",
    "Este cuaderno cubre, con ejemplos ejecutables, los siguientes temas:\n",
    "\n",
    "- Workflows y pipelines de LLM (preprocesamiento -> LLM -> postprocesamiento)\n",
    "- Multi-step prompting y chain-of-actions\n",
    "- Patrones de workflow: secuencial, branching, paralelo\n",
    "- Integración con APIs y microservicios\n",
    "- Observabilidad de workflows (logs de prompts, métricas, trazas)\n",
    "- Agentes (agent loop: observar -> razonar -> actuar -> observar)\n",
    "- Agentes reactivos, planificadores, con memoria\n",
    "- Agentes + RAG + herramientas, tool routing\n",
    "- Seguridad y guardrails en agentes (validación, límites, auditoría)\n",
    "- Workflows multi-agente (Manager + Workers, critic/reviewer)\n",
    "- Casos de uso: QA empresarial con RAG + agentes, generación de código + agente tester\n",
    "- RAG y reducción de alucinaciones \n",
    "- Razonamiento sobre contexto recuperado\n",
    "- Impacto de RAG en costo de inferencia y técnicas para abaratarlo\n",
    "\n",
    "**Restricción clave**\n",
    "\n",
    "Este cuaderno evita el uso de APIs propietarias con credenciales (por ejemplo, OpenAI).  \n",
    "Las opciones de LLM son locales:\n",
    "\n",
    "1) Ollama (servidor local + modelos descargados)  \n",
    "2) Transformers en modo offline (`local_files_only=True`)  \n",
    "3) MockLLM determinista (para que el cuaderno corra incluso sin modelos)\n",
    "\n",
    "Nota: si deseas \"offline estricto\", pre-descarga modelos y ejecuta este cuaderno con `local_files_only=True` (ya está así por defecto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Instalación (opcional)\n",
    "# En entornos limpios (Docker/VM), descomenta e instala.\n",
    "\n",
    "# !pip install -U \"requests>=2.31\" \"pydantic>=2.6\" \"fastapi>=0.110\" \"uvicorn>=0.27\" \\\n",
    "#     \"prometheus-client>=0.20\" \"opentelemetry-api>=1.24\" \"opentelemetry-sdk>=1.24\" \\\n",
    "#     \"numpy>=1.26\" \"scikit-learn>=1.4\"\n",
    "\n",
    "# Opcional (embeddings densos):\n",
    "# !pip install -U \"sentence-transformers>=3.0\" \"torch>=2.2\"\n",
    "\n",
    "# Opcional (LLM local con Transformers, offline si el modelo ya existe):\n",
    "# !pip install -U \"transformers>=4.41\" \"sentencepiece>=0.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5744730",
   "metadata": {},
   "source": [
    "#### **Backends LLM locales (Ollama/Transformers offline/Mock)**\n",
    "\n",
    "La interfaz `LLM.generate(prompt, system=None)` unifica backends.\n",
    "\n",
    "- Ollama: requiere `ollama serve` y un modelo local (`ollama pull llama3.1:8b` u otro).\n",
    "- Transformers: se ejecuta sin internet usando `local_files_only=True` (si el modelo ya existe en cache o volumen).\n",
    "- MockLLM: simula tool-calling por reglas (útil para reproducibilidad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9010e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "class LLMBase:\n",
    "    def generate(self, prompt: str, system: Optional[str] = None, max_new_tokens: int = 256) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class OllamaLLM(LLMBase):\n",
    "    def __init__(self, model: str = \"llama3.1:8b\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "\n",
    "    def _is_up(self) -> bool:\n",
    "        try:\n",
    "            r = requests.get(f\"{self.base_url}/api/tags\", timeout=1.0)\n",
    "            return r.status_code == 200\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def generate(self, prompt: str, system: Optional[str] = None, max_new_tokens: int = 256) -> str:\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt if system is None else f\"{system}\\n\\n{prompt}\",\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"num_predict\": max_new_tokens},\n",
    "        }\n",
    "        r = requests.post(f\"{self.base_url}/api/generate\", json=payload, timeout=120.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"response\", \"\")\n",
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline  # type: ignore\n",
    "    _HAS_TRANSFORMERS = True\n",
    "except Exception:\n",
    "    AutoTokenizer = None  # type: ignore\n",
    "    AutoModelForSeq2SeqLM = None  # type: ignore\n",
    "    pipeline = None  # type: ignore\n",
    "    _HAS_TRANSFORMERS = False\n",
    "if _HAS_TRANSFORMERS:\n",
    "    class TransformersLLM(LLMBase):\n",
    "        \"\"\"Backend local para Transformers.\n",
    "\n",
    "        - Corre en CPU (device=-1)\n",
    "        - Soporta modo offline estricto: no descarga nada si offline=True\n",
    "        - Requiere que el modelo ya exista en cache/volumen local\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model_name_or_path: str = \"google/flan-t5-small\", offline: bool = True):\n",
    "            if offline:\n",
    "                os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "                os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name_or_path,\n",
    "                local_files_only=offline,\n",
    "            )\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name_or_path,\n",
    "                local_files_only=offline,\n",
    "            )\n",
    "\n",
    "            # Importante: NO pasar local_files_only al pipeline.\n",
    "            self.pipe = pipeline(\n",
    "                task=\"text2text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=-1,\n",
    "                framework=\"pt\",\n",
    "            )\n",
    "\n",
    "        def generate(self, prompt: str, system: Optional[str] = None, max_new_tokens: int = 256) -> str:\n",
    "            text = prompt if system is None else f\"{system}\\n\\n{prompt}\"\n",
    "            out = self.pipe(text, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "            return out[0][\"generated_text\"]\n",
    "else:\n",
    "    TransformersLLM = None  # type: ignore\n",
    "\n",
    "\n",
    "class MockLLM(LLMBase):\n",
    "    \"\"\"LLM de respaldo determinista.\n",
    "\n",
    "    Objetivo:\n",
    "    - Permitir que el cuaderno corra \"Restart + Run All\" aun sin Ollama/Transformers.\n",
    "    - Soportar demos de tool-calling (una llamada -> observación -> respuesta final).\n",
    "    - Soportar demos RAG (respuesta basada en CONTEXTO con citas mínimas).\n",
    "    \"\"\"\n",
    "\n",
    "    def _extract_obs(self, text: str) -> Dict[str, Any]:\n",
    "        # Intento 1: bloque \"Observación actual (JSON): { ... }\"\n",
    "        m = re.search(r\"Observación actual \\(JSON\\):\\s*(\\{.*?\\})\\s*\\n\\nHerramientas disponibles:\", text, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Intento 2: cualquier JSON que contenga last_tool/last_result\n",
    "        m = re.search(r\"(\\{.*\\\"last_tool\\\".*\\})\", text, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return {}\n",
    "\n",
    "    def generate(self, prompt: str, system: Optional[str] = None, max_new_tokens: int = 256) -> str:\n",
    "        text = (system or \"\") + \"\\n\" + prompt\n",
    "\n",
    "        # 1) Si ya existe una observación con resultado de herramienta, finalizar.\n",
    "        obs = self._extract_obs(text)\n",
    "        if isinstance(obs, dict) and \"last_tool\" in obs and \"last_result\" in obs:\n",
    "            tool = obs.get(\"last_tool\")\n",
    "            res = obs.get(\"last_result\", {})\n",
    "            # Normaliza errores de herramienta\n",
    "            if isinstance(res, dict) and (\"tool_error\" in res or \"error\" in res):\n",
    "                return json.dumps({\"final\": f\"Hubo un problema al ejecutar {tool}: {res}\"}, ensure_ascii=False)\n",
    "\n",
    "            if tool == \"flight_lookup\" and isinstance(res, dict):\n",
    "                options = res.get(\"options\") or res.get(\"flights\") or []\n",
    "                if options:\n",
    "                    # Resumen compacto\n",
    "                    top = options[0]\n",
    "                    return json.dumps({\"final\": f\"Opciones de vuelo encontradas (ejemplo): {top}. Si quieres, pido más opciones.\"}, ensure_ascii=False)\n",
    "                return json.dumps({\"final\": \"No encontré opciones de vuelo con los parámetros dados.\"}, ensure_ascii=False)\n",
    "\n",
    "            if tool == \"hotel_lookup\" and isinstance(res, dict):\n",
    "                hotels = res.get(\"hotels\") or res.get(\"options\") or []\n",
    "                if hotels:\n",
    "                    return json.dumps({\"final\": f\"Hoteles sugeridos (ejemplo): {hotels[:2]}.\"}, ensure_ascii=False)\n",
    "                return json.dumps({\"final\": \"No encontré hoteles con los parámetros dados.\"}, ensure_ascii=False)\n",
    "\n",
    "            if tool == \"calculator\" and isinstance(res, dict):\n",
    "                if \"value\" in res:\n",
    "                    return json.dumps({\"final\": f\"Resultado: {res['value']}\"}, ensure_ascii=False)\n",
    "\n",
    "            if tool == \"rag_search\" and isinstance(res, dict):\n",
    "                passages = res.get(\"passages\") or []\n",
    "                if passages:\n",
    "                    pid = passages[0].get(\"id\", \"doc\")\n",
    "                    return json.dumps({\"final\": f\"Según el contexto recuperado [{pid}], la respuesta depende de la evidencia disponible. Si falta evidencia, debo decirlo explícitamente.\"}, ensure_ascii=False)\n",
    "\n",
    "        # 2) Demos RAG: si el prompt incluye CONTEXTO con citas, generar respuesta corta con citas.\n",
    "        if \"CONTEXTO:\" in text and re.search(r\"\\[[^\\]]+\\]\", text):\n",
    "            # toma hasta 2 ids de cita\n",
    "            ids = re.findall(r\"\\[([^\\]]+)\\]\", text)\n",
    "            ids = [i for i in ids if i and not i.lower().startswith(\"http\")]\n",
    "            cite = \"\"\n",
    "            if ids:\n",
    "                cite = f\" [{ids[0]}]\" + (f\" [{ids[1]}]\" if len(ids) > 1 else \"\")\n",
    "            return f\"Respuesta basada en el contexto recuperado.{cite}\"\n",
    "\n",
    "        # 3) Tool selection (primera iteración)\n",
    "        if re.search(r\"\\b(vuelo|flight)\\b\", text, re.IGNORECASE):\n",
    "            return json.dumps({\"tool\": \"flight_lookup\", \"args\": {\"departure_city\": \"LAX\", \"destination_city\": \"JFK\", \"num_options\": 3}}, ensure_ascii=False)\n",
    "        if re.search(r\"\\b(hotel|alojamiento)\\b\", text, re.IGNORECASE):\n",
    "            return json.dumps({\"tool\": \"hotel_lookup\", \"args\": {\"city\": \"San Francisco\", \"num_options\": 3}}, ensure_ascii=False)\n",
    "        if re.search(r\"\\b(calcula|calculator|sum|add|multiplica|divide)\\b\", text, re.IGNORECASE):\n",
    "            return json.dumps({\"tool\": \"calculator\", \"args\": {\"expression\": \"2*(7+5)\"}}, ensure_ascii=False)\n",
    "\n",
    "        return json.dumps({\"final\": \"No tengo evidencia suficiente. Usa RAG o proporciona contexto.\"}, ensure_ascii=False)\n",
    "\n",
    "def pick_llm() -> LLMBase:\n",
    "    # 1) Ollama si está disponible\n",
    "    try:\n",
    "        ollama = OllamaLLM(model=os.environ.get(\"OLLAMA_MODEL\", \"llama3.1:8b\"))\n",
    "        if ollama._is_up():\n",
    "            print(\"Usando backend: Ollama\")\n",
    "            return ollama\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Transformers offline estricto (opcional)\n",
    "    if TransformersLLM is not None:\n",
    "        try:\n",
    "            model = os.environ.get(\"LOCAL_TFM_MODEL\", \"google/flan-t5-small\")\n",
    "            llm = TransformersLLM(model_name_or_path=model, offline=True)\n",
    "            print(\"Usando backend: Transformers (offline=True)\")\n",
    "            return llm\n",
    "        except Exception as e:\n",
    "            print(\"Transformers no disponible en modo offline. Error:\", type(e).__name__, str(e)[:160])\n",
    "\n",
    "    # 3) Mock\n",
    "    print(\"Usando backend: MockLLM (determinista)\")\n",
    "    return MockLLM()\n",
    "\n",
    "LLM = pick_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2514864",
   "metadata": {},
   "source": [
    "#### **Primitivas de Workflow: preprocess -> LLM -> postprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3145ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "class Step(Protocol):\n",
    "    name: str\n",
    "    def run(self, ctx: Dict[str, Any]) -> Dict[str, Any]: ...\n",
    "\n",
    "@dataclass\n",
    "class FunctionStep:\n",
    "    name: str\n",
    "    fn: Callable[[Dict[str, Any]], Dict[str, Any]]\n",
    "    def run(self, ctx: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        return self.fn(ctx)\n",
    "\n",
    "@dataclass\n",
    "class Workflow:\n",
    "    name: str\n",
    "    steps: List[Step]\n",
    "    def run(self, ctx: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        ctx = dict(ctx)\n",
    "        ctx.setdefault(\"trace\", [])\n",
    "        for s in self.steps:\n",
    "            t0 = time.time()\n",
    "            ctx[\"trace\"].append({\"step\": s.name, \"event\": \"start\", \"ts\": t0})\n",
    "            ctx = s.run(ctx)\n",
    "            ctx[\"trace\"].append({\"step\": s.name, \"event\": \"end\", \"ts\": time.time(), \"dt_ms\": (time.time()-t0)*1000})\n",
    "        return ctx\n",
    "\n",
    "def basic_preprocess(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def basic_postprocess(text: str) -> str:\n",
    "    return text.strip()\n",
    "\n",
    "def llm_step(ctx: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ctx[\"raw_llm\"] = LLM.generate(ctx[\"prompt\"], system=ctx.get(\"system\"), max_new_tokens=256)\n",
    "    return ctx\n",
    "\n",
    "wf = Workflow(\n",
    "    name=\"pre_llm_post\",\n",
    "    steps=[\n",
    "        FunctionStep(\"preprocess\", lambda c: {**c, \"prompt\": basic_preprocess(c[\"prompt\"])}),\n",
    "        FunctionStep(\"llm\", llm_step),\n",
    "        FunctionStep(\"postprocess\", lambda c: {**c, \"output\": basic_postprocess(c[\"raw_llm\"])}),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ctx = wf.run({\"prompt\": \"  Resume en 1 línea: Workflows de LLM para QA empresarial.   \"})\n",
    "ctx[\"output\"], ctx[\"trace\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bff8b2",
   "metadata": {},
   "source": [
    "#### **Multi-step prompting y chain-of-actions: tool calls en JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool registry + routing\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Tool:\n",
    "    name: str\n",
    "    description: str\n",
    "    fn: Callable[..., Any]\n",
    "    schema: Dict[str, Any]\n",
    "\n",
    "class ToolError(Exception):\n",
    "    pass\n",
    "\n",
    "TOOLS: Dict[str, Tool] = {}\n",
    "\n",
    "\n",
    "# Punto único de ejecución: permite activar guardrails sin reescribir el loop.\n",
    "def default_tool_execute(tool_name: str, args: Dict[str, Any]) -> Any:\n",
    "    return TOOLS[tool_name].fn(**args)\n",
    "\n",
    "TOOL_EXECUTOR: Callable[[str, Dict[str, Any]], Any] = default_tool_execute\n",
    "\n",
    "def register_tool(tool: Tool) -> None:\n",
    "    TOOLS[tool.name] = tool\n",
    "\n",
    "def safe_json_loads(text: str) -> Dict[str, Any]:\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON object found in text.\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "def tool_call_loop(user_request: str, system: str, max_steps: int = 6) -> Dict[str, Any]:\n",
    "    transcript: List[Dict[str, Any]] = []\n",
    "    obs = {\"user_request\": user_request}\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        tool_names = \", \".join(sorted(TOOLS.keys()))\n",
    "        prompt = (\n",
    "            \"Solicitud del usuario:\\n\"\n",
    "            f\"{user_request}\\n\\n\"\n",
    "            \"Observación actual (JSON):\\n\"\n",
    "            f\"{json.dumps(obs, ensure_ascii=False)}\\n\\n\"\n",
    "            f\"Herramientas disponibles: {tool_names}\\n\\n\"\n",
    "            \"Responde SOLO con JSON en UNA de estas formas:\\n\"\n",
    "            '1) { \"tool\": \"<nombre>\", \"args\": { ... } }\\n'\n",
    "            '2) { \"final\": \"<respuesta final>\" }\\n'\n",
    "        )\n",
    "        raw = LLM.generate(prompt, system=system, max_new_tokens=256)\n",
    "        transcript.append({\"step\": step, \"llm_raw\": raw})\n",
    "\n",
    "        try:\n",
    "            decision = safe_json_loads(raw)\n",
    "        except Exception as e:\n",
    "            transcript.append({\"parse_error\": str(e)})\n",
    "            return {\"status\": \"error\", \"error\": \"invalid_json\", \"transcript\": transcript}\n",
    "\n",
    "        if \"final\" in decision:\n",
    "            return {\"status\": \"ok\", \"final\": decision[\"final\"], \"transcript\": transcript, \"obs\": obs}\n",
    "\n",
    "        tool_name = decision.get(\"tool\")\n",
    "        args = decision.get(\"args\", {})\n",
    "        if tool_name not in TOOLS:\n",
    "            return {\"status\": \"error\", \"error\": f\"Unknown tool: {tool_name}\", \"transcript\": transcript}\n",
    "        if not isinstance(args, dict):\n",
    "            return {\"status\": \"error\", \"error\": \"Tool args must be a dict.\", \"transcript\": transcript}\n",
    "\n",
    "        try:\n",
    "            result = TOOL_EXECUTOR(tool_name, args)\n",
    "        except Exception as e:\n",
    "            result = {\"tool_error\": type(e).__name__, \"message\": str(e)[:200]}\n",
    "        transcript.append({\"tool\": tool_name, \"args\": args, \"result\": result})\n",
    "        obs = {\"last_tool\": tool_name, \"last_result\": result}\n",
    "\n",
    "    return {\"status\": \"error\", \"error\": \"max_steps_exceeded\", \"transcript\": transcript}\n",
    "\n",
    "# system prompt para tool calling\n",
    "system_toolcaller = (\n",
    "    \"Eres un agente que decide qué herramienta usar.\\n\"\n",
    "    \"Debes seguir el formato JSON exactamente como se solicita.\\n\"\n",
    "    \"No incluyas texto extra.\\n\"\n",
    "    \"Usa herramientas cuando falten datos; sintetiza cuando ya estén disponibles.\"\n",
    ")\n",
    "\n",
    "# Tools locales (demostración)\n",
    "def calculator(expression: str) -> Dict[str, Any]:\n",
    "    if not re.fullmatch(r\"[0-9\\.\\+\\-\\*\\/\\(\\)\\s]+\", expression):\n",
    "        raise ToolError(\"Expression contains unsupported characters.\")\n",
    "    return {\"expression\": expression, \"value\": eval(expression, {\"__builtins__\": {}}, {})}\n",
    "\n",
    "register_tool(Tool(\n",
    "    name=\"calculator\",\n",
    "    description=\"Evalúa una expresión aritmética segura (solo + - * / paréntesis).\",\n",
    "    fn=calculator,\n",
    "    schema={\"type\":\"object\",\"properties\":{\"expression\":{\"type\":\"string\"}},\"required\":[\"expression\"]},\n",
    "))\n",
    "\n",
    "# Travel provider (demo local, sin dependencias externas)\n",
    "TRAVEL_PROVIDER_PATH = \"travel_provider.py\"\n",
    "SUPPORTED_LOCATIONS_PATH = \"supported_locations.json\"\n",
    "\n",
    "DEFAULT_LOCATIONS = {\n",
    "    \"airports\": [\"LAX\",\"JFK\",\"ORD\",\"ATL\",\"DFW\",\"DEN\",\"SEA\",\"SAN\"],\n",
    "    \"hotel_cities\": [\"New York\",\"Los Angeles\",\"Chicago\",\"Miami\",\"San Francisco\",\"Seattle\",\"San Diego\"],\n",
    "}\n",
    "\n",
    "def ensure_travel_provider_files() -> None:\n",
    "    # supported_locations.json\n",
    "    if not os.path.exists(SUPPORTED_LOCATIONS_PATH):\n",
    "        with open(SUPPORTED_LOCATIONS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(DEFAULT_LOCATIONS, f, indent=2)\n",
    "\n",
    "    # travel_provider.py (sin faker; solo stdlib)\n",
    "    if not os.path.exists(TRAVEL_PROVIDER_PATH):\n",
    "        TP_TEMPLATE = '# travel_provider.py (demo local, sin dependencias externas)\\nimport json\\nimport os\\nimport random\\nfrom datetime import datetime, timedelta\\n\\ndef load_supported_locations(default=None):\\n    default = default or {\\n        \"airports\": [\"LAX\",\"JFK\",\"ORD\",\"ATL\",\"DFW\",\"DEN\",\"SEA\",\"SAN\"],\\n        \"hotel_cities\": [\"New York\",\"Los Angeles\",\"Chicago\",\"Miami\",\"San Francisco\",\"Seattle\",\"San Diego\"],\\n    }\\n    json_file = os.path.join(os.path.dirname(__file__), \"supported_locations.json\")\\n    if not os.path.exists(json_file):\\n        return default\\n    with open(json_file, \"r\", encoding=\"utf-8\") as f:\\n        try:\\n            return json.load(f)\\n        except Exception:\\n            return default\\n\\nsupported_locations = load_supported_locations()\\n\\ndef _rand_dt(start: datetime, end: datetime) -> datetime:\\n    if end <= start:\\n        return start\\n    delta = int((end - start).total_seconds())\\n    return start + timedelta(seconds=random.randint(0, delta))\\n\\nclass TravelProvider:\\n    def flight_lookup(self, departure_city, destination_city, num_options=3):\\n        if departure_city not in supported_locations[\"airports\"]:\\n            return {\"error\": f\"Unsupported departure city: {departure_city}. Supported airports are {supported_locations[\\'airports\\']}\"}\\n        if destination_city not in supported_locations[\"airports\"]:\\n            return {\"error\": f\"Unsupported destination city: {destination_city}. Supported airports are {supported_locations[\\'airports\\']}\"}\\n        if departure_city == destination_city:\\n            return {\"error\": \"Departure and destination cities cannot be the same.\"}\\n\\n        flights = []\\n        now = datetime.utcnow()\\n        for _ in range(int(num_options)):\\n            airline = random.choice([\"Delta\",\"United\",\"Southwest\",\"JetBlue\",\"American Airlines\"])\\n            flight_number = f\"{random.choice([\\'DL\\',\\'UA\\',\\'SW\\',\\'JB\\',\\'AA\\'])}{random.randint(100,9999)}\"\\n            dep = _rand_dt(now, now + timedelta(days=30))\\n            arr = _rand_dt(dep + timedelta(hours=2), dep + timedelta(hours=12))\\n            price = round(random.uniform(100, 300), 2)\\n            flights.append({\\n                \"airline\": airline,\\n                \"departure_airport\": departure_city,\\n                \"destination_airport\": destination_city,\\n                \"flight_number\": flight_number,\\n                \"departure_time\": dep.isoformat(),\\n                \"arrival_time\": arr.isoformat(),\\n                \"price\": price,\\n            })\\n        return {\"status_code\": 200, \"flight_options\": flights}\\n\\n    def hotel_lookup(self, city, num_options=3):\\n        if city not in supported_locations[\"hotel_cities\"]:\\n            return {\"error\": f\"Unsupported city: {city}. Supported cities are {supported_locations[\\'hotel_cities\\']}\"}\\n\\n        hotels = []\\n        now = datetime.utcnow()\\n        for _ in range(int(num_options)):\\n            hotel_name = random.choice([\"Hilton\",\"Marriott\",\"Hyatt\",\"Holiday Inn\",\"Sheraton\"])\\n            check_in = _rand_dt(now, now + timedelta(days=30))\\n            nights = random.randint(1, 7)\\n            check_out = check_in + timedelta(days=nights)\\n            price_per_night = round(random.uniform(100, 500), 2)\\n            total_price = round(price_per_night * nights, 2)\\n            hotels.append({\\n                \"hotel_name\": hotel_name,\\n                \"city\": city,\\n                \"check_in\": check_in.isoformat(),\\n                \"check_out\": check_out.isoformat(),\\n                \"price_per_night\": price_per_night,\\n                \"total_price\": total_price,\\n            })\\n        return hotels\\n\\ntravel_provider = TravelProvider()\\n'\n",
    "        with open(TRAVEL_PROVIDER_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(TP_TEMPLATE)\n",
    "\n",
    "ensure_travel_provider_files()\n",
    "\n",
    "try:\n",
    "    from travel_provider import travel_provider  # type: ignore\n",
    "except Exception as e:\n",
    "    # Fallback 100% local para que el cuaderno quede 'verde' aun sin archivo externo.\n",
    "    print(\"No se pudo importar travel_provider. Se activará fallback local. Error:\", type(e).__name__, str(e)[:120])\n",
    "    class _DummyTravelProvider:\n",
    "        def flight_lookup(self, departure_city: str, destination_city: str, num_options: int = 3) -> Dict[str, Any]:\n",
    "            return {\n",
    "                \"query\": {\"from\": departure_city, \"to\": destination_city},\n",
    "                \"options\": [\n",
    "                    {\"airline\": \"LocalAir\", \"from\": departure_city, \"to\": destination_city, \"price_usd\": 320 + 10*i, \"stops\": i%2}\n",
    "                    for i in range(max(1, int(num_options)))\n",
    "                ],\n",
    "            }\n",
    "        def hotel_lookup(self, city: str, num_options: int = 3) -> Dict[str, Any]:\n",
    "            return {\n",
    "                \"query\": {\"city\": city},\n",
    "                \"hotels\": [\n",
    "                    {\"name\": f\"Hotel {city} {i+1}\", \"price_usd\": 120 + 15*i, \"rating\": 4.0 + 0.1*(i%5)}\n",
    "                    for i in range(max(1, int(num_options)))\n",
    "                ],\n",
    "            }\n",
    "    travel_provider = _DummyTravelProvider()\n",
    "\n",
    "def flight_lookup(departure_city: str, destination_city: str, num_options: int = 3) -> Dict[str, Any]:\n",
    "    if travel_provider is None:\n",
    "        raise ToolError(\"travel_provider no está disponible.\")\n",
    "    return travel_provider.flight_lookup(departure_city, destination_city, num_options=num_options)\n",
    "\n",
    "def hotel_lookup(city: str, num_options: int = 3) -> Any:\n",
    "    if travel_provider is None:\n",
    "        raise ToolError(\"travel_provider no está disponible.\")\n",
    "    return travel_provider.hotel_lookup(city, num_options=num_options)\n",
    "\n",
    "\n",
    "# Registrar herramientas de viaje\n",
    "register_tool(Tool(\n",
    "    name=\"flight_lookup\",\n",
    "    description=\"Busca opciones de vuelo (demo local; devuelve opciones simuladas o del provider local).\",\n",
    "    fn=flight_lookup,\n",
    "    schema={\"type\":\"object\",\"properties\":{\"departure_city\":{\"type\":\"string\"},\"destination_city\":{\"type\":\"string\"},\"num_options\":{\"type\":\"integer\"}},\"required\":[\"departure_city\",\"destination_city\"]},\n",
    "))\n",
    "\n",
    "register_tool(Tool(\n",
    "    name=\"hotel_lookup\",\n",
    "    description=\"Busca opciones de hotel (demo local; devuelve hoteles simulados o del provider local).\",\n",
    "    fn=hotel_lookup,\n",
    "    schema={\"type\":\"object\",\"properties\":{\"city\":{\"type\":\"string\"},\"num_options\":{\"type\":\"integer\"}},\"required\":[\"city\"]},\n",
    "))\n",
    "\n",
    "print(\"Tools registrados:\", sorted(TOOLS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ed329",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tool_call_loop(\n",
    "    user_request=\"Necesito un vuelo de LAX a JFK.\",\n",
    "    system=system_toolcaller,\n",
    "    max_steps=3,\n",
    ")\n",
    "res[\"status\"], res.get(\"final\"), res[\"transcript\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a188a",
   "metadata": {},
   "source": [
    "#### **RAG local (TF-IDF + opcional dense) y respuesta grounded con citas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eac022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 220, overlap: int = 40) -> List[str]:\n",
    "    text = basic_preprocess(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "DOCS = [\n",
    "    (\"doc1\",\n",
    "     \"Fundamentos de RAG (Retrieval-Augmented Generation): un sistema RAG separa el problema en tres etapas: \"\n",
    "     \"retrieve → read → generate. Primero recupera pasajes relevantes (sparse, dense o híbrido), luego arma un \"\n",
    "     \"contexto y finalmente genera una respuesta. RAG reduce alucinaciones al exigir que las afirmaciones se anclen \"\n",
    "     \"en evidencia recuperada; si el contexto no contiene soporte suficiente, el asistente debe declararlo explícitamente.\"),\n",
    "    (\"doc2\",\n",
    "     \"Chunking en RAG: la ingesta transforma documentos en chunks para embeddings/índices. El tamaño y solapamiento \"\n",
    "     \"controlan el trade-off entre precisión y cobertura. Chunks muy pequeños pierden coherencia; muy grandes \"\n",
    "     \"diluyen señal y encarecen el contexto. El chunking semántico intenta cortar por unidades discursivas (títulos, \"\n",
    "     \"párrafos, secciones) para mejorar recuperación y citas.\"),\n",
    "    (\"doc3\",\n",
    "     \"Retrievers: sparse (BM25) favorece coincidencia léxica; dense (embeddings) captura similitud semántica; híbrido \"\n",
    "     \"combina ambos con re-ranking. Un patrón común es: (i) recuperar top-k amplio, (ii) re-rank con cross-encoder, \"\n",
    "     \"(iii) seleccionar pocos chunks para el prompt. Métricas típicas: recall@k, MRR, nDCG; y evaluación end-to-end \"\n",
    "     \"con EM/F1 o LLM-as-a-judge (con controles de sesgo).\"),\n",
    "    (\"doc4\",\n",
    "     \"Workflows/pipelines de LLM: estructurar la ejecución como pasos (preprocesar → LLM → postprocesar) reduce \"\n",
    "     \"errores y mejora auditabilidad. El preprocesamiento normaliza entrada; el postprocesamiento valida formato \"\n",
    "     \"(JSON/tabla), aplica políticas (por ejemplo, límites de longitud) y puede ejecutar verificadores. \"\n",
    "     \"El tracing por step (start/end/error, latencia) es clave para depuración y observabilidad.\"),\n",
    "    (\"doc5\",\n",
    "     \"Agentes: un agente implementa un loop observar → razonar → actuar → observar. Actuar suele significar usar \"\n",
    "     \"herramientas (APIs, DB, vector DB, calculadora). Guardrails en agentes incluyen: validación de argumentos, \"\n",
    "     \"allowlists de herramientas, límites de presupuesto (pasos, k, contexto), y auditoría del loop (transcripts). \"\n",
    "     \"Un patrón avanzado es Manager+Workers con un Critic que revisa y pide correcciones.\"),\n",
    "    (\"doc6\",\n",
    "     \"Observabilidad aplicada a RAG y agentes: además de métricas (p95 latencia, errores por herramienta), se \"\n",
    "     \"registran logs estructurados de prompts, tamaño de contexto, ids citados, y trazas distribuidas por request \"\n",
    "     \"(trace_id). Esto permite relacionar calidad con costos (tokens/latencia) y detectar regresiones por cambios \"\n",
    "     \"en el corpus, embeddings o reglas de routing.\"),\n",
    "]\n",
    "\n",
    "CHUNKS: List[Tuple[str,str]] = []\n",
    "for doc_id, text in DOCS:\n",
    "    for j, ch in enumerate(chunk_text(text, chunk_size=170, overlap=30)):\n",
    "        CHUNKS.append((f\"{doc_id}#c{j}\", ch))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([c[1] for c in CHUNKS])\n",
    "\n",
    "def sparse_retrieve(query: str, k: int = 3) -> List[Tuple[str, float, str]]:\n",
    "    qv = vectorizer.transform([query])\n",
    "    scores = (X @ qv.T).toarray().ravel()\n",
    "    idx = np.argsort(-scores)[:k]\n",
    "    return [(CHUNKS[i][0], float(scores[i]), CHUNKS[i][1]) for i in idx]\n",
    "\n",
    "def build_context(passages: List[Tuple[str,float,str]]) -> str:\n",
    "    return \"\\n\".join([f\"[{cid}] {txt}\" for cid,_,txt in passages])\n",
    "\n",
    "def rag_answer(query: str, k: int = 3) -> Dict[str, Any]:\n",
    "    passages = sparse_retrieve(query, k=k)\n",
    "    context = build_context(passages)\n",
    "    system = (\n",
    "        \"Eres un asistente técnico.\\n\"\n",
    "        \"Reglas: responde usando SOLO el CONTEXTO provisto, incluye citas [doc#chunk].\\n\"\n",
    "        \"Si el contexto no contiene evidencia, di: 'No tengo evidencia suficiente en el contexto recuperado.'\"\n",
    "    )\n",
    "    prompt = f\"PREGUNTA:\\n{query}\\n\\nCONTEXTO:\\n{context}\\n\\nRESPUESTA (con citas):\\n\"\n",
    "    out = LLM.generate(prompt, system=system, max_new_tokens=220)\n",
    "    return {\"query\": query, \"context\": context, \"answer\": out}\n",
    "\n",
    "rag_answer(\"¿Qué costo extra introduce RAG y cómo se abarata?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0700f",
   "metadata": {},
   "source": [
    "#### **Agente RAG-first (observar->razonar->actuar->observar)**\n",
    "\n",
    "Este agente primero llama `rag_search` (herramienta) y luego sintetiza con citas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4653e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_search(query: str, k: int = 4) -> Dict[str, Any]:\n",
    "    passages = sparse_retrieve(query, k=k)\n",
    "    return {\"passages\": [{\"id\":cid, \"score\":score, \"text\":txt} for cid,score,txt in passages]}\n",
    "\n",
    "register_tool(Tool(\n",
    "    name=\"rag_search\",\n",
    "    description=\"Recupera pasajes relevantes del corpus local (RAG).\",\n",
    "    fn=rag_search,\n",
    "    schema={\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\"},\"k\":{\"type\":\"integer\"}},\"required\":[\"query\"]},\n",
    "))\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    goal: str\n",
    "    short_memory: List[Dict[str, Any]]\n",
    "    long_memory: List[Dict[str, Any]]\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def run(self, user_request: str, max_turns: int = 4) -> Dict[str, Any]:\n",
    "        state = AgentState(goal=user_request, short_memory=[], long_memory=[])\n",
    "        obs: Dict[str, Any] = {\"user_request\": user_request}\n",
    "\n",
    "        for t in range(max_turns):\n",
    "            action = self.reason(state, obs)\n",
    "            state.short_memory.append({\"turn\": t, \"obs\": obs, \"action\": action})\n",
    "\n",
    "            if action.get(\"type\") == \"final\":\n",
    "                return {\"status\": \"ok\", \"final\": action[\"content\"], \"memory\": state.short_memory}\n",
    "\n",
    "            if action.get(\"type\") == \"tool\":\n",
    "                tool = action[\"tool\"]\n",
    "                args = action[\"args\"]\n",
    "                try:\n",
    "                    result = TOOLS[tool].fn(**args)\n",
    "                except Exception as e:\n",
    "                    result = {\"tool_error\": type(e).__name__, \"message\": str(e)[:200]}\n",
    "                obs = {\"last_tool\": tool, \"last_result\": result}\n",
    "                continue\n",
    "\n",
    "            return {\"status\": \"error\", \"error\": \"unknown_action_type\", \"memory\": state.short_memory}\n",
    "\n",
    "        return {\"status\": \"error\", \"error\": \"max_turns_exceeded\", \"memory\": state.short_memory}\n",
    "\n",
    "    def reason(self, state: AgentState, obs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class RagAgent(BaseAgent):\n",
    "    def reason(self, state: AgentState, obs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if obs.get(\"last_tool\") == \"rag_search\":\n",
    "            passages = obs[\"last_result\"][\"passages\"]\n",
    "            context = \"\\n\".join([f\"[{p['id']}] {p['text']}\" for p in passages])\n",
    "            prompt = (\n",
    "                f\"PREGUNTA:\\n{state.goal}\\n\\n\"\n",
    "                f\"CONTEXTO:\\n{context}\\n\\n\"\n",
    "                \"Responde SOLO con información del contexto y añade citas [id].\\n\"\n",
    "                \"Si falta evidencia, di: 'No tengo evidencia suficiente en el contexto recuperado.'\\n\"\n",
    "            )\n",
    "            ans = LLM.generate(prompt, system=\"Eres un asistente RAG.\", max_new_tokens=220)\n",
    "            return {\"type\": \"final\", \"content\": ans}\n",
    "        return {\"type\": \"tool\", \"tool\": \"rag_search\", \"args\": {\"query\": state.goal, \"k\": 3}}\n",
    "\n",
    "rag_agent = RagAgent(\"rag_agent\")\n",
    "rag_agent.run(\"Explica cómo RAG reduce alucinaciones y qué costo extra añade.\", max_turns=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85492399",
   "metadata": {},
   "source": [
    "#### **Guardrails (validación + límites + auditoría)**\n",
    "\n",
    "Se muestran guardrails mínimos:\n",
    "- Validación de argumentos (Pydantic)\n",
    "- Policy gate (reglas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError  # type: ignore\n",
    "\n",
    "# -------------------------\n",
    "# Modelos de validación\n",
    "# -------------------------\n",
    "\n",
    "class FlightArgs(BaseModel):\n",
    "    departure_city: str = Field(min_length=3, max_length=3, description=\"Código IATA (ej: LAX)\")\n",
    "    destination_city: str = Field(min_length=3, max_length=3, description=\"Código IATA (ej: JFK)\")\n",
    "    num_options: int = Field(default=3, ge=1, le=10)\n",
    "\n",
    "class HotelArgs(BaseModel):\n",
    "    city: str = Field(min_length=2, max_length=80)\n",
    "    num_options: int = Field(default=3, ge=1, le=10)\n",
    "\n",
    "class RagArgs(BaseModel):\n",
    "    query: str = Field(min_length=5, max_length=800)\n",
    "    k: int = Field(default=3, ge=1, le=8)\n",
    "\n",
    "class CalcArgs(BaseModel):\n",
    "    expression: str = Field(min_length=1, max_length=80)\n",
    "\n",
    "# -------------------------\n",
    "# Políticas (guardrails)\n",
    "# -------------------------\n",
    "\n",
    "def policy_allow(tool_name: str, args: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    # Ejemplo 1: regla obvia (evitar consulta sin sentido)\n",
    "    if tool_name == \"flight_lookup\" and args.get(\"departure_city\") == args.get(\"destination_city\"):\n",
    "        return False, \"Policy: departure_city must differ from destination_city.\"\n",
    "\n",
    "    # Ejemplo 2: evitar abuso de contexto (RAG)\n",
    "    if tool_name == \"rag_search\" and int(args.get(\"k\", 3)) > 8:\n",
    "        return False, \"Policy: k too large for rag_search.\"\n",
    "\n",
    "    # Ejemplo 3: (defensa adicional) negar expresiones sospechosas en calculator\n",
    "    if tool_name == \"calculator\":\n",
    "        exp = str(args.get(\"expression\", \"\"))\n",
    "        if any(tok in exp for tok in [\"__\", \"import\", \"open\", \"eval\", \"exec\"]):\n",
    "            return False, \"Policy: expression contains forbidden tokens.\"\n",
    "\n",
    "    return True, \"ok\"\n",
    "\n",
    "# -------------------------\n",
    "# Ejecución guardada\n",
    "# -------------------------\n",
    "\n",
    "def guarded_tool_execute(tool_name: str, args: Dict[str, Any]) -> Any:\n",
    "    # 1) Policy gating\n",
    "    ok, msg = policy_allow(tool_name, args)\n",
    "    if not ok:\n",
    "        raise ToolError(msg)\n",
    "\n",
    "    # 2) Validación por herramienta\n",
    "    try:\n",
    "        if tool_name == \"flight_lookup\":\n",
    "            args = FlightArgs(**args).model_dump()\n",
    "        elif tool_name == \"hotel_lookup\":\n",
    "            args = HotelArgs(**args).model_dump()\n",
    "        elif tool_name == \"rag_search\":\n",
    "            args = RagArgs(**args).model_dump()\n",
    "        elif tool_name == \"calculator\":\n",
    "            # valida longitud (la validación fuerte de caracteres está en la tool)\n",
    "            args = CalcArgs(**args).model_dump()\n",
    "    except ValidationError as e:\n",
    "        raise ToolError(f\"Args validation failed: {e.errors()[:1]}\")\n",
    "\n",
    "    # 3) Ejecución real (tool registry)\n",
    "    return TOOLS[tool_name].fn(**args)\n",
    "\n",
    "# Activación: el loop de tool-calling usará TOOL_EXECUTOR (definido en la sección de tools).\n",
    "# Esto mantiene el loop estable y permite encender/apagar guardrails sin reescribirlo.\n",
    "TOOL_EXECUTOR = guarded_tool_execute  # type: ignore[name-defined]\n",
    "\n",
    "# Demo rápido: debe bloquearse por política\n",
    "try:\n",
    "    guarded_tool_execute(\"flight_lookup\", {\"departure_city\":\"LAX\",\"destination_city\":\"LAX\",\"num_options\":3})\n",
    "except Exception as e:\n",
    "    print(\"Bloqueado:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f12a03",
   "metadata": {},
   "source": [
    "#### **Docker/offline (sin credenciales)**\n",
    "\n",
    "Recomendación:\n",
    "- Monta un volumen para caches (`HF_HOME` / `TRANSFORMERS_CACHE`) y mantén offline.\n",
    "- Pre-descarga modelos una vez (si aplica) y luego ejecuta con `local_files_only=True` (ya está así en este cuaderno)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a780a",
   "metadata": {},
   "source": [
    "#### **Patrones de workflow: secuencial, branching y paralelo**\n",
    "\n",
    "- **Secuencial**: pasos lineales (fácil de auditar, ideal en entornos regulados).\n",
    "- **Branching**: bifurcar según condición (por ejemplo,  intención del usuario, confianza, presencia de contexto).\n",
    "- **Paralelo**: ejecutar tareas independientes concurrentemente (p.ej. detección de idioma + extracción de entidades).\n",
    "\n",
    "Ejemplo didáctico: detectar `intent` y ejecutar ruta correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f989606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    if re.search(r\"\\b(el|la|los|las|pero|porque|entonces)\\b\", text.lower()):\n",
    "        return \"es\"\n",
    "    return \"en\"\n",
    "\n",
    "def classify_intent(text: str) -> str:\n",
    "    if re.search(r\"\\b(vuelo|flight)\\b\", text, re.IGNORECASE):\n",
    "        return \"travel_flight\"\n",
    "    if re.search(r\"\\b(hotel|alojamiento)\\b\", text, re.IGNORECASE):\n",
    "        return \"travel_hotel\"\n",
    "    if re.search(r\"\\b(rag|retriev)\\b\", text, re.IGNORECASE):\n",
    "        return \"rag_qa\"\n",
    "    return \"general_qa\"\n",
    "\n",
    "def route_request(user_text: str) -> dict:\n",
    "    with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "        futures = {ex.submit(detect_language, user_text): \"lang\",\n",
    "                   ex.submit(classify_intent, user_text): \"intent\"}\n",
    "        meta = {}\n",
    "        for f in as_completed(futures):\n",
    "            meta[futures[f]] = f.result()\n",
    "\n",
    "    intent = meta[\"intent\"]\n",
    "    if intent in (\"travel_flight\", \"travel_hotel\"):\n",
    "        return {\"route\": intent, \"meta\": meta, \"result\": tool_call_loop(user_text, system_toolcaller, max_steps=3)}\n",
    "    if intent == \"rag_qa\":\n",
    "        return {\"route\": intent, \"meta\": meta, \"result\": rag_answer(user_text, k=3)}\n",
    "    # fallback: LLM directo\n",
    "    ans = LLM.generate(f\"Responde con máximo 3 viñetas:\\n{user_text}\", system=\"Eres un asistente técnico.\", max_new_tokens=180)\n",
    "    return {\"route\": intent, \"meta\": meta, \"result\": {\"answer\": ans}}\n",
    "\n",
    "route_request(\"¿Cómo reduce RAG las alucinaciones y qué costo extra introduce?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ed3eb",
   "metadata": {},
   "source": [
    "#### **Integración con APIs y microservicios (local)**\n",
    "\n",
    "En escenarios reales, \"tools\" suelen estar detrás de HTTP/gRPC.  \n",
    "A continuación se levanta (opcional) un microservicio local con FastAPI que expone:\n",
    "\n",
    "- `/flight` (usa `flight_lookup`)\n",
    "- `/hotel` (usa `hotel_lookup`)\n",
    "- `/rag_search` (retrieval local)\n",
    "\n",
    "El objetivo es mostrar **tool use** vía red, sin credenciales externas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608b093",
   "metadata": {},
   "source": [
    "#### **Observabilidad del workflow (logs, métricas, trazas) - local**\n",
    "\n",
    "**Logs**\n",
    "- Incluir `trace_id`, `step`, `tool`, `latency_ms`, tamaños (`prompt_chars`, `context_chars`).\n",
    "\n",
    "**Métricas (Prometheus client)**\n",
    "- Contadores: tool calls por tool y status\n",
    "- Histograma: latencia de LLM y retrieval\n",
    "\n",
    "**Trazas (OpenTelemetry)**\n",
    "- Spans por etapa: preprocess, retrieve, generate, validate\n",
    "\n",
    "A continuación, una instrumentación mínima local (sin `collectors` externos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: si no tienes fastapi/uvicorn, instala:\n",
    "# !pip install -U fastapi uvicorn\n",
    "\n",
    "import threading\n",
    "\n",
    "def start_tool_server(host=\"127.0.0.1\", port=8088):\n",
    "    from fastapi import FastAPI\n",
    "    from pydantic import BaseModel\n",
    "    import uvicorn\n",
    "\n",
    "    app = FastAPI(title=\"Local Tools API\")\n",
    "\n",
    "    class FlightReq(BaseModel):\n",
    "        departure_city: str\n",
    "        destination_city: str\n",
    "        num_options: int = 3\n",
    "\n",
    "    class HotelReq(BaseModel):\n",
    "        city: str\n",
    "        num_options: int = 3\n",
    "\n",
    "    class RagReq(BaseModel):\n",
    "        query: str\n",
    "        k: int = 4\n",
    "\n",
    "    @app.post(\"/flight\")\n",
    "    def _flight(req: FlightReq):\n",
    "        return flight_lookup(req.departure_city, req.destination_city, req.num_options)\n",
    "\n",
    "    @app.post(\"/hotel\")\n",
    "    def _hotel(req: HotelReq):\n",
    "        return hotel_lookup(req.city, req.num_options)\n",
    "\n",
    "    @app.post(\"/rag_search\")\n",
    "    def _rag(req: RagReq):\n",
    "        return rag_search(req.query, req.k)\n",
    "\n",
    "    config = uvicorn.Config(app, host=host, port=port, log_level=\"warning\")\n",
    "    server = uvicorn.Server(config)\n",
    "\n",
    "    th = threading.Thread(target=server.run, daemon=True)\n",
    "    th.start()\n",
    "    return {\"host\": host, \"port\": port, \"thread\": th}\n",
    "\n",
    "# Descomenta para iniciar:\n",
    "# srv = start_tool_server()\n",
    "# srv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d3923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cliente HTTP (si levantaste el server arriba)\n",
    "def http_post(url: str, payload: dict) -> dict:\n",
    "    r = requests.post(url, json=payload, timeout=5.0)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# Ejemplo (requiere server):\n",
    "# http_post(\"http://127.0.0.1:8088/rag_search\", {\"query\":\"costo extra de RAG\", \"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: si no tienes prometheus_client / opentelemetry, instala:\n",
    "# !pip install -U prometheus-client opentelemetry-api opentelemetry-sdk\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "log = logging.getLogger(\"obs\")\n",
    "\n",
    "# Prometheus (opcional). Este bloque está escrito para poder re-ejecutarse en Jupyter\n",
    "# sin lanzar \"Duplicated timeseries\".\n",
    "try:\n",
    "    from prometheus_client import Counter, Histogram, REGISTRY  # type: ignore\n",
    "\n",
    "    def _get_or_create_collector(name: str, factory):\n",
    "        existing = getattr(REGISTRY, \"_names_to_collectors\", {}).get(name)  # type: ignore[attr-defined]\n",
    "        return existing if existing is not None else factory()\n",
    "\n",
    "    WF_RUNS = _get_or_create_collector(\n",
    "        \"wf_runs_total\",\n",
    "        lambda: Counter(\"wf_runs_total\", \"Ejecuciones de workflows\", [\"name\"]),\n",
    "    )\n",
    "    TOOL_CALLS = _get_or_create_collector(\n",
    "        \"tool_calls_total\",\n",
    "        lambda: Counter(\"tool_calls_total\", \"Llamadas a tools\", [\"tool\", \"status\"]),\n",
    "    )\n",
    "    LLM_LAT = _get_or_create_collector(\n",
    "        \"llm_latency_seconds\",\n",
    "        lambda: Histogram(\"llm_latency_seconds\", \"Latencia de LLM (s)\", buckets=(0.1,0.3,0.7,1.5,3,6,12)),\n",
    "    )\n",
    "    RETR_LAT = _get_or_create_collector(\n",
    "        \"retrieval_latency_seconds\",\n",
    "        lambda: Histogram(\"retrieval_latency_seconds\", \"Latencia de retrieval (s)\", buckets=(0.01,0.03,0.1,0.3,0.7,1.5)),\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    # Fallback: métricas no-op (para que el notebook no falle si falta prometheus_client)\n",
    "    class _Noop:\n",
    "        def labels(self, *args, **kwargs): return self\n",
    "        def inc(self, *args, **kwargs): return None\n",
    "        def observe(self, *args, **kwargs): return None\n",
    "\n",
    "    WF_RUNS = _Noop()\n",
    "    TOOL_CALLS = _Noop()\n",
    "    LLM_LAT = _Noop()\n",
    "    RETR_LAT = _Noop()\n",
    "\n",
    "def obs_llm(prompt: str, system: str) -> str:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        return LLM.generate(prompt, system=system, max_new_tokens=220)\n",
    "    finally:\n",
    "        try:\n",
    "            LLM_LAT.observe(time.time() - t0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def obs_retrieve(query: str, k: int = 3):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        return sparse_retrieve(query, k=k)\n",
    "    finally:\n",
    "        try:\n",
    "            RETR_LAT.observe(time.time() - t0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def traced_rag_answer(query: str, k: int = 3) -> dict:\n",
    "    trace_id = f\"tr_{int(time.time()*1000)}\"\n",
    "    try:\n",
    "        WF_RUNS.labels(\"rag\").inc()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    passages = obs_retrieve(query, k=k)\n",
    "    context = build_context(passages)\n",
    "\n",
    "    log.info(json.dumps({\"event\":\"retrieve\", \"trace_id\":trace_id, \"k\":k, \"hits\":len(passages), \"context_chars\":len(context)}))\n",
    "    prompt = f\"PREGUNTA:\\n{query}\\n\\nCONTEXTO:\\n{context}\\n\\nRESPUESTA (con citas):\\n\"\n",
    "    ans = obs_llm(prompt, system=\"Eres un asistente RAG.\")\n",
    "    log.info(json.dumps({\"event\":\"generate\", \"trace_id\":trace_id, \"answer_chars\":len(ans)}))\n",
    "    return {\"trace_id\": trace_id, \"answer\": ans, \"context\": context}\n",
    "\n",
    "traced_rag_answer(\"¿Qué componentes tiene un workflow de LLM?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd29c7",
   "metadata": {},
   "source": [
    "#### **Workflows multi-agente (Manager + Workers + Critic)**\n",
    "\n",
    "Patrón didáctico:\n",
    "\n",
    "- **Worker RAG**: obtiene contexto + redacta respuesta con citas.\n",
    "- **Critic/Reviewer**: valida grounding (\"¿hay citas?\", \"¿afirma sin evidencia?\"), y pide correcciones.\n",
    "- **Manager**: orquesta.\n",
    "\n",
    "El objetivo es mostrar *separación de responsabilidades* y auditoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab01c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def review(self, draft: str) -> str:\n",
    "        prompt = (\n",
    "            \"Revisa el borrador para:\\n\"\n",
    "            \"- evitar alucinaciones\\n\"\n",
    "            \"- exigir citas si afirma hechos\\n\"\n",
    "            \"- señalar incertidumbre si falta evidencia\\n\\n\"\n",
    "            f\"BORRADOR:\\n{draft}\\n\\n\"\n",
    "            \"Devuelve OK o una lista de problemas y cómo corregirlos.\"\n",
    "        )\n",
    "        return LLM.generate(prompt, system=\"Eres un revisor estricto.\", max_new_tokens=220)\n",
    "\n",
    "class Manager:\n",
    "    def __init__(self, max_corrections: int = 2):\n",
    "        self.max_corrections = max_corrections\n",
    "        self.critic = Critic()\n",
    "\n",
    "    def run(self, question: str) -> dict:\n",
    "        # Worker: RAG con contexto explícito para evitar inventar durante correcciones.\n",
    "        base = rag_answer(question, k=3)\n",
    "        draft = base[\"answer\"]\n",
    "        context = base[\"context\"]\n",
    "\n",
    "        last_review = \"\"\n",
    "        for i in range(self.max_corrections + 1):\n",
    "            last_review = self.critic.review(draft)\n",
    "            if last_review.strip().upper().startswith(\"OK\"):\n",
    "                return {\"draft\": draft, \"review\": last_review, \"iterations\": i + 1}\n",
    "\n",
    "            revise_prompt = f\"\"\"Corrige el borrador siguiendo el feedback.\n",
    "\n",
    "Reglas:\n",
    "- Usa SOLO el CONTEXTO (no agregues hechos externos).\n",
    "- Mantén o mejora las citas [doc#chunk] cuando hagas afirmaciones.\n",
    "- Si falta evidencia, dilo explícitamente.\n",
    "\n",
    "FEEDBACK:\n",
    "{last_review}\n",
    "\n",
    "CONTEXTO:\n",
    "{context}\n",
    "\n",
    "BORRADOR:\n",
    "{draft}\n",
    "\n",
    "BORRADOR CORREGIDO:\n",
    "\"\"\"\n",
    "            draft = LLM.generate(revise_prompt, system=\"Eres un editor técnico estricto.\", max_new_tokens=260)\n",
    "\n",
    "        return {\"draft\": draft, \"review\": last_review, \"iterations\": self.max_corrections + 1}\n",
    "\n",
    "mgr = Manager()\n",
    "mgr.run(\"Explica cómo RAG reduce alucinaciones y cómo afecta el costo, con citas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cc6b3",
   "metadata": {},
   "source": [
    "#### **Razonamiento sobre contexto recuperado (deductivo, inductivo, abductivo)**\n",
    "\n",
    "Una práctica útil en clases es pedir al agente que separe explícitamente:\n",
    "\n",
    "- **Evidencia**: citas [doc#chunk] (lo que está en el contexto)\n",
    "- **Inferencias deductivas**: consecuencias lógicas\n",
    "- **Inferencias inductivas**: generalizaciones (marcar incertidumbre)\n",
    "- **Inferencias abductivas**: mejor explicación plausible (marcar especulación)\n",
    "\n",
    "Ejemplo de prompt estructurado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_reasoning_template(question: str) -> dict:\n",
    "    passages = sparse_retrieve(question, k=3)\n",
    "    context = build_context(passages)\n",
    "    system = \"Eres un asistente técnico. No inventes.\"\n",
    "    prompt = (\n",
    "        f\"PREGUNTA:\\n{question}\\n\\n\"\n",
    "        f\"CONTEXTO:\\n{context}\\n\\n\"\n",
    "        \"Produce una respuesta con estas secciones:\\n\"\n",
    "        \"1) Evidencia (citas)\\n\"\n",
    "        \"2) Razonamiento deductivo\\n\"\n",
    "        \"3) Razonamiento inductivo (marca incertidumbre)\\n\"\n",
    "        \"4) Razonamiento abductivo (marca especulación)\\n\"\n",
    "    )\n",
    "    out = LLM.generate(prompt, system=system, max_new_tokens=260)\n",
    "    return {\"context\": context, \"answer\": out}\n",
    "\n",
    "rag_reasoning_template(\"¿Qué relación hay entre RAG y mitigación de alucinaciones?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534369e5",
   "metadata": {},
   "source": [
    "#### **RAG e impacto en optimización de inferencia (costo extra) y técnicas para abaratar**\n",
    "\n",
    "RAG añade costos de:\n",
    "- embeddings\n",
    "- búsqueda\n",
    "- contexto mayor (más tokens)\n",
    "\n",
    "Mitigaciones típicas:\n",
    "- caching (query embeddings, retrieval results)\n",
    "- bajar k + híbrido\n",
    "- dos etapas (retrieve -> rerank solo si es necesario)\n",
    "- chunking adecuado\n",
    "- embeddings pequeños/cuantiizados\n",
    "\n",
    "Ejemplo mínimo: cache del retrieval sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def cached_sparse(query: str, k: int = 3):\n",
    "    return tuple(sparse_retrieve(query, k=k))\n",
    "\n",
    "t0 = time.time(); _ = cached_sparse(\"costo extra de rag\", 3); t1 = time.time()\n",
    "t2 = time.time(); _ = cached_sparse(\"costo extra de rag\", 3); t3 = time.time()\n",
    "\n",
    "{\"first_ms\": (t1-t0)*1000, \"cached_ms\": (t3-t2)*1000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c207e12f-78cd-4b92-b5a9-032361f42eca",
   "metadata": {},
   "source": [
    "#### **Ejercicios propuestos para el cuaderno**\n",
    "\n",
    "**E1. Instrumentación mínima del workflow (traza útil)**\n",
    "\n",
    "**Tarea:** extiende `Workflow.run()` para registrar también:\n",
    "\n",
    "* `input_keys`, `output_keys`\n",
    "* `prompt_chars` (antes de LLM) y `output_chars` (después)\n",
    "* `error` si un step falla (sin romper toda la ejecución)\n",
    "\n",
    "**Criterio de éxito:** `ctx[\"trace\"]` contiene eventos `start/end/error` por step y el workflow retorna un `ctx` válido aun con error.\n",
    "\n",
    "**E2. Branching: ruta \"RAG obligatorio\" vs \"LLM directo\"**\n",
    "\n",
    "**Tarea:** implementa un step `route()` que:\n",
    "\n",
    "* si la pregunta contiene patrones de \"hecho\" (por ejemplo, \"¿qué costo...?\", \"¿qué afirma...?\") obligue RAG\n",
    "* si es \"creativo\" o \"resumen de estilo\" permita LLM directo\n",
    "\n",
    "**Criterio de éxito:** imprime `route=rag` o `route=direct` y la salida se genera por la ruta correcta.\n",
    "\n",
    "**E3. Paralelo: extracción de metadatos + retrieval**\n",
    "\n",
    "**Tarea:** ejecuta en paralelo:\n",
    "\n",
    "1. `classify_intent(text)`\n",
    "2. `sparse_retrieve(query,k)`\n",
    "   y luego combina resultados en un solo `ctx`.\n",
    "\n",
    "**Criterio de éxito:** el tiempo total medido es menor (o igual) que hacerlo secuencialmente y `ctx` contiene `intent` y `passages`.\n",
    "\n",
    "**E4. Prompting multi-step: plan -> respuesta -> verificación**\n",
    "\n",
    "**Tarea:** crea 3 steps:\n",
    "\n",
    "1. `plan_step`: produce una lista corta de pasos (máx. 5)\n",
    "2. `answer_step`: responde usando el plan\n",
    "3. `verify_step`: revisa que haya citas si se usó RAG y que no existan afirmaciones sin evidencia\n",
    "\n",
    "**Criterio de éxito:** el output final incluye (a) plan, (b) respuesta, (c) verificación \"OK\" o lista de problemas.\n",
    "\n",
    "**E5. Tool routing: relacional vs vectorial vs API**\n",
    "\n",
    "**Tarea:** implementa una función `choose_tools(question)` que devuelva una lista ordenada de tools candidatas:\n",
    "\n",
    "* DB relacional (simulada): para consultas con estructura (\"estado=\", \"id=\", \"fecha=\")\n",
    "* vector DB (tu `rag_search`): para preguntas semánticas\n",
    "* API interna (FastAPI local): para acciones (flight/hotel)\n",
    "\n",
    "**Criterio de éxito:** para 6 preguntas de prueba, el router elige razonablemente la(s) herramienta(s).\n",
    "\n",
    "**E6. Guardrails: validación fuerte + allowlist + budgets**\n",
    "\n",
    "**Tarea:** añade:\n",
    "\n",
    "* allowlist por tool (`{\"rag_search\",\"calculator\",\"flight_lookup\",\"hotel_lookup\"}`)\n",
    "* límite de pasos del agente + **límite de herramientas por tipo** (por ejemplo,  máximo 2 llamadas a APIs por turno)\n",
    "* validación Pydantic para al menos 2 tools (por ejemplo, `rag_search`, `calculator`)\n",
    "\n",
    "**Criterio de éxito:** si el LLM pide una tool no permitida o excede límites, el agente retorna una respuesta segura (rechazo con motivo) y queda auditado.\n",
    "\n",
    "**E7. Observabilidad: métricas Prometheus que no rompan al re-ejecutar**\n",
    "\n",
    "**Tarea:** implementa un helper `get_metric(name, factory)` que:\n",
    "\n",
    "* si la métrica ya existe en el registry, reutilice la existente\n",
    "* si no existe, la cree\n",
    "\n",
    "**Criterio de éxito:** puedes re-ejecutar la celda de métricas sin error y los contadores siguen incrementándose.\n",
    "\n",
    "**E8. RAG con citas obligatorias y \"no sé\" controlado**\n",
    "\n",
    "**Tarea:** modifica `rag_answer()` para:\n",
    "\n",
    "* exigir que cada párrafo tenga al menos una cita `[doc#chunk]`, o bien devolver \"No tengo evidencia suficiente...\"\n",
    "* agregar una sección \"Evidencia usada:\" listando los `chunk_id` utilizados\n",
    "\n",
    "**Criterio de éxito:** en preguntas fuera del corpus, devuelve \"No tengo evidencia...\". En preguntas dentro, devuelve citas consistentes.\n",
    "\n",
    "**E9. Razonamiento sobre contexto: separar evidencia vs inferencia**\n",
    "\n",
    "**Tarea:** crea un prompt/plantilla que produzca 4 secciones:\n",
    "\n",
    "1. Evidencia (solo citas)\n",
    "2. Deductivo (derivable)\n",
    "3. Inductivo (marcar incertidumbre)\n",
    "4. Abductivo (marcar hipótesis)\n",
    "\n",
    "**Criterio de éxito:** la salida marca explícitamente incertidumbre (\"posiblemente\", \"hipótesis\") y nunca presenta abductivo como hecho.\n",
    "\n",
    "**E10. Memoria de largo plazo (simple) con TTL y metadatos**\n",
    "\n",
    "**Tarea:** implementa un `MemoryStore` local con:\n",
    "\n",
    "* `put(item, ttl_seconds, tags={...})`\n",
    "* `search(query, k)` (puede ser TF-IDF sobre textos guardados)\n",
    "* limpieza de expirados\n",
    "\n",
    "**Criterio de éxito:** lo que expira deja de recuperarse; lo vigente sí aparece y se cita como \"memoria\".\n",
    "\n",
    "**E11. Multi-agente: Manager + Worker(RAG) + Critic con corrección iterativa**\n",
    "\n",
    "**Tarea:** el `Critic` debe:\n",
    "\n",
    "* rechazar si no hay citas cuando corresponde\n",
    "* pedir \"otra pasada de retrieval\" si scores son bajos\n",
    "  El `Manager` re-ejecuta el Worker hasta 2 veces.\n",
    "\n",
    "**Criterio de éxito:** se ve un bucle de corrección: `draft1 -> review -> draft2 (mejorado)`.\n",
    "\n",
    "**E12. Costos de RAG: presupuesto de contexto + caching**\n",
    "\n",
    "**Tarea:** agrega un \"presupuesto\" `max_context_chars`:\n",
    "\n",
    "* si el contexto supera el presupuesto, reduce k o trunca por chunks con mayor score\n",
    "* cachea retrieval por query, con invalidación por \"versión de corpus\"\n",
    "\n",
    "**Criterio de éxito:** el contexto nunca excede el límite y el caching reduce latencia en la segunda ejecución.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a39926-afed-47ed-89a2-f3b0b51241dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
