{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d98984",
   "metadata": {},
   "source": [
    "### **Búsqueda Semántica y RAG local (FastAPI + LangGraph + GraphRAG + LATS)**\n",
    "\n",
    "Todo el flujo se basa en modelos y servicios **locales** y en frameworks modernos:\n",
    "\n",
    "- Embeddings locales con `sentence-transformers`.\n",
    "- Vector store local con FAISS **y opción de Qdrant** como base de datos vectorial.\n",
    "- LLM local tipo **Llama** servido vía **Ollama**.\n",
    "- RAG básico con LangChain.\n",
    "- RAG como grafo con LangGraph.\n",
    "- Esqueleto de GraphRAG con **Neo4j**.\n",
    "- API HTTP con FastAPI + métricas para **Prometheus/Grafana**.\n",
    "- Esqueletos de LATS (Burr + LangGraph).\n",
    "\n",
    " Muchos bloques están en modo plantilla (`TODO` o \"esqueleto\") para que se completen y adapten a su propia infraestructura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e1738-b066-4323-9fd2-38ec85986f43",
   "metadata": {},
   "source": [
    "#### **0. Fundamentos: ¿por qué RAG y no solo *fine-tuning*?**\n",
    "\n",
    "En un escenario real partimos casi siempre de un LLM ya pre-entrenado. Para adaptarlo a un dominio concreto hay dos grandes estrategias:\n",
    "\n",
    "- **Fine-tuning clásico**: entrenar de nuevo el modelo (total o parcialmente) con ejemplos del nuevo dominio.\n",
    "- **RAG (Retrieval-Augmented Generation)**: dejar fijo el modelo y conectarlo a una base de conocimiento externa que se consulta en tiempo de inferencia.\n",
    "\n",
    "**Fine-tuning** es útil cuando:\n",
    "\n",
    "- queremos cambiar el \"comportamiento\" del modelo (estilo de respuesta, formato, instrucciones complejas).\n",
    "- disponemos de miles/millones de ejemplos limpios y etiquetados.\n",
    "- aceptamos pagar coste de cómputo y de mantenimiento del modelo afinado.\n",
    "\n",
    "Sus limitaciones:\n",
    "\n",
    "- actualizar conocimiento requiere volver a entrenar.\n",
    "- es difícil borrar o aislar información concreta (riesgos legales/cumplimiento).\n",
    "- puede degradar capacidades generales si el dataset es pequeño o sesgado.\n",
    "\n",
    "**RAG**, en cambio:\n",
    "\n",
    "- mantiene fijo el LLM base;\n",
    "- recupera documentos relevantes (*retrieve*) desde una base de conocimiento;\n",
    "- y genera la respuesta condicionada por ese contexto (*generate*).\n",
    "\n",
    "Ventajas de RAG:\n",
    "\n",
    "- actualizar el sistema = actualizar los documentos (sin re-entrenar el modelo);\n",
    "- se pueden devolver **citas** y trazabilidad de la fuente;\n",
    "- más barato de iterar para muchos proyectos \"data-centric\".\n",
    "\n",
    "Desventajas de RAG:\n",
    "\n",
    "- la calidad de la respuesta depende fuertemente del **retriever** y del **chunking**;\n",
    "- el contexto está limitado por la ventana del modelo;\n",
    "- hay que diseñar bien la base de conocimiento y los metadatos.\n",
    "\n",
    "En este cuaderno nos vamos a centrar en **RAG local** (FAISS/Qdrant + LLM vía Ollama) y en sus variantes (GraphRAG, agentes tipo LATS). El *fine-tuning* aparecerá como **complemento** posible, pero no como la herramienta principal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f3e08-5d41-405f-b8e5-856bd144c7db",
   "metadata": {},
   "source": [
    "#### **Ejercicio 1 - RAG vs *Fine-Tuning***\n",
    "\n",
    " Considera estos cuatro escenarios:\n",
    " \n",
    " 1. Un *chatbot* para soporte interno de una universidad, que debe responder preguntas sobre reglamentos y documentos que cambian cada semestre.\n",
    " 2. Un asistente de programación que debe aprender el estilo de código de una empresa concreta.\n",
    " 3. Un buscador de jurisprudencia legal donde cada respuesta debe venir acompañada de citas exactas a artículos y sentencias.\n",
    " 4. Un modelo que debe aprender a escribir poesía al estilo de un poeta específico.\n",
    " \n",
    " **Tareas:**\n",
    " - (a) Para cada escenario, indica si usarías **RAG**, **fine-tuning**, o una **combinación**. Justifica en 3-5 líneas.\n",
    " - (b) Elige uno de los escenarios donde hayas propuesto RAG y explica:\n",
    "   - Qué tipo de documentos indexarías.\n",
    "   - Cada cuánto actualizarías la base de conocimiento.\n",
    "   - Qué riesgos ves si sólo hicieras *fine-tuning* y no RAG.\n",
    " - (c) Redacta en una frase tu propia definición de \"RAG\" y otra de \"fine-tuning\" que un compañero de pregrado pueda entender sin leer el cuaderno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b398079-53ab-46ce-a9eb-f8e143de1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4ed79",
   "metadata": {},
   "source": [
    "#### **1. Requisitos de entorno**\n",
    "\n",
    "Instala estos paquetes en tu entorno local (o WSL/Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9505f7-689b-4fe2-9b55-2da18ae17c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "  sentence-transformers \\\n",
    "  faiss-cpu \\\n",
    "  neo4j \\\n",
    "  rank-bm25 \\\n",
    "  langchain \\\n",
    "  langchain-community \\\n",
    "  langchain-qdrant \\\n",
    "  qdrant-client \\\n",
    "  langgraph \\\n",
    "  \"fastapi[standard]\" uvicorn[standard] \\\n",
    "  prometheus-fastapi-instrumentator prometheus-client \\\n",
    "  \"burr[start]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de839a04",
   "metadata": {},
   "source": [
    "#### **2. Corpus de ejemplo**\n",
    "\n",
    "Pequeño corpus basado en *Interstellar*. Sustituye por tu propio conjunto de textos si lo deseas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f4f10-f76d-4711-b574-a895792b1806",
   "metadata": {},
   "source": [
    "**2.1 Pipeline de ingesta y *chunking* en un caso real**\n",
    "\n",
    "El \"corpus de ejemplo\" que usamos aquí (`texts = [...]`) simula el resultado de un pipeline más completo que, en producción, suele tener estos pasos:\n",
    "\n",
    "1. **Ingesta**: leer PDFs, páginas web, Markdown, etc.\n",
    "2. **Limpieza y normalización**: quitar ruido, headers, pie de página, código HTML innecesario, etc.\n",
    "3. **Segmentación en *chunks***:\n",
    "   - dividir los documentos en trozos de longitud razonable (por ejemplo 500-1000 caracteres);\n",
    "   - añadir un solapamiento (por ejemplo 50-100 caracteres) para no cortar ideas a la mitad.\n",
    "4. **Enriquecimiento con metadatos**: fuente, autor, fecha, idioma, tipo de documento, nivel de confidencialidad, etc.\n",
    "5. **Cálculo de embeddings + almacenamiento en una base vectorial**.\n",
    "\n",
    "En pseudo-código con LangChain podría verse así:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Ingesta: cargar PDFs desde una carpeta\n",
    "raw_docs = []\n",
    "for pdf_path in Path(\"data/\").glob(\"*.pdf\"):\n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    raw_docs.extend(loader.load())\n",
    "\n",
    "# 2-3. Chunking: dividir en trozos con solapamiento\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    ")\n",
    "chunked_docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# 4. Resultado listo para embeddings: cada chunk tiene page_content + metadata\n",
    "print(len(chunked_docs), chunked_docs[0].page_content[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512831d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus de ejemplo: lista de párrafos cortos\n",
    "texts = [\n",
    "    \"Interstellar is a science fiction film directed by Christopher Nolan. \"\n",
    "    \"It explores themes of space travel, black holes, and the survival of humanity.\",\n",
    "    \"The movie relies heavily on theoretical physics, including concepts such as wormholes, \"\n",
    "    \"time dilation, and higher-dimensional spaces.\",\n",
    "    \"The main character, Cooper, joins a mission through a wormhole near Saturn in search of \"\n",
    "    \"habitable planets for humans to colonize.\",\n",
    "    \"Interstellar's depiction of a rotating black hole, Gargantua, was praised for its scientific \"\n",
    "    \"accuracy thanks to physicist Kip Thorne.\",\n",
    "    \"Beyond the science, the film emphasizes emotional connections, especially between Cooper \"\n",
    "    \"and his daughter Murph.\",\n",
    "]\n",
    "\n",
    "len(texts), texts[0][:120]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56f2e0-17a9-437f-8c99-f6f6d45d4777",
   "metadata": {},
   "source": [
    "En este cuaderno usamos un corpus pequeño para centrarnos en las ideas, pero mentalmente puedes imaginar que la lista `texts` proviene de un pipeline como el anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5af2e1-05a5-4b2e-9dec-b7ef035a950f",
   "metadata": {},
   "source": [
    " #### **Ejercicio 2 - Comparando estrategias de *chunking***\n",
    " \n",
    " Supón que tienes un documento de ~20 páginas en PDF. Vas a probar dos configuraciones de *chunking*:\n",
    " \n",
    " - Configuración A: `chunk_size = 500`, `chunk_overlap = 50`\n",
    " - Configuración B: `chunk_size = 1200`, `chunk_overlap = 200`\n",
    " \n",
    " **Tareas (modificando el código de la sección 2.1):**\n",
    " 1. Implementa ambas configuraciones con `RecursiveCharacterTextSplitter`.\n",
    " 2. Para cada configuración, imprime:\n",
    "    - Número total de *chunks*.\n",
    "    - Longitud media de `page_content` (en caracteres).\n",
    " 3. Comenta:\n",
    "    - ¿Cuál configuración crees que es mejor para **preguntas muy específicas**?\n",
    "    - ¿Cuál configuración favorece más preguntas **muy generales**?\n",
    "    - ¿En cuál configuración esperas ver más \"cortes raros\" de frases?\n",
    " 4. Elige una de las dos configuraciones como la que usarías \"por defecto\" en tu proyecto y justifica en 5-7 líneas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec271ae-54b1-4449-88ff-40293c8ef04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff384202-d4ed-404b-9639-ee88d70ce6d8",
   "metadata": {},
   "source": [
    "**2.2 Conceptos clave: embeddings, vector DB y búsqueda aproximada**\n",
    "\n",
    "Antes de meternos en FAISS, conviene fijar el vocabulario:\n",
    "\n",
    "- Un **embedding** es un vector de dimensión `d` (por ejemplo `d = 384` o `768`) que representa el significado de un texto en un **espacio vectorial**.\n",
    "- Textos \"parecidos\" se mapean a vectores cercanos; textos \"distintos\" quedan lejos.\n",
    "\n",
    "Tipos habituales:\n",
    "\n",
    "- **Token embeddings**: un vector por token (palabra/sub-palabra).\n",
    "- **Sentence / document embeddings**: un vector por oración, párrafo o documento (lo que usamos en este cuaderno).\n",
    "\n",
    "Para comparar vectores podemos usar varias **métricas de distancia/similitud**:\n",
    "\n",
    "- **Producto punto** (lo que usamos con `IndexFlatIP` de FAISS).\n",
    "- **Similaridad de coseno**: producto punto entre vectores normalizados.\n",
    "- **L2 /Distancia Euclidiana**: distancia geométrica clásica.\n",
    "\n",
    "La búsqueda \"ingenua\" sería comparar la consulta con **todos** los vectores (búsqueda exacta), pero cuando tenemos millones de documentos esto se vuelve caro. Ahí entran los **índices ANN (Approximate Nearest Neighbors)**:\n",
    "\n",
    "- estructuras de datos especializadas (IVF, HNSW, etc.) que permiten encontrar vecinos \"suficientemente buenos\" mucho más rápido;\n",
    "- muchas bases vectoriales (FAISS, Qdrant, Milvus, etc.) implementan estas técnicas por debajo.\n",
    "\n",
    "En las siguientes secciones verás primero un índice FAISS simple en memoria, y luego cómo envolver estos embeddings en un `vectorstore` (FAISS/Qdrant) listo para usarse desde un RAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02957269-7fed-4416-86f0-a71d0ac45caa",
   "metadata": {},
   "source": [
    "#### **Ejercicio 3 -Explorando métricas de distancia en la búsqueda vectorial**\n",
    " \n",
    " En esta sección usamos un índice FAISS con producto punto/*inner product*.\n",
    " \n",
    " **Tareas:**\n",
    " 1. Modifica el código para probar dos configuraciones:\n",
    "    - (a) Normalizar los embeddings (norma 1) y seguir usando *inner product*.\n",
    "    - (b) Usar un índice FAISS basado en L2 (`IndexFlatL2`) sin normalización.\n",
    " 2. Define al menos **3 consultas** diferentes sobre el corpus (ejemplo,  \"black hole\", \"Cooper\", \"Earth\" si usas Interstellar).\n",
    " 3. Para cada consulta:\n",
    "    - Muestra los **top-5 documentos** recuperados con (a) y con (b).\n",
    "    - Señala en cuáles casos te parece que el ranking es mejor con (a) y en cuáles con (b).\n",
    " 4. Escribe un breve comentario (8-10 líneas) sobre:\n",
    "    - Cómo afecta la métrica de distancia al resultado.\n",
    "    - Por qué podrías preferir una u otra en un sistema real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f99890-56b1-467d-bcbc-7b619992aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9710c0",
   "metadata": {},
   "source": [
    "#### **3. Embeddings locales y búsqueda semántica con FAISS**\n",
    "\n",
    "Usamos `sentence-transformers` (por ejemplo `BAAI/bge-small-en-v1.5`) y FAISS para k-NN.\n",
    "\n",
    "Aquí nos centraremos primero en la parte de **búsqueda densa**: cómo pasar de textos a vectores y cómo hacer *k*-NN sobre ellos. En la siguiente sección verás cómo complementar esto con un modelo **sparse** (BM25) y luego combinarlos en un *retriever* híbrido más robusto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d70e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "def get_doc_embeddings_local(texts):\n",
    "    return embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "def get_query_embedding_local(query: str):\n",
    "    return embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "\n",
    "doc_embeddings = get_doc_embeddings_local(texts)\n",
    "dim = doc_embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(doc_embeddings.astype(\"float32\"))\n",
    "\n",
    "print(\"Índice FAISS construido:\", doc_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88d76d",
   "metadata": {},
   "source": [
    "#### **3.1 Función de búsqueda semántica**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ce0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_semantic(query: str, k: int = 3):\n",
    "    q_vec = get_query_embedding_local(query).astype(\"float32\")[None, :]\n",
    "    scores, indices = index.search(q_vec, k)\n",
    "    scores, indices = scores[0], indices[0]\n",
    "    results = []\n",
    "    for s, i in zip(scores, indices):\n",
    "        results.append({\n",
    "            \"score\": float(s),\n",
    "            \"index\": int(i),\n",
    "            \"text\": texts[i],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "for r in search_semantic(\"How accurate was the black hole science?\", k=3):\n",
    "    print(f\"[score={r['score']:.4f}] idx={r['index']} - {r['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0245b",
   "metadata": {},
   "source": [
    "#### **4. Sparse retrieval con BM25 y combinación híbrida**\n",
    "\n",
    "La búsqueda **sparse** clásica (tipo BM25) no usa embeddings: se basa en las palabras que aparecen en cada documento y en la frecuencia con la que lo hacen.  \n",
    "\n",
    "En esta sección verás:\n",
    "\n",
    "- cómo construir un índice BM25 sobre el mismo corpus,\n",
    "- cómo obtener resultados sólo con BM25,\n",
    "- y cómo combinar sus *scores* con los de la búsqueda densa anterior para obtener un *retriever* híbrido más robusto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [t.lower().split() for t in texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def search_bm25(query: str, k: int = 3):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_idx = np.argsort(scores)[::-1][:k]\n",
    "    results = []\n",
    "    for i in ranked_idx:\n",
    "        results.append({\n",
    "            \"score\": float(scores[i]),\n",
    "            \"index\": int(i),\n",
    "            \"text\": texts[i],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "for r in search_bm25(\"black hole and physics\", k=3):\n",
    "    print(f\"[bm25={r['score']:.4f}] idx={r['index']} - {r['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65148ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(arr):\n",
    "    arr = np.array(arr, dtype=\"float32\")\n",
    "    if arr.max() == arr.min():\n",
    "        return np.ones_like(arr)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "def search_hybrid(query: str, k: int = 3, alpha: float = 0.5):\n",
    "    # Puntajes densos\n",
    "    q_vec = get_query_embedding_local(query).astype(\"float32\")[None, :]\n",
    "    dense_scores, _ = index.search(q_vec, len(texts))\n",
    "    dense_scores = dense_scores[0]\n",
    "    #Puntaje sparse\n",
    "    tokenized_query = query.lower().split()\n",
    "    sparse_scores = np.array(bm25.get_scores(tokenized_query), dtype=\"float32\")\n",
    "\n",
    "    d_norm = _normalize(dense_scores)\n",
    "    s_norm = _normalize(sparse_scores)\n",
    "    fused = alpha * d_norm + (1 - alpha) * s_norm\n",
    "    ranked_idx = np.argsort(fused)[::-1][:k]\n",
    "\n",
    "    results = []\n",
    "    for i in ranked_idx:\n",
    "        results.append({\n",
    "            \"score\": float(fused[i]),\n",
    "            \"index\": int(i),\n",
    "            \"text\": texts[i],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "for r in search_hybrid(\"wormholes and relativity\", k=3, alpha=0.6):\n",
    "    print(f\"[hybrid={r['score']:.4f}] idx={r['index']} - {r['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074005aa-2efd-4c1a-81b4-6cddc82ff23d",
   "metadata": {},
   "source": [
    " #### **Ejercicio 4-Comparando *retrievers*: BM25 vs denso vs híbrido**\n",
    " \n",
    " En esta sección tienes implementados:\n",
    " \n",
    " - Un *retriever* **denso** (embeddings + FAISS).\n",
    " - Un *retriever* **sparse** (BM25).\n",
    " - Una función de búsqueda **híbrida** que combina ambas señales.\n",
    " \n",
    " **Tareas:**\n",
    " 1. Elige al menos **5 consultas** sobre tu corpus.\n",
    " 2. Para cada consulta, ejecuta:\n",
    "    - Sólo BM25.\n",
    "    - Sólo denso.\n",
    "    - Híbrido.\n",
    " 3. Marca manualmente, para cada consulta, qué documentos son \"claramente relevantes\" (pueden ser 1-3 por consulta).\n",
    " 4. Calcula a mano para cada *retriever*:\n",
    "    - **Recall@3** (¿aparece algún doc relevante en el top-3?).\n",
    "    - Opcional: la posición del primer documento relevante.\n",
    " 5. Comenta:\n",
    "    - ¿En qué tipo de consultas gana BM25? (por ejemplo, con palabras muy poco frecuentes, nombres, etc.)\n",
    "    - ¿En qué tipo de consultas gana el denso?\n",
    "    - ¿El híbrido aporta algo o se parece demasiado a uno de los dos?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838eee7-4f8f-4273-87fe-8ff922140ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe606b",
   "metadata": {},
   "source": [
    "#### **5. RAG básico con LangChain (Ollama + FAISS/Qdrant)**\n",
    "\n",
    "Construimos un pipeline sencillo: `Retriever + LLM`.\n",
    "\n",
    "En este cuaderno asumiremos un **stack local realista**:\n",
    "\n",
    "- **LLM** local vía **Ollama** (por ejemplo un modelo `llama3` o similar).\n",
    "- **Vector store** por defecto en memoria con **FAISS**, y opcionalmente **Qdrant** como base de datos vectorial persistente.\n",
    "- Más adelante conectaremos esto con:\n",
    "  - **LangGraph** para orquestar el flujo RAG como grafo.\n",
    "  - **Neo4j** (esqueleto) para GraphRAG.\n",
    "  - **FastAPI + Prometheus/Grafana** para exponer el RAG como servicio observable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1c19c-58a3-48e9-bb73-b13006416ba5",
   "metadata": {},
   "source": [
    "#### **5.1 Estrategias de *prompting* en RAG**\n",
    "\n",
    "Una vez que tenemos un `Retriever`, el siguiente reto es **cómo** pasar el contexto recuperado al LLM. Hay varios patrones típicos:\n",
    "\n",
    "1. **Stuffing (todo en un solo *prompt*)**\n",
    "\n",
    "   - Se concatenan los `k` chunks recuperados en un único bloque de contexto.\n",
    "   - Ventajas: simple, fácil de depurar.\n",
    "   - Limitaciones: si el contexto es grande puedes chocar con la ventana del modelo y \"ahogar\" la pregunta.\n",
    "\n",
    "2. **Map-Reduce**\n",
    "\n",
    "   - Paso *map*: el LLM genera un pequeño resumen/respuesta parcial por cada chunk relevante.\n",
    "   - Paso *reduce*: otro *prompt* combina esos resúmenes en una respuesta final.\n",
    "   - Útil cuando recuperas muchos documentos o cuando cada doc es largo.\n",
    "\n",
    "3. **Refine**\n",
    "\n",
    "   - Se procesa el primer chunk y se obtiene una respuesta inicial.\n",
    "   - Para cada chunk siguiente se llama al LLM con \"respuesta actual + nuevo contexto\" y se pide refinar/mejorar la respuesta.\n",
    "   - Es una especie de acumulación paso a paso.\n",
    "\n",
    "4. **Citations / respuestas con fuentes**\n",
    "\n",
    "   - El *prompt* pide explícitamente citar las fuentes (por ejemplo `[doc_3]`, `[página 5]`) y nunca inventarlas.\n",
    "   - Después puedes mapear esas etiquetas a URLs, títulos de archivo, etc., para mostrar trazabilidad al usuario.\n",
    "\n",
    "En este cuaderno, la función `simple_rag` implementa un patrón de tipo **stuffing**. Más adelante puedes extenderla para añadir variantes *map-reduce* o *refine* usando cadenas compuestas de LangChain o grafos más elaborados en LangGraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS as LC_FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Soporte opcional para Qdrant como vector DB\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# LLM local vía Ollama (por ejemplo llama3)\n",
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "\n",
    "# Embeddings para el RAG (puedes reutilizar el mismo modelo que arriba)\n",
    "emb_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=emb_model_name)\n",
    "\n",
    "docs = [Document(page_content=t, metadata={\"id\": i}) for i, t in enumerate(texts)]\n",
    "\n",
    "# Flag para elegir FAISS (in-memory) o Qdrant (persistente)\n",
    "USE_QDRANT = False  # ponlo en True si tienes Qdrant corriendo en localhost:6333\n",
    "\n",
    "if USE_QDRANT:\n",
    "    # Asume Qdrant en Docker: `docker run -p 6333:6333 qdrant/qdrant`\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"),\n",
    "        api_key=os.getenv(\"QDRANT_API_KEY\", None),\n",
    "    )\n",
    "    collection_name = os.getenv(\"QDRANT_COLLECTION\", \"chapter8_semantic_search\")\n",
    "\n",
    "    vectorstore = QdrantVectorStore.from_documents(\n",
    "        docs,\n",
    "        embeddings,\n",
    "        client=qdrant_client,\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "else:\n",
    "    vectorstore = LC_FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# LLM local (Ollama)\n",
    "# Requisitos:\n",
    "# 1) Instalar Ollama (https://ollama.ai) y arrancar el servidor local.\n",
    "# 2) Ejecutar: `ollama pull llama3` (u otro modelo que prefieras).\n",
    "# 3) Ajustar el nombre del modelo aquí:\n",
    "llm = Ollama(\n",
    "    model=os.getenv(\"OLLAMA_MODEL\", \"llama3\"),\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "def simple_rag(question: str) -> str:\n",
    "    \"\"\"\n",
    "    RAG mínimo: recupera k documentos relevantes y le pasa el contexto al LLM local.\n",
    "    \"\"\"\n",
    "    rel_docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in rel_docs)\n",
    "    prompt = (\n",
    "        \"You are an assistant that answers questions based ONLY on the following context.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"If the answer is not in the context, say 'I don't know'.\"\n",
    "    )\n",
    "    raw_answer = llm.invoke(prompt)\n",
    "    # Ollama (vía LangChain) ya devuelve un string\n",
    "    return str(raw_answer)\n",
    "\n",
    "# Ejemplo (cuando tengas Ollama corriendo):\n",
    "# print(simple_rag(\"How accurate was the science of the black hole?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f3f6f-4572-4475-ac65-1bc1316902e0",
   "metadata": {
    "tags": []
   },
   "source": [
    " #### **Ejercicio 5-Probando diferentes estrategias de *prompting* en RAG**\n",
    " \n",
    " Usando el `Retriever` de esta sección, diseña tres variantes de cadena RAG:\n",
    " \n",
    " 1. **Stuffing**:\n",
    "    - Concatena los `k` chunks en un solo contexto y llama al LLM una sola vez.\n",
    " 2. **Map-Reduce**:\n",
    "    - Paso *map*: para cada chunk relevante, pide al LLM un mini-resumen o mini-respuesta.\n",
    "    - Paso *reduce*: combina esos resúmenes en una respuesta final.\n",
    " 3. **Con citas**:\n",
    "    - Pide al LLM que responda pero citando explícitamente `[doc_i]` o `[chunk_j]` sin inventarse fuentes.\n",
    " \n",
    " **Tareas:**\n",
    " 1. Elige **3 preguntas** sobre tu corpus.\n",
    " 2. Para cada estrategia (stuffing, map-reduce, citas):\n",
    "    - Guarda la respuesta del modelo.\n",
    "    - Registra cuántos **tokens** approx. consumiste (puedes estimar por longitud de *prompt* + respuesta).\n",
    " 3. Compara:\n",
    "    - ¿Cuál estrategia dio respuestas más completas?\n",
    "    - ¿Cuál consume más contexto?\n",
    "    - ¿Cuál te gusta más para un sistema que deba ser explicable ante un \"auditor\"?\n",
    " 4. Escribe 5-8 líneas de reflexión sobre el trade-off entre **calidad**, **coste** y **explicabilidad**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dca61-0b5b-4190-a132-78454196bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc17a5-548a-42e7-9146-9c2924436ad9",
   "metadata": {},
   "source": [
    "#### **5.2 Diseño de colecciones y *drift* de embeddings**\n",
    "\n",
    "Cuando pasamos de un tutorial a un sistema real, ya no tenemos \"un solo vectorstore\", sino varias **colecciones/namespaces** con políticas distintas. Algunas decisiones típicas:\n",
    "\n",
    "- Separar colecciones por:\n",
    "  - **cliente** o proyecto,\n",
    "  - **idioma**,\n",
    "  - **tipo de documento** (manuales, tickets, políticas internas),\n",
    "  - **nivel de confidencialidad**.\n",
    "- Usar metadatos (`metadata={...}`) para poder filtrar en el `retriever`:\n",
    "  - por ejemplo, buscar sólo en `{\"idioma\": \"es\", \"tipo\": \"manual\"}`.\n",
    "\n",
    "Con Qdrant (y otras bases vectoriales) es buena práctica **versionar** las colecciones, por ejemplo:\n",
    "\n",
    "- `docs_v1_bge_small`\n",
    "- `docs_v2_bge_m3`\n",
    "\n",
    "Esto ayuda a manejar el llamado **embedding drift**:\n",
    "\n",
    "- cambias de modelo de embeddings (por calidad, coste, idioma);\n",
    "- o cambia el dominio de tus datos con el tiempo.\n",
    "\n",
    "Estrategias básicas para el *drift*:\n",
    "\n",
    "- mantener colecciones antiguas **sólo lectura** mientras reindexas en una colección nueva;\n",
    "- registrar en los metadatos qué versión de modelo generó cada embedding;\n",
    "- definir una política clara de \"deprecación\" de colecciones viejas.\n",
    "\n",
    "En el bloque de código anterior puedes imaginar que el `collection_name` de Qdrant incluye ya la versión del modelo de embeddings que estás usando.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64454008-af20-4c60-a9b6-6da64e56a068",
   "metadata": {},
   "source": [
    " #### **Ejercicio 6-Diseñando colecciones y manejando *drift* de embeddings**\n",
    " \n",
    " Imagina que trabajas en una empresa que quiere un RAG para:\n",
    " - Documentación interna de DevSecOps (español/inglés).\n",
    " - Políticas de RR.HH. (sólo español).\n",
    " - Manuales de producto para clientes externos (multilingüe).\n",
    " \n",
    " Además, planean cambiar de modelo de embeddings en 6 meses.\n",
    " \n",
    " **Tareas:**\n",
    " 1. Propón un esquema de **colecciones/namespaces** (por ejemplo, nombres de colecciones en Qdrant) y describe en 5-6 líneas las reglas para:\n",
    "    - qué va en cada colección;\n",
    "    - quién puede consultar cada colección (alto nivel: interno vs externo).\n",
    " 2. Diseña los **metadatos mínimos** que almacenarías por chunk (`metadata={...}`).\n",
    " 3. Describe un plan para manejar el **cambio de modelo de embeddings** (\"v1\" - \"v2\"):\n",
    "    - ¿Cómo versionarías las colecciones?\n",
    "    - ¿Cómo migrarías a la nueva versión minimizando cortes?\n",
    " 4. Opcional: Diseña un pequeño \"playbook\" en 4-5 pasos de qué hacer si detectas que la nueva versión empeora mucho el rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc39722-527b-4b57-9d26-b024a81b5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bb699",
   "metadata": {},
   "source": [
    "#### **6. RAG como grafo con LangGraph**\n",
    "\n",
    "El mismo RAG anterior se puede expresar como un grafo de estados con LangGraph.\n",
    "\n",
    "Pasar a un grafo nos permite:\n",
    "\n",
    "- hacer explícito el flujo de estados (entrada - recuperación - generación),\n",
    "- añadir fácilmente pasos intermedios (post-procesado, re-ranqueo, verificación, etc.),\n",
    "- y conectar luego este grafo con APIs (FastAPI) o con agentes más complejos (LATS, GraphRAG).\n",
    "\n",
    "En el bloque de código siguiente definiremos un estado mínimo (`RAGState`) y dos nodos (`retrieve` y `generate`) para capturar el RAG básico como grafo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class RAGState(TypedDict, total=False):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    question = state[\"question\"]\n",
    "    docs_ = retriever.invoke(question)\n",
    "    return {**state, \"retrieved_docs\": docs_}\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    if llm is None:\n",
    "        raise ValueError(\"Define 'llm' antes de ejecutar el grafo RAG.\")\n",
    "    docs_ = state.get(\"retrieved_docs\", [])\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs_)\n",
    "    question = state[\"question\"]\n",
    "    prompt = (\n",
    "        \"You are an assistant that answers questions based ONLY on the given context.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "    answer = llm.invoke(prompt)\n",
    "    return {**state, \"answer\": answer}\n",
    "\n",
    "g_builder = StateGraph(RAGState)\n",
    "g_builder.add_node(\"retrieve\", retrieve_node)\n",
    "g_builder.add_node(\"generate\", generate_node)\n",
    "g_builder.set_entry_point(\"retrieve\")\n",
    "g_builder.add_edge(\"retrieve\", \"generate\")\n",
    "g_builder.add_edge(\"generate\", END)\n",
    "\n",
    "rag_graph = g_builder.compile()\n",
    "\n",
    "# Ejemplo (cuando tengas LLM):\n",
    "# result = rag_graph.invoke({\"question\": \"What are the main scientific themes in Interstellar?\"})\n",
    "# print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64497b81",
   "metadata": {},
   "source": [
    "#### **7. FastAPI + LangGraph: servicio RAG (con métricas Prometheus)**\n",
    "\n",
    "API mínima `POST /rag/query` que invoca el grafo `rag_graph`.\n",
    "\n",
    "Además, instrumentamos FastAPI con **prometheus-fastapi-instrumentator** para exponer un endpoint\n",
    "`/metrics` que pueda ser scrapeado por Prometheus y visualizado en Grafana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Observabilidad con Prometheus\n",
    "from prometheus_fastapi_instrumentator import Instrumentator\n",
    "from prometheus_client import Counter\n",
    "\n",
    "app = FastAPI(title=\"Local RAG with LangGraph\", version=\"0.1.0\")\n",
    "\n",
    "# Instrumentar FastAPI: añade /metrics automáticamente\n",
    "Instrumentator().instrument(app).expose(app)\n",
    "\n",
    "# Métrica personalizada: número de consultas RAG\n",
    "RAG_REQUESTS = Counter(\n",
    "    \"rag_requests_total\",\n",
    "    \"Total de consultas al endpoint /rag/query\",\n",
    "    [\"status\"],\n",
    ")\n",
    "\n",
    "class RAGQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    answer: str\n",
    "    documents: List[str]\n",
    "\n",
    "@app.post(\"/rag/query\", response_model=RAGResponse)\n",
    "def rag_endpoint(payload: RAGQuery):\n",
    "    if llm is None:\n",
    "        RAG_REQUESTS.labels(status=\"error_no_llm\").inc()\n",
    "        raise ValueError(\"Define 'llm' antes de levantar la API RAG.\")\n",
    "\n",
    "    question = payload.question\n",
    "    result = rag_graph.invoke({\"question\": question})\n",
    "    answer = result[\"answer\"]\n",
    "    docs_ = result.get(\"retrieved_docs\", [])\n",
    "    docs_text = [d.page_content for d in docs_]\n",
    "\n",
    "    RAG_REQUESTS.labels(status=\"ok\").inc()\n",
    "    return RAGResponse(answer=str(answer), documents=docs_text)\n",
    "\n",
    "\"\"\"\n",
    "Ejemplo para lanzar el servidor en consola:\n",
    "\n",
    "uvicorn nombre_de_este_archivo:app --reload --port 8000\n",
    "\n",
    "curl -X POST \"http://localhost:8000/rag/query\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"question\": \"How accurate was the black hole science?\"}'\n",
    "\n",
    "# Prometheus:\n",
    "#   - Configura un job que apunte a localhost:8000/metrics\n",
    "# Grafana:\n",
    "#   - Añade Prometheus como datasource y crea dashboards con las métricas\n",
    "#     http_request_duration_seconds y rag_requests_total.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e38c1-5f85-4c22-8198-a0a758e034ad",
   "metadata": {},
   "source": [
    "#### **7.1 Evaluación de la calidad de un sistema RAG**\n",
    "\n",
    "Además de métricas de uso (latencia, errores, `rag_requests_total` en Prometheus), necesitamos métricas para evaluar **qué tan bien responde** nuestro sistema RAG. Hay dos niveles:\n",
    "\n",
    "\n",
    "##### a) Evaluación del *retriever*\n",
    "\n",
    "Partimos de un conjunto de prueba con pares *(pregunta, documentos relevantes)*. Algunas métricas:\n",
    "\n",
    "- **Recall@k**  \n",
    "  Proporción de preguntas en las que, entre los `k` documentos recuperados, aparece al menos un documento marcado como relevante.\n",
    "\n",
    "- **MRR (Mean Reciprocal Rank)**  \n",
    "  Mide en promedio en qué posición aparece el primer documento relevante. Penaliza que el documento correcto esté \"enterrado\" al final de la lista.\n",
    "\n",
    "- **nDCG (normalized Discounted Cumulative Gain)**  \n",
    "  Extiende la idea anterior permitiendo varios niveles de relevancia (muy relevante, algo relevante, etc.) y dando más peso a los primeros puestos del ranking.\n",
    "\n",
    "Estas métricas son independientes del LLM: sólo miran la calidad de la recuperación.\n",
    "\n",
    "\n",
    "##### b) Evaluación de las respuestas generadas\n",
    "\n",
    "Suponiendo que tenemos respuestas de referencia:\n",
    "\n",
    "- **EM (Exact Match)**: porcentaje de respuestas que coinciden exactamente con la referencia (útil en QA muy estructurado).\n",
    "- **F1 de *token overlap***: mide solapamiento de palabras entre la respuesta del modelo y la referencia (más tolerante a variaciones de estilo).\n",
    "\n",
    "En tareas abiertas es habitual usar un **LLM-as-a-judge**:\n",
    "\n",
    "1. Se pasa al \"juez\" la pregunta, la respuesta del sistema y, opcionalmente, una respuesta de referencia.\n",
    "2. Se le pide puntuar según un criterio (exactitud, cobertura, citación correcta, etc.).\n",
    "3. Se promedian las puntuaciones sobre el conjunto de prueba.\n",
    "\n",
    "En código, suele verse como una función `score_answer(question, answer, reference) - score de 1 a 5`.  \n",
    "Este tipo de evaluación debe diseñarse con cuidado (instrucciones claras al juez, muestreo manual para detectar sesgos), pero es práctico cuando las respuestas son largas y semánticamente ricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9561851-34a5-4093-96bf-44b5949f6dc2",
   "metadata": {},
   "source": [
    " #### **Ejercicio 7-Implementando métricas de evaluación**\n",
    " \n",
    " Vas a construir un mini *benchmark* de RAG para tu propio corpus.\n",
    " \n",
    " **Tareas:**\n",
    " 1. Define un conjunto de al menos **5 pares**:\n",
    "    - pregunta `q_i`;\n",
    "    - conjunto de documentos relevantes `D_i` (índices de tus textos/chunks);\n",
    "    - una respuesta de referencia corta `a_i` (2-4 líneas).\n",
    " 2. Implementa en código funciones para:\n",
    "    - `recall_at_k(queries, retriever, gold_docs, k=3)`;\n",
    "    - `mrr_at_k(queries, retriever, gold_docs, k=10)`.\n",
    " 3. Ejecuta las métricas para:\n",
    "    - el *retriever* denso;\n",
    "    - el *retriever* híbrido.\n",
    " 4. Implementa funciones simples para:\n",
    "    - **Exact Match (EM)** (ignora mayúsculas y puntuación básica);\n",
    "    - **F1 de solapamiento de tokens** entre respuesta del modelo y `a_i`.\n",
    " 5. Comenta:\n",
    "    - ¿Hay correlación entre \"buen *retrieval*\" y \"buena respuesta generada\" en tu experimento?\n",
    "    - ¿En qué casos el LLM respondió bien aunque el *retriever* falló?\n",
    "    - ¿En qué casos el *retriever* dio buen contexto pero el LLM alucinó?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6b139-69eb-42d5-be6c-796a0b2e7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e350e9-9427-4433-83bd-b6f717407419",
   "metadata": {
    "tags": []
   },
   "source": [
    " #### **Ejercicio 8 - Observabilidad básica para un servicio RAG**\n",
    " \n",
    " El servicio FastAPI ya expone:\n",
    " - un endpoint `POST /rag/query`;\n",
    " - métricas instrumentadas con `prometheus-fastapi-instrumentator` y un `Counter`.\n",
    " \n",
    " **Tareas:**\n",
    " 1. Añade una métrica de tipo **Histogram** o **Summary** para:\n",
    "    - latencia de las peticiones RAG (por ejemplo, `rag_request_latency_seconds`).\n",
    " 2. Etiqueta la métrica (labels) al menos con:\n",
    "    - `status` (success/error);\n",
    "    - opcional: `retrieval_strategy` (dense/sparse/hybrid) si tu código lo soporta.\n",
    " 3. Usa un pequeño script o el archivo de tráfico de demo para enviar:\n",
    "    - ≥ 20 peticiones exitosas;\n",
    "    - algunas peticiones que fuercen error (por ejemplo, tiempo de espera excesivo, query inválida, etc.).\n",
    " 4. En Prometheus:\n",
    "    - Muestra la latencia media de los últimos 5 minutos.\n",
    "    - Filtra por `status` para comparar éxito vs error.\n",
    " 5. Escribe 5-8 líneas explicando:\n",
    "    - Qué *SLO* razonable pondrías a la latencia de tu endpoint RAG.\n",
    "    - Qué tipo de alerta (regla) te gustaría configurar si la latencia se dispara o la tasa de errores sube mucho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07a4ff-a869-4928-b3a5-1097a1d7e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef25918",
   "metadata": {},
   "source": [
    "#### **8. Esqueleto de GraphRAG**\n",
    "\n",
    "GraphRAG combina vector store + base de datos de grafos. Aquí sólo dejamos el esqueleto conceptual para integrarlo con Neo4j, FalkorDB, etc.\n",
    "\n",
    "Mientras que el RAG \"clásico\" consulta sólo la base vectorial, en GraphRAG podemos:\n",
    "\n",
    "- recuperar nodos relacionados (entidades, conceptos, documentos) a través de aristas,\n",
    "- enriquecer el contexto con relaciones explícitas,\n",
    "- y aplicar algoritmos de recorrido de grafos antes de llamar al LLM.\n",
    "\n",
    "Esto abre la puerta a un razonamiento más estructurado, que conectaremos más adelante con agentes tipo LATS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd49503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esqueleto conceptual (no ejecutable tal cual) para GraphRAG con Neo4j\n",
    "# La idea: combinar el retriever vectorial (FAISS/Qdrant) con un grafo de conocimiento en Neo4j.\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def _cypher_neighbors(tx, entity_id: str) -> List[str]:\n",
    "    query = \"\"\"    MATCH (e:Entity {id: $entity_id})-[:RELATED_TO*1..2]-(n)\n",
    "    RETURN DISTINCT n.summary AS text\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    result = tx.run(query, entity_id=entity_id)\n",
    "    return [r[\"text\"] for r in result if r[\"text\"]]\n",
    "\n",
    "def graph_rag(question: str) -> str:\n",
    "    \"\"\"\n",
    "    1) Usar el retriever vectorial para encontrar documentos/contextos ancla.\n",
    "    2) Extraer de sus metadatos un id de entidad (por ejemplo, `metadata[\"id\"]` o `metadata[\"entity_id\"]`).\n",
    "    3) Ampliar el contexto con nodos vecinos en Neo4j (1-2 hops).\n",
    "    4) Enviar contexto + pregunta al LLM local (Ollama).\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        raise ValueError(\"Define 'llm' para usar graph_rag.\")\n",
    "\n",
    "    base_docs = retriever.invoke(question)\n",
    "    base_context = \"\\n\\n\".join(d.page_content for d in base_docs)\n",
    "\n",
    "    # Ejemplo simple: usar los ids de metadatos como entity_id\n",
    "    entity_ids = [str(d.metadata.get(\"id\")) for d in base_docs if \"id\" in d.metadata]\n",
    "\n",
    "    graph_texts: List[str] = []\n",
    "    if entity_ids:\n",
    "        with driver.session() as session:\n",
    "            for eid in entity_ids:\n",
    "                graph_texts.extend(session.execute_read(_cypher_neighbors, entity_id=eid))\n",
    "\n",
    "    graph_context = \"\\n\\n\".join(graph_texts)\n",
    "\n",
    "    full_context = base_context\n",
    "    if graph_context:\n",
    "        full_context += \"\\n\\n--- Graph context (Neo4j) ---\\n\\n\" + graph_context\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a GraphRAG assistant. Answer based on the following hybrid context \"\n",
    "        \"(vector store + Neo4j graph).\\n\\n\"\n",
    "        f\"Context:\\n{full_context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"If you are not sure, say that you don't know.\"\n",
    "    )\n",
    "    raw_answer = llm.invoke(prompt)\n",
    "    return str(raw_answer)\n",
    "\n",
    "# NOTA: este esqueleto asume que:\n",
    "# - Ya cargaste entidades y relaciones en Neo4j.\n",
    "# - Tus documentos tienen metadatos que permiten mapearlos a nodos del grafo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c0f15",
   "metadata": {},
   "source": [
    "#### **9. Esqueleto de LATS con Burr**\n",
    "\n",
    "LATS (Language Agent Tree Search) se puede implementar como máquina de estados en Burr.\n",
    "\n",
    "Este esqueleto ilustra la idea de tratar un agente como una **máquina de estados** donde cada acción (planificar, recuperar, evaluar, decidir siguiente paso) es un nodo del grafo. En la siguiente sección verás una variante similar implementada directamente con LangGraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from burr.core import action, State, ApplicationBuilder\n",
    "\n",
    "class QAState(State):\n",
    "    \"\"\"Estado del agente de QA/RAG.\"\"\"\n",
    "    pass\n",
    "\n",
    "@action(reads=[\"question\"], writes=[\"subquestions\"])\n",
    "def plan_step(state: QAState) -> QAState:\n",
    "    q = state[\"question\"]\n",
    "    # TODO: llamar a un LLM para descomponer la pregunta en subpreguntas\n",
    "    subqs = [q]  # placeholder\n",
    "    return state.update(subquestions=subqs)\n",
    "\n",
    "@action(reads=[\"subquestions\"], writes=[\"candidate_answers\"])\n",
    "def retrieve_step(state: QAState) -> QAState:\n",
    "    subqs = state[\"subquestions\"]\n",
    "    cands = []\n",
    "    for sq in subqs:\n",
    "        # TODO: usar simple_rag / graph_rag / search_semantic + LLM\n",
    "        # ans = simple_rag(sq)\n",
    "        ans = f\"TODO: answer for '{sq}'\"\n",
    "        cands.append({\"question\": sq, \"answer\": ans})\n",
    "    return state.update(candidate_answers=cands)\n",
    "\n",
    "@action(reads=[\"candidate_answers\"], writes=[\"final_answer\"])\n",
    "def evaluate_step(state: QAState) -> QAState:\n",
    "    cands = state[\"candidate_answers\"]\n",
    "    # TODO: usar LLM-as-a-judge para elegir la mejor combinación\n",
    "    if cands:\n",
    "        final = cands[0][\"answer\"]\n",
    "    else:\n",
    "        final = \"No answer found.\"\n",
    "    return state.update(final_answer=final)\n",
    "\n",
    "lats_app = (\n",
    "    ApplicationBuilder()\n",
    "    .with_actions(plan_step, retrieve_step, evaluate_step)\n",
    "    .with_transitions(\n",
    "        (\"plan_step\", \"retrieve_step\"),\n",
    "        (\"retrieve_step\", \"evaluate_step\"),\n",
    "    )\n",
    "    .with_state(question=None)\n",
    "    .with_entrypoint(\"plan_step\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Ejemplo (cuando conectes LLM y RAG reales):\n",
    "# *_, end_state = lats_app.run(\n",
    "#     halt_after=[\"evaluate_step\"],\n",
    "#     inputs={\"question\": \"How accurate is the science in Interstellar?\"},\n",
    "# )\n",
    "# print(\"Final answer:\", end_state[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d7969",
   "metadata": {},
   "source": [
    "#### **10. LATS / tree-search con LangGraph**\n",
    "\n",
    "Versión simplificada de búsqueda en árbol (tipo LATS) usando LangGraph directamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48112167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class SearchNode(TypedDict, total=False):\n",
    "    hypothesis: str\n",
    "    score: float\n",
    "\n",
    "class LATSState(TypedDict, total=False):\n",
    "    question: str\n",
    "    frontier: list[SearchNode]\n",
    "    best_answer: str\n",
    "\n",
    "def expand_node(state: LATSState) -> LATSState:\n",
    "    q = state[\"question\"]\n",
    "    frontier = state.get(\"frontier\", [])\n",
    "    # TODO: usar LLM para generar nuevas hipótesis\n",
    "    new_h = {\"hypothesis\": f\"TODO: hypothesis for '{q}'\", \"score\": 0.0}\n",
    "    frontier.append(new_h)\n",
    "    return {**state, \"frontier\": frontier}\n",
    "\n",
    "def score_node(state: LATSState) -> LATSState:\n",
    "    frontier = state.get(\"frontier\", [])\n",
    "    if not frontier:\n",
    "        return state\n",
    "    # TODO: usar LLM o función de reward para puntuar\n",
    "    frontier[-1][\"score\"] = 1.0\n",
    "    best = max(frontier, key=lambda n: n.get(\"score\", 0.0))\n",
    "    return {**state, \"frontier\": frontier, \"best_answer\": best[\"hypothesis\"]}\n",
    "\n",
    "search_builder = StateGraph(LATSState)\n",
    "search_builder.add_node(\"expand\", expand_node)\n",
    "search_builder.add_node(\"score\", score_node)\n",
    "search_builder.set_entry_point(\"expand\")\n",
    "search_builder.add_edge(\"expand\", \"score\")\n",
    "search_builder.add_edge(\"score\", END)  # en un LATS real se haría un bucle\n",
    "\n",
    "lats_graph = search_builder.compile()\n",
    "\n",
    "# result = lats_graph.invoke({\"question\": \"How precise was the black hole depiction?\"})\n",
    "# print(\"Best hypothesis:\", result.get(\"best_answer\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec72fd-17e8-4742-81a9-27d7d7680788",
   "metadata": {},
   "source": [
    " #### **Ejercicio 9-LATS (Language Agent Tree Search) sobre el grafo de LangGraph**\n",
    " \n",
    " En esta sección has visto un esqueleto de LATS donde el agente:\n",
    " - descompone la pregunta en sub-preguntas (*plan*),\n",
    " - recupera contexto para cada sub-pregunta (*retrieve*),\n",
    " - genera respuestas parciales o candidatas (*answer*),\n",
    " - y recorre un árbol de posibilidades.\n",
    " \n",
    "**Parte A-Preguntas conceptuales**\n",
    " \n",
    " 1. **RAG lineal vs LATS en árbol**  \n",
    "    Explica con tus palabras cómo LATS extiende un RAG \"lineal\" (retrieve -> generate) a un proceso de **búsqueda en árbol**:\n",
    "    - ¿Qué papel juegan los pasos de `plan`, `retrieve` y `answer` (o `evaluate`, si lo tienes) en el grafo de LATS del cuaderno?  \n",
    "    - ¿Por qué puede ser útil tener varios \"nodos de respuesta candidata\" en el árbol en lugar de una única respuesta directa del LLM?\n",
    " \n",
    " 2. **Heurísticas y verificación**  \n",
    "    Imagina que quieres robustecer el esqueleto de LATS añadiendo una etapa explícita de **verificador** (*LLM-as-a-judge*) antes de elegir la respuesta final:  \n",
    "    - ¿Qué tipo de criterio o *score* (por ejemplo, de 1 a 5) le pedirías al verificador que asigne a cada respuesta candidata?  \n",
    "    - Menciona al menos **dos riesgos** si esa heurística o verificador están mal diseñados (por ejemplo, aceptar respuestas alucinadas, rechazar respuestas correctas, etc.).\n",
    " \n",
    " \n",
    "**Parte B- Modificación práctica del grafo LATS**\n",
    " \n",
    " A partir del grafo de la celda `LATS/tree-search con LangGraph`, extiende el código para añadir un nodo de verificación.\n",
    " \n",
    " 3. **Ampliar el estado de LATS**  \n",
    "    - Localiza la definición de tu estado (por ejemplo `class LATSState(TypedDict, ...)` o similar).  \n",
    "    - Añade uno o más campos para almacenar evaluación del verificador, por ejemplo:\n",
    "      - `scores: dict[str, float]` (score por respuesta candidata), o  \n",
    "      - `best_answer: str` y `best_score: float`, o  \n",
    "      - `accepted: bool` si decides usar un simple \"aceptar/rechazar\".\n",
    " \n",
    " 4. **Implementar un nodo `verify_step`**  \n",
    "    - Crea una nueva función/nodo, por ejemplo:\n",
    "      ```python\n",
    "      @app.node\n",
    "      def verify_step(state: LATSState) - LATSState:\n",
    "          \\\"\\\"\\\"Usa el LLM como juez sobre las respuestas candidatas.\\\"\\\"\\\"\n",
    "          # 1. Construye un prompt del tipo:\n",
    "          #    - pregunta original\n",
    "          #    - respuestas candidatas (por ejemplo, una lista numerada)\n",
    "          #    - instrucción: \"evalúa cada respuesta de 1 a 5 según exactitud y uso correcto del contexto\"\n",
    "          # 2. Llama al LLM en modo \"juez\" y parsea la salida\n",
    "          # 3. Actualiza el estado con los puntajes (scores) y/o selecciona la mejor respuesta\n",
    "          return state\n",
    "      ```\n",
    "    - No necesitas que el nodo sea perfecto, basta con que:\n",
    "      - lea del estado las respuestas candidatas,\n",
    "      - devuelva el estado enriquecido con algún tipo de score o bandera.\n",
    " \n",
    " 5. **Conectar `verify_step` en el grafo**  \n",
    "    - Modifica la definición de tu grafo LATS para que, después del nodo que genera las respuestas (por ejemplo `answer_step`):\n",
    "      - se ejecute `verify_step` antes de llegar al estado final.  \n",
    "    - Diseña una regla simple:\n",
    "      - si el `best_score` (o score medio) es **mayor o igual a un umbral** `τ` (por ejemplo 4.0), el grafo termina y devuelve esa respuesta;  \n",
    "      - si el score es **menor que `τ`**  vuelve a un nodo anterior (por ejemplo `plan_step` o `retrieve_step`) para refinar la descomposición o recuperar más contexto.\n",
    " \n",
    " 6. **Comparación rápida LATS vs RAG lineal**  \n",
    "    - Elige **3 preguntas**:\n",
    "      1. Una sencilla que no requiera descomposición.\n",
    "      2. Una que naturalmente se divida en 2-3 sub-preguntas.\n",
    "      3. Una ambigua o incompleta.\n",
    "    - Para cada pregunta:\n",
    "      - ejecuta tu RAG \"lineal\" y guarda la respuesta,\n",
    "      - ejecuta tu LATS extendido con `verify_step` y guarda la respuesta;\n",
    "      - anota cuántas llamadas al LLM se hicieron en cada caso (aproximado).\n",
    "    - Escribe 6-8 líneas comentando:\n",
    "      - en cuál de las preguntas LATS aporta una mejora clara frente al RAG lineal;\n",
    "      - en qué casos el coste (más llamadas al LLM, más complejidad) no se justifica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68603064-0d42-4315-afff-de3c355dd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
