{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebfa65f",
   "metadata": {},
   "source": [
    "##  Construyendo tu propio modelo de lenguaje n-gramas\n",
    "\n",
    "En esta actividad, crearás tu propio modelo de lenguaje basado en n-gramas y la estimación de máxima verosimilitud.\n",
    "\n",
    "Asumiremos que nuestro texto ya está \"tokenizado\" (dividido en palabras). \n",
    "\n",
    "Como ejemplo, trabajemos con dos frases de \"[The Disappearance of Lady Frances Carfax](https://en.wikipedia.org/wiki/The_Disappearance_of_Lady_Frances_Carfax)\", un cuento escrito por [Sir Arthur Conan Doyle](https://en.wikipedia.org/wiki/Arthur_Conan_Doyle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e68e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens para la oracion \"It shows, my dear Watson, that we are dealing\n",
    "# with an exceptionally astude and dangerous man.\"\n",
    "muestra1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "# Tokens para la oracion \"How would Lausanne do, my dear Watson?\"\n",
    "muestra2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cedc28",
   "metadata": {},
   "source": [
    "Tu primera tarea es escribir una función que divida la secuencia de \"tokens\" en sus `n`-gramas.\n",
    "\n",
    "Por ejemplo, cuando `tokens=muestra1` y `n=3`, tu función debería devolver:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    " \n",
    "Nota: Debes devolver una [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists)  de Python que contenga [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences). Las `tuplas` son secuencias inmutables, que serán útiles más adelante cuando construyas tu modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def construir_ngramas(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    ngramas = []\n",
    "    \n",
    "    # Completa...\n",
    "\n",
    "# Ejemplo\n",
    "construir_ngramas(muestra1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(construir_ngramas(muestra1, n=3)) == 17\n",
    "assert construir_ngramas(muestra1, n=3)[0] == ('It', 'shows', ',')\n",
    "assert construir_ngramas(muestra1, n=3)[10] == ('dealing', 'with', 'an')\n",
    "assert len(construir_ngramas(muestra1, n=2)) == 18\n",
    "assert construir_ngramas(muestra1, n=2)[0] == ('It', 'shows')\n",
    "assert construir_ngramas(muestra1, n=2)[10] == ('dealing', 'with')\n",
    "assert len(construir_ngramas(muestra2, n=2)) == 8\n",
    "assert construir_ngramas(muestra2, n=2)[0] == ('How', 'would')\n",
    "assert construir_ngramas(muestra2, n=2)[7] == ('Watson', '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332659e6",
   "metadata": {},
   "source": [
    "Con la función actual, no hay forma de saber si un n-grama está al principio, en el medio o al final de la secuencia. Para superar este problema, los modelos de lenguaje n-grama a menudo incluyen tokens de control especiales de \"principio de cadena\" `(BOS)` y \"fin de cadena\" `(EOS)`.\n",
    "\n",
    "Escribe una nueva versión de tu función `construir_ngrams` que incluya estos tokens de control. Por ejemplo, cuando `tokens=sample1` y `n=3`, tu nueva función debería devolver:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def construir_ngramas_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n debe ser mayor que 0.\")\n",
    "    \n",
    "    # Completa ...\n",
    "    return ngramas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(construir_ngramas_ctrl(muestra1, n=3)) == 21\n",
    "assert construir_ngramas_ctrl(muestra1, n=3)[0] == ('<BOS>', '<BOS>', 'It')\n",
    "assert construir_ngramas_ctrl(muestra1, n=3)[10] == ('we', 'are', 'dealing')\n",
    "assert len(construir_ngramas_ctrl(muestra1, n=2)) == 20\n",
    "assert construir_ngramas_ctrl(muestra1, n=2)[0] == ('<BOS>', 'It')\n",
    "assert construir_ngramas_ctrl(muestra1, n=2)[10] == ('are', 'dealing')\n",
    "assert len(construir_ngramas_ctrl(muestra2, n=2)) == 10\n",
    "assert construir_ngramas_ctrl(muestra2, n=2)[0] == ('<BOS>', 'How')\n",
    "assert construir_ngramas_ctrl(muestra2, n=2)[9] == ('?', '<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27836863",
   "metadata": {},
   "source": [
    "Ahora que puedes crear n-gramas, tenemos casi todo lo que necesitamos para crear un modelo de lenguaje de n-gramas.\n",
    "\n",
    "Para calcular las estimaciones de máxima verosimilitud, primero debes contar el número de veces que cada palabra sigue a un n-grama de tamaño `n-1`.\n",
    "\n",
    "Puedes construir esta estructura como un [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)  de Python que asigna los n-gramas de tamaño `n-1` a otro ` dict` que asigna las siguientes palabras a sus respectivos conteos.\n",
    "\n",
    "Por ejemplo, cuando `textos=[muestra1, muestra2]` y `n=3`, tu función debería devolver:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def conteo_ngramas(textos: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    ngrama_conteos = {}\n",
    "    \n",
    "    # Completa...\n",
    "       \n",
    "       \n",
    "    return ngrama_conteos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aeefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(conteo_ngramas([muestra1, muestra2], n=3)) == 28\n",
    "assert len(conteo_ngramas([muestra1, muestra2], n=3)['<BOS>', '<BOS>']) == 2\n",
    "assert conteo_ngramas([muestra1, muestra2], n=3)['<BOS>', '<BOS>']['It'] == 1\n",
    "assert conteo_ngramas([muestra1, muestra2], n=3)['<BOS>', '<BOS>']['How'] == 1\n",
    "assert conteo_ngramas([muestra1, muestra2], n=3)['my', 'dear']['Watson'] == 2\n",
    "assert len(conteo_ngramas([muestra1, muestra2], n=2)) == 24\n",
    "assert len(conteo_ngramas([muestra1, muestra2], n=2)['<BOS>',]) == 2\n",
    "assert conteo_ngramas([muestra1, muestra2], n=2)['<BOS>',]['It'] == 1\n",
    "assert conteo_ngramas([muestra1, muestra2], n=2)['<BOS>',]['How'] == 1\n",
    "assert conteo_ngramas([muestra1, muestra2], n=2)['dear',]['Watson'] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb1275",
   "metadata": {},
   "source": [
    "¡Ya casi estás ahí! El último paso es convertir los conteos en estimaciones de probabilidad.\n",
    "\n",
    "Cuando `textos=[muestra1, muestra2]` y `n=3`, tu función debería devolver:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60897aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_modelos_ngramas(textos: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    ngrama_conteos = {}\n",
    "    modelo = {}\n",
    "\n",
    "    for tokens in textos:\n",
    "        ngramas = construir_ngramas_ctrl(tokens, n)\n",
    "        for ngrama in ngramas:\n",
    "            n_menos_1_grama = ngrama[:-1]\n",
    "            siguiente_palabra = ngrama[-1]\n",
    "\n",
    "            if n_menos_1_grama not in ngrama_conteos:\n",
    "                ngrama_conteos[n_menos_1_grama] = {}\n",
    "            \n",
    "            if siguiente_palabra not in ngrama_conteos[n_menos_1_grama]:\n",
    "                ngrama_conteos[n_menos_1_grama][siguiente_palabra] =1\n",
    "            else:\n",
    "                ngrama_conteos[n_menos_1_grama][siguiente_palabra] +=1\n",
    "    \n",
    "    # Convertimos los conteos en probabilidades\n",
    "    # Completa...\n",
    "    \n",
    "    return modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert construir_modelos_ngramas([muestra1, muestra2], n=3)['<BOS>', '<BOS>']['It'] == 0.5\n",
    "assert construir_modelos_ngramas([muestra1, muestra2], n=3)['<BOS>', '<BOS>']['How'] == 0.5\n",
    "assert construir_modelos_ngramas([muestra1, muestra2], n=3)['my', 'dear']['Watson'] == 1.0\n",
    "assert construir_modelos_ngramas([muestra1, muestra2], n=2)['<BOS>',]['It'] == 0.5\n",
    "assert construir_modelos_ngramas([muestra1, muestra2], n=2)['<BOS>',]['How'] == 0.5\n",
    "assert construir_modelos_ngramas([muestra1, muestra2], n=2)['dear',]['Watson'] == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30522d61",
   "metadata": {},
   "source": [
    "Un modelo de lenguaje construido a partir de sólo unas pocas oraciones no es muy informativo. ¡Aumentemos la escala y veamos cómo se ve tu modelo de lenguaje cuando entrenemos en las obras completas de Sir Arthur Conon Doyle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50371bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_completo = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for linea in fin:\n",
    "        texto_completo.append(list(linea.split()))\n",
    "modelo = construir_modelos_ngramas(texto_completo, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefijo in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefijo)\n",
    "    probs_ordenada = sorted(modelo[prefijo].items(), key=lambda x: -x[1])\n",
    "    for k, v in probs_ordenada[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(probs_ordenada)-5} mas...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecd652",
   "metadata": {},
   "source": [
    "¿Estas probabilidades parecen razonables? ¿Cómo podemos evaluar sistemáticamente la calidad de nuestro modelo?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19900ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee85ac1",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "Las técnicas de suavizado, como [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), a menudo se agregan a los modelos de lenguaje de n-gramas para manejar probabilidades de 0. ¿Cómo podrías modificar tu código para incluir el suavizado? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15647cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2147c1",
   "metadata": {},
   "source": [
    "### Evaluación de un modelo de lenguaje n-gramas\n",
    "\n",
    "En esta parte de la actividad, evaluarás la calidad de un modelo de lenguaje de n-gramas utilizando perplejidad.\n",
    "\n",
    "Hemos creado varios modelos de lenguaje de n-gramas y proporcionamos una implementación para calcular las probabilidades. La implementación incluye el suavizado Laplaciano, que asigna cierta probabilidad a secuencias que nunca se encontraron durante el entrenamiento.\n",
    "\n",
    "Primero, revisa la implementación siguiente para asegurarte de que tenga sentido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.001, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.modelo = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def obtener_prob(self, contexto, token):\n",
    "        contexto = tuple(contexto[-self.n+1:])\n",
    "        while len(contexto) < (self.n-1):\n",
    "            contexto = (BOS,) + contexto\n",
    "        contexto = tuple((c if c in self.V else OOV) for c in contexto)\n",
    "        if token not in self.V:\n",
    "            token = OOV\n",
    "        if contexto in self.modelo:\n",
    "            # Maximum Likelihood Estimation - Laplace Smoothing\n",
    "            conteo = self.modelo[contexto].get(token, 0)\n",
    "            prob = (conteo + self.smoothing) / (sum(self.modelo[contexto].values()) + self.smoothing * len(self.V))\n",
    "        else:\n",
    "            prob = 1 / len(self.V)\n",
    "        if self.verbose:\n",
    "            print(f'{prob:.4n}', *contexto, '->', token)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_unigrama = NGramLM('arthur-conan-doyle.tok.train.n1.pkl')\n",
    "modelo_bigrama = NGramLM('arthur-conan-doyle.tok.train.n2.pkl')\n",
    "modelo_trigrama = NGramLM('arthur-conan-doyle.tok.train.n3.pkl')\n",
    "modelo_4grama = NGramLM('arthur-conan-doyle.tok.train.n4.pkl')\n",
    "modelo_5grama = NGramLM('arthur-conan-doyle.tok.train.n5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564b1a6",
   "metadata": {},
   "source": [
    "Ahora es el momento de ver qué tan bien se ajustan estos modelos a nuestros datos. Usaremos `perplejidad` para este cálculo, pero depende de ti implementarlo a continuación.\n",
    "\n",
    "Recuerda la fórmula de la perplejidad de la clase:\n",
    "\n",
    "$$\n",
    "perplejidad = 2^{\\frac{-1}{n}\\sum \\log_2(P(w_i|w_{<i}))}\n",
    "$$\n",
    "\n",
    "Sugerencia: querrás usar la función [`math.log2`](https://docs.python.org/3/library/math.html#math.log2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "def perplejidad(modelo: NGramLM, textos: List[Tuple[str]]) -> float:\n",
    "    entropia = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    pass\n",
    "\n",
    "    return perplejidad\n",
    "\n",
    "perplejidad(modelo_unigrama, [('My', 'dear', 'Watson', '.'), ('Come', 'over', 'here', '!')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert round(perplejidad(modelo_unigrama, [('My', 'dear', 'Watson')])) == 7530\n",
    "assert round(perplejidad(modelo_bigrama, [('My', 'dear', 'Watson')])) == 18\n",
    "assert round(perplejidad(modelo_trigrama, [('My', 'dear', 'Watson')])) == 484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d4a7a",
   "metadata": {},
   "source": [
    "Ahora veamos qué tan bien se ajusta el modelo a un conjunto de prueba disponible. Los datos de prueba cubren algunas de las historias y representan aproximadamente el 12% de los datos totales.\n",
    "\n",
    "Tu tarea es imprimir la perplejidad de los modelos de unigrama, bigrama, trigrama, 4 gramas y 5 gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44577c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_test = []\n",
    "with open('arthur-conan-doyle.tok.test.txt', 'rt') as fin:\n",
    "    for linea in fin:\n",
    "        toks_test.append(list(linea.split()))\n",
    "\n",
    "perplejidad_test_unigrama = perplejidad(modelo_unigrama, toks_test)\n",
    "print(f\"Perplejidad del conjunto de pruebas (Unigrama): {perplejidad_test_unigrama}\")\n",
    "\n",
    "perplejidad_test_bigrama = perplejidad(modelo_bigrama, toks_test)\n",
    "print(f\"Perplejidad del conjunto de pruebas (Bigrama): {perplejidad_test_bigrama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d2c7b",
   "metadata": {},
   "source": [
    "Deberías ver que la perplejidad por el modelo bigrama es menor que por los demás. ¿Qué indica esto?\n",
    "\n",
    "Es una mala idea determinar la calidad de un modelo basándose en la perplejidad de los datos que se utilizaron para el entrenamiento. A continuación, evalúa los mismos cinco modelos utilizando los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e88d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73b0cb",
   "metadata": {},
   "source": [
    "Deberías ver que obtienes perplejidades mucho menores al medir los datos de entrenamiento, especialmente para los modelos con valores más grandes de `n`. Esto sugiere que el modelo se está sobreajustando a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51ef9f",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "* En los modelos que exploramos anteriormente, utilizamos suavizado. ¿Qué sucede con los cálculos de perplejidad cuando no se aplica el suavizado? Puedes probar esto estableciendo `smoothing = 0`.\n",
    "\n",
    " * A veces se utiliza el suavizado interpolado o de \"back-off\" en los modelos de lenguaje de n-gramas. Esta técnica calcula la probabilidad promedio ponderada de modelos con diferentes valores de `n`. Intenta implementar esto. ¿Cómo afecta esto la perplejidad en el conjunto de pruebas retenido? ¿Qué pasa con la perplejidad sobre los datos de entrenamiento?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f09cb",
   "metadata": {},
   "source": [
    "### Generación de texto con un modelo de lenguaje n-grama\n",
    "\n",
    "En esta parte de la actividad, implementará dos estrategias de generación de texto.\n",
    "\n",
    "Aquí hay una revisión de la implementación del modelo de lenguaje n-gramas incluyendo una función `obtener_prob_dist()`, que devuelve las probabilidades de todos los tokens dado el contexto.\n",
    "\n",
    "Revisa la implementación para asegurarse de comprenderla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.001, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.modelo = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def obtener_prob_dist(self, contexto):\n",
    "        contexto = tuple(contexto[-self.n+1:])\n",
    "        while len(contexto) < (self.n-1):\n",
    "            contexto = (BOS,) + contexto\n",
    "        contexto = tuple((c if c in self.V else OOV) for c in contexto)\n",
    "        if contexto in self.modelo:\n",
    "            norm = sum(self.modelo[contexto].values()) + self.smoothing * len(self.V)\n",
    "            prob_dist = {k: (c + self.smoothing) / norm for k, c in self.modelo[contexto].items()}\n",
    "            for word in self.V - prob_dist.keys():\n",
    "                prob_dist[word] = self.smoothing / norm\n",
    "        else:\n",
    "            prob = 1 / len(self.V)\n",
    "            prob_dist = {k: prob for k in self.V}\n",
    "        prob_dist = dict(sorted(prob_dist.items(), key=lambda x: (-x[1], x[0])))\n",
    "        return prob_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605ed3e",
   "metadata": {},
   "source": [
    "Echemos un vistazo a algunas de las distribuciones de probabilidad. ¿Son razonables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc30d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_bigrama.obtener_prob_dist(['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92913840",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_trigrama.obtener_prob_dist(['my', 'dear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6c24a",
   "metadata": {},
   "source": [
    "Ahora tenemos todas las herramientas que necesitamos para comenzar a generar texto\n",
    "\n",
    "Comenzaremos con un enfoque simple de generación greedy. Tu tarea es implementar la generación greedy a continuación.\n",
    "\n",
    "Nota: tenemos un parámetro `max_length` para asegurarnos de que el proceso de generación no continúe para siempre. Puede detenerse cuando tu secuencia alcance un token `<EOS>` o tenga la longitud máxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff038020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def generacion_greedy(modelo: NGramLM, contexto: List[str], max_longitud: int = 100) -> List[str]:\n",
    "    secuencia_generada = contexto.copy()\n",
    "    for _ in range(max_longitud):\n",
    "        prob_dist = modelo.obtener_prob_dist(secuencia_generada)\n",
    "        prox_palabra = max(prob_dist, key=prob_dist.get)\n",
    "        secuencia_generada.append(prox_palabra)\n",
    "        if prox_palabra == EOS:\n",
    "            break\n",
    "    return secuencia_generada\n",
    "\n",
    "generacion_greedy(modelo_trigrama, ['\"\"', 'My', 'dear', 'Watson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3547a",
   "metadata": {},
   "source": [
    "¿Cómo se ve la generación? ¿Es determinista? ¿Son interesantes las secuencias generadas?\n",
    "\n",
    "Considera probar diferentes tipos de modelos. ¿Cuáles son las diferentes cualidades que ves en los modelos de unigrama, bigrama, trigrama, 4 gramas y 5 gramos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a25def",
   "metadata": {},
   "source": [
    "Ahora es el momento de implementar el muestreo.\n",
    "\n",
    "Ahora incluimos un argumento `topk`. Esto reduce el conjunto candidato de probabilidades a solo los elementos `topk` de mayor probabilidad. Esto ayuda a reducir la posibilidad de generar secuencias muy improbables.\n",
    "\n",
    "Nota: considera usar [`random.choices`](https://docs.python.org/3/library/random.html#random.choices) para ayudar en el muestreo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def generacion_muestreo(modelo: NGramLM, contexto: List[str], max_longitud: int = 100, topk=10) -> List[str]:\n",
    "    secuencia_generada = contexto.copy()\n",
    "    for _ in range(max_longitud):\n",
    "        prob_dist = modelo.obtener_prob_dist(secuencia_generada)\n",
    "        top_palabras = sorted(prob_dist, key=prob_dist.get, reverse=True)[:topk]\n",
    "        prox_palabra = random.choice(top_palabras)\n",
    "        secuencia_generada.append(prox_palabra)\n",
    "        if prox_palabra == EOS:\n",
    "            break\n",
    "    return secuencia_generada\n",
    "\n",
    "generacion_muestreo(modelo_trigrama, ['\"\"', 'My', 'dear', 'Watson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902041f",
   "metadata": {},
   "source": [
    "Ahora compare cualitativamente tu generación de muestreo con la generación greedy.\n",
    "\n",
    "¿Qué notas sobre las secuencias generadas? ¿Cómo se comportan los modelos de diferentes tamaños? ¿Cuál es el efecto de `topk`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee90f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d85480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def generacion_muestreo(modelo: NGramLM, contexto: List[str], max_longitud: int = 100, topk=100) -> List[str]:\n",
    "    secuencia_generada = contexto.copy()\n",
    "    for _ in range(max_longitud):\n",
    "        prob_dist = modelo.obtener_prob_dist(secuencia_generada)\n",
    "        top_palabras = sorted(prob_dist, key=prob_dist.get, reverse=True)[:topk]\n",
    "        prox_palabra = random.choice(top_palabras)\n",
    "        secuencia_generada.append(prox_palabra)\n",
    "        if prox_palabra == EOS:\n",
    "            break\n",
    "    return secuencia_generada\n",
    "\n",
    "\n",
    "generacion_muestreo(modelo_4grama, ['\"\"', 'My', 'dear', 'Watson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638decab",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "  - Implementa una estrategia de [beam search](https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d). ¿Tiende a conducir a resultados cualitativamente mejores que los otros dos enfoques?\n",
    "  - ¿Qué estrategia podrías seguir para encontrar eficientemente la secuencia más probable para un modelo de lenguaje de n-gramas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
