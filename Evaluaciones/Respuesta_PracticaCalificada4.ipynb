{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Respuestas de la Práctica Calificada 4 CC0C2**\n",
        "\n",
        "#### **1. Atención y longitud de secuencia (4 pts)**\n",
        "\n",
        "**(a) ¿Por qué *self-attention* maneja mejor dependencias largas que una RNN clásica?**\n",
        "\n",
        "En una RNN \"pura\" (o incluso en una LSTM), la información fluye paso a paso:\n",
        "token 1 -> estado 1 -> estado 2 -> ... -> estado T.\n",
        "\n",
        "Si la dependencia es entre la palabra en la posición 5 y la de la posición 100, la señal tiene que atravesar muchas multiplicaciones de matrices. Eso genera:\n",
        "\n",
        "* **Camino largo de gradientes** (profundidad efectiva T), lo que favorece el **desvanecimiento o explosión de gradientes**.\n",
        "* Dificultad práctica para \"recordar\" detalles que vienen de muy atrás.\n",
        "\n",
        "En *self-attention*, en cambio, cada token puede \"atender\" directamente a **todos los demás tokens de la secuencia en una sola capa**. El camino entre la posición 5 y la 100 es básicamente de **longitud 1** dentro de una capa de atención (o $\\log(T)$ si piensas en varias capas apiladas). Eso:\n",
        "\n",
        "* Facilita la propagación de gradientes.\n",
        "* Permite modelar dependencias largas sin tener que \"transportar\" información paso a paso.\n",
        "* Además, se puede paralelizar el cálculo de todas las posiciones, cosa que no pasa en RNNs.\n",
        "\n",
        "\n",
        "**(b) ¿Qué se pierde sin codificación posicional? Ejemplo de dos frases.**\n",
        "\n",
        "La atención pura sobre los embeddings de palabras, sin ninguna información de posición, es **invariante a permutaciones**: ve un *multiset* de vectores, no una secuencia ordenada.\n",
        "Es decir, \"perro muerde hombre\" y \"hombre muerde perro\" se verían igual si solo se consideran las mismas tres palabras.\n",
        "\n",
        "Sin codificación posicional, el modelo:\n",
        "\n",
        "* No sabe **qué viene antes o después**.\n",
        "* No puede distinguir cambios de significado basados en el orden.\n",
        "* Empeora en tareas donde el orden importa (sintaxis, rol semántico, etc.).\n",
        "\n",
        "Ejemplo en español:\n",
        "\n",
        "1. \"El perro muerde al hombre.\"\n",
        "2. \"El hombre muerde al perro.\"\n",
        "\n",
        "Las palabras son las mismas, pero el sujeto y el objeto cambian. Sin posiciones, el modelo no puede saber quién muerde a quién.\n",
        "\n",
        "**(c) Una idea para hacer la atención más eficiente en secuencias largas y su *trade-off*.**\n",
        "\n",
        "Una idea clásica: **atención local/dispersa (sparse)**.\n",
        "\n",
        "* En lugar de que cada token atienda a todos los demás (complejidad $O(n^2))$, cada token solo atiende a:\n",
        "\n",
        "  * una **ventana local** (por ejemplo $\\pm$ posiciones), y/o\n",
        "  * algunos **tokens \"globales\"** (CLS, títulos, etc.).\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "* Complejidad baja, por ejemplo $O(n*w)$ con $w \\ll n$.\n",
        "* Permite llegar a secuencias mucho más largas sin reventar memoria.\n",
        "\n",
        "*Trade-off*:\n",
        "\n",
        "* El modelo puede **perder dependencias muy largas y sutiles** si dos tokens distantes no están conectados por la ventana local o por los pocos tokens globales.\n",
        "* En general, se gana eficiencia a costa de **aproximar** la atención completa y, por tanto, del potencial de calidad en algunos casos.\n",
        "\n",
        "\n",
        "#### **2. Clasificación vs traducción con Transformers (4 pts)**\n",
        "\n",
        "**(a) Diferencia de flujo entre Transformer para clasificación vs traducción.**\n",
        "\n",
        "* **Clasificación de texto** :\n",
        "\n",
        "  * Se suele usar solo la parte de **encoder** (tipo BERT).\n",
        "  * Se mete la secuencia de entrada, se obtienen representaciones contextualizadas.\n",
        "  * Se toma el embedding de un token especial (por ejemplo, `[CLS]`) o un *pooling* sobre todos los tokens.\n",
        "  * Ese vector se pasa por una o varias capas densas para sacar una **clase** (salida de dimensión fija).\n",
        "\n",
        "* **Traducción**:\n",
        "\n",
        "  * Usa arquitectura **encoder-decoder**.\n",
        "  * El **encoder** procesa la oración origen y produce representaciones para cada token.\n",
        "  * El **decoder** genera la oración destino **paso a paso**, usando:\n",
        "\n",
        "    * *self-attention* en los tokens generados hasta ahora (con máscara causal).\n",
        "    * *cross-attention* sobre las salidas del encoder.\n",
        "  * La salida es una **secuencia** de tokens, no un solo vector.\n",
        "\n",
        "\n",
        "**(b) Rol de la atención encoder-decoder y del *masking* en el decoder.**\n",
        "\n",
        "* **Atención encoder-decoder**:\n",
        "\n",
        "  * Permite que el decoder, en cada paso, \"mire\" a las representaciones del encoder.\n",
        "  * Funciona como un mecanismo de **alineación**: para generar la siguiente palabra en el idioma destino, el modelo pondera qué partes de la oración origen son más relevantes.\n",
        "  * Esto ayuda a mantener la **fidelidad** a la entrada y a manejar reordenamientos sintácticos.\n",
        "\n",
        "* **Masking en el decoder (máscara causal)**:\n",
        "\n",
        "  * En el *self-attention* del decoder, se impide que la posición t vea posiciones futuras (> t).\n",
        "  * Así se implementa el carácter **autoregresivo**: cada token solo depende de los anteriores.\n",
        "  * Evita el \"hacer trampa\" viendo el futuro durante el entrenamiento.\n",
        "\n",
        "**(c) Reutilizar el sistema de traducción para un clasificador en un idioma de bajo recurso.**\n",
        "\n",
        "Ideas clave:\n",
        "\n",
        "1. El encoder de un sistema de traducción ya ha aprendido una representación **semántica** razonable para el idioma de entrada (o para varios idiomas, si es multilingüe).\n",
        "2. Si el sistema de traducción soporta el nuevo idioma (aunque con pocos datos), podemos reutilizar su encoder como **extractor de características**.\n",
        "\n",
        "Un posible enfoque:\n",
        "\n",
        "* **Compartir vocabulario** entre idiomas (BPE/SentencePiece multilingüe).\n",
        "* Tomar el **encoder** del sistema de traducción (que ya ve el idioma de bajo recurso) y:\n",
        "\n",
        "  * congelarlo total o parcialmente,\n",
        "  * añadir una capa de clasificación arriba (`[CLS]` o *pooling* de los estados del encoder),\n",
        "  * entrenar ese cabezal con las pocas etiquetas disponibles (few-shot).\n",
        "* Para mejorar, se puede hacer algo de **preentrenamiento adicional en texto sin etiqueta** del nuevo idioma usando un objetivo tipo MLM o denoising, reutilizando el encoder.\n",
        "\n",
        "En resumen: el encoder del traductor hace de **backbone multilingüe** y solo se añade/adapta una capa de clasificación ligera para el nuevo idioma.\n",
        "\n",
        "#### **3. BERT, preentrenamiento y fine-tuning (5 pts)**\n",
        "\n",
        "**(a) Objetivo de *masked language modeling* (MLM) y por qué ayuda a clasificación.**\n",
        "\n",
        "En MLM:\n",
        "\n",
        "* Se enmascaran aleatoriamente algunos tokens (por ejemplo, 15 %) de la secuencia.\n",
        "* El modelo debe predecir estos tokens enmascarados usando el **contexto completo** (izquierda y derecha).\n",
        "\n",
        "Esto obliga al modelo a aprender:\n",
        "\n",
        "* Representaciones **bidireccionales** y sensibles tanto al pasado como al futuro.\n",
        "* Relaciones semánticas y sintácticas profundas, porque para adivinar una palabra hay que entender qué \"pega\" en esa posición.\n",
        "\n",
        "Para clasificación:\n",
        "\n",
        "* El embedding de `[CLS]` (o de la secuencia agregada) se beneficia de ese preentrenamiento general.\n",
        "* Con pocas etiquetas, basta con **ajustar una capa de clasificación** encima de esas representaciones ricas para obtener buen rendimiento.\n",
        "\n",
        "\n",
        "**(b) Adaptar el modelo genérico al dominio legal con textos sin etiqueta.**\n",
        "\n",
        "Esto es un caso típico de **preentrenamiento adicional por dominio** (*domain-adaptive pretraining* o DAPT):\n",
        "\n",
        "1. Toma el BERT genérico preentrenado en texto general.\n",
        "2. Continúa el entrenamiento usando **solo texto legal sin etiquetas**, con el mismo objetivo de MLM (enmascarar y predecir palabras).\n",
        "3. Usa una tasa de aprendizaje más baja y un número moderado de pasos para no destruir el conocimiento general.\n",
        "\n",
        "Resultado:\n",
        "\n",
        "* El modelo aprende vocabulario, giros y patrones propios del dominio legal.\n",
        "* Luego, al hacer *fine-tuning* supervisado para tareas legales (clasificación de tipo de proceso, detección de cláusulas, etc.), necesita menos datos etiquetados.\n",
        "\n",
        "**(c) Qué cambiar \"arriba\" del modelo para clasificar tipo de proceso y riesgos de congelar vs actualizar.**\n",
        "\n",
        "Para clasificar \"tipo de proceso judicial\":\n",
        "\n",
        "* Se añade una **capa de clasificación** encima del embedding `[CLS]`:\n",
        "  $$\n",
        "  h_{\\text{CLS}} \\to \\text{Linear} \\to \\text{Softmax}(N_\\text{clases})\n",
        "  $$\n",
        "* Ese cabezal se entrena con las etiquetas disponibles.\n",
        "\n",
        "Sobre congelar vs actualizar:\n",
        "\n",
        "* **Congelar todo BERT y entrenar solo la capa de clasificación**:\n",
        "\n",
        "  * Ventaja: menos riesgo de sobreajuste, muy barato.\n",
        "  * Problema: si el dominio legal es muy distinto de los datos originales, la representación puede no ser suficientemente especializada.\n",
        "\n",
        "* **Actualizar todas las capas con muy pocos datos etiquetados**:\n",
        "\n",
        "  * Ventaja: máxima capacidad de adaptación.\n",
        "  * Problema: alto riesgo de **sobreajuste** y de **olvido catastrófico** del conocimiento general; además puede volverse inestable el entrenamiento.\n",
        "\n",
        "* **Punto intermedio habitual**:\n",
        "\n",
        "  * Ajustar solo las últimas capas del encoder (o usar tasas de aprendizaje diferenciadas: pequeña para el encoder, mayor para el cabezal de clasificación).\n",
        "  * Equilibra capacidad de adaptación y estabilidad.\n",
        "\n",
        "\n",
        "**(d) \"Olvido\" del conocimiento general y cómo mitigarlo.**\n",
        "\n",
        "El fenómeno es el **olvido catastrófico**:\n",
        "\n",
        "* Al entrenar el modelo únicamente en datos legales, las actualizaciones de parámetros se alinean con ese dominio.\n",
        "* El modelo puede \"olvidar\" patrones útiles para otros dominios o incluso perder habilidades generales (por ejemplo, entender lenguaje cotidiano).\n",
        "\n",
        "Formas de mitigarlo:\n",
        "\n",
        "* **Mezclar datos**: durante el preentrenamiento por dominio, usar un *mix* de texto legal + parte de texto general.\n",
        "* **Regularización hacia el modelo original**:\n",
        "\n",
        "  * L2 hacia los pesos iniciales (\"no te alejes mucho del BERT base\").\n",
        "  * Métodos tipo **Elastic Weight Consolidation (EWC)**, que penalizan cambiar demasiado los pesos cruciales.\n",
        "* **Mantener dos modelos**:\n",
        "\n",
        "  * Uno general (sin tocar) y otro adaptado a lo legal.\n",
        "  * Usar el que corresponda según la tarea.\n",
        "\n",
        "\n",
        "#### **4. GPT y generación controlada (3 pts)**\n",
        "\n",
        "**(a) Diferencia de objetivos GPT vs BERT y por qué GPT es más natural para generación.**\n",
        "\n",
        "* **BERT**:\n",
        "\n",
        "  * Objetivo principal: *masked language modeling* (más eventualmente NSP).\n",
        "  * Es **bidireccional** y ve toda la secuencia a la vez, con algunos tokens enmascarados.\n",
        "  * Excelente como **encoder** para clasificación, extracción, QA, etc., pero no está entrenado explícitamente a generar texto token por token.\n",
        "\n",
        "* **GPT**:\n",
        "\n",
        "  * Objetivo: **modelado de lenguaje autoregresivo**.\n",
        "  * Aprende a predecir el siguiente token dado todos los anteriores:\n",
        "    $$\n",
        "    p(x_t \\mid x_1, \\dots, x_{t-1})\n",
        "    $$\n",
        "  * Es **unidireccional (causal)**: siempre mira hacia atrás.\n",
        "\n",
        "Para generación:\n",
        "\n",
        "* GPT fue entrenado precisamente en el **acto de continuar una secuencia**, que es lo que hacemos al generar texto.\n",
        "* Por eso, en inferencia basta con alimentar un prompt inicial y repetir:\n",
        "  *predecir el siguiente token -> añadirlo -> repetir*, lo que coincide con su objetivo de entrenamiento.\n",
        "\n",
        "\n",
        "**(b) Dos mecanismos (más allá solo de \"prompting\") para controlar longitud y reducir alucinaciones.**\n",
        "\n",
        "1. **Control mediante el propio decodificador (decoding constraints / hiperparámetros)**\n",
        "\n",
        "   * Longitud:\n",
        "\n",
        "     * Fijar `max_new_tokens`, `min_length`, o usar **penalización de longitud** (por ejemplo en *beam search*) para evitar que la probabilidad favorezca secuencias ridículamente cortas o largas.\n",
        "   * Estabilidad y coherencia:\n",
        "\n",
        "     * Ajustar **temperatura**, **top-k**, **top-p (nucleus sampling)**:\n",
        "\n",
        "       * Temperatura baja + top-k/top-p restringidos -> menos diversidad pero más coherencia y menos \"fantasía\".\n",
        "       * Reduce la probabilidad de saltar a regiones poco probables del espacio de salida.\n",
        "\n",
        "2. **Decodificación condicionada/con restricciones externas (grounded decoding)**\n",
        "\n",
        "   * Integrar el modelo con una **fuente de conocimiento**:\n",
        "\n",
        "     * RAG: recuperar documentos relevantes y restringir la respuesta a información que aparezca en esos documentos (por ejemplo, penalizando tokens que introducen entidades no presentes en el contexto).\n",
        "     * **Constrained decoding**: imponer restricciones de estilo o formato (por ejemplo, gramáticas o esquemas JSON) de modo que el modelo solo pueda producir secuencias válidas bajo unas reglas.\n",
        "   * Para alucinaciones:\n",
        "\n",
        "     * Añadir un **verificador** (otro modelo o regla) que rechace continuaciones inconsistente con los datos y fuerce replantear la respuesta.\n",
        "\n",
        "#### **5. Adaptadores y LoRA (4 pts)**\n",
        "\n",
        "**(a) Diferencia entre *full fine-tuning* y adaptadores/LoRA (qué se entrena).**\n",
        "\n",
        "* **Full fine-tuning**:\n",
        "\n",
        "  * Se actualizan prácticamente **todos los parámetros** del modelo base (todas las capas de atención, FFN, embeddings, etc.).\n",
        "  * El modelo resultante es una nueva copia especializada.\n",
        "\n",
        "* **Adaptadores**:\n",
        "\n",
        "  * Se insertan pequeños módulos adicionales (típicamente con estructura botella: proyección a menor dimensión + no linealidad + proyección de vuelta).\n",
        "  * Durante el entrenamiento, **los pesos originales del modelo se congelan** y **solo se entrenan las capas adaptadoras (y a veces LayerNorm / cabezal de salida)**.\n",
        "  * En inferencia, la salida es la combinación del modelo base + adaptadores.\n",
        "\n",
        "* **LoRA**:\n",
        "\n",
        "  * En vez de modificar por completo cada matriz W grande, se entrena una **actualización de bajo rango**:\n",
        "    $$\n",
        "    W' = W + \\Delta W,\\quad \\Delta W = A B\n",
        "    $$\n",
        "    donde A y B son matrices de bajo rango entrenables.\n",
        "  * El W original permanece congelado; sólo **A y B** se entrenan.\n",
        "\n",
        "En ambos casos (adaptadores, LoRA), la gran mayoría de parámetros del modelo base se mantienen fijos.\n",
        "\n",
        "\n",
        "**(b) Ventajas en almacenamiento y despliegue.**\n",
        "\n",
        "* **Almacenamiento**:\n",
        "\n",
        "  * Full fine-tuning -> necesitas guardar una copia completa del modelo por tarea (por ejemplo, 7B parámetros por cada tarea).\n",
        "  * Adaptadores/LoRA -> guardas:\n",
        "\n",
        "    * una sola copia del modelo base,\n",
        "    * más módulos pequeños por tarea (típicamente ≪ 10 % del tamaño, a veces 1-2 %).\n",
        "  * Esto permite tener muchos \"sabores\" de modelo especializados ocupando mucho menos espacio total.\n",
        "\n",
        "* **Despliegue**:\n",
        "\n",
        "  * Puedes cargar el modelo base **una sola vez** en memoria y luego \"montar\" distintos adaptadores/LoRA según la tarea o el cliente, cambiando solo un pequeño conjunto de pesos.\n",
        "  * Facilita:\n",
        "\n",
        "    * cambiar de tarea **en caliente** (hot-swap),\n",
        "    * escenarios multi-tenant,\n",
        "    * actualizaciones rápidas (subir solo adaptadores, no todo el modelo).\n",
        "\n",
        "**(c) ¿Por qué PEFT puede ser insuficiente en una tarea muy rara? Explicación técnica.**\n",
        "\n",
        "Hay situaciones donde los cambios necesarios en el espacio de funciones son demasiado \"grandes\" para expresarlos con pequeñas capas adicionales o actualizaciones de bajo rango:\n",
        "\n",
        "* **Dominio extremadamente alejado del preentrenamiento**:\n",
        "\n",
        "  * Por ejemplo, pasar de un lenguaje natural general a un lenguaje formal simbólico muy exótico o a un dominio logístico/financiero ultra específico sin casi solapamiento de vocabulario.\n",
        "  * El modelo base puede no tener las **representaciones de alto nivel necesarias**, y pequeños parches (LoRA/adapters) no son suficientes; habría que reentrenar gran parte del backbone.\n",
        "\n",
        "* **Necesidad de cambios de alta dimensión**:\n",
        "\n",
        "  * LoRA impone que la actualización de cada matriz sea de **bajo rango**; esto restringe el tipo de transformaciones posibles.\n",
        "  * Si la adaptación requiere una transformación de los pesos que **no es bien aproximable por una matriz de bajo rango** (o por unos pocos adaptadores de tamaño pequeño), el modelo no alcanza el rendimiento deseado.\n",
        "\n",
        "* **Cambio profundo de vocabulario o tokenización**:\n",
        "\n",
        "  * Si la tarea rara exige un vocabulario muy distinto (por ejemplo, nuevo alfabeto, símbolos técnicos, lenguaje multimodal), quizá haya que cambiar embeddings, capas de entrada/salida u otras partes estructurales que PEFT normalmente no toca.\n",
        "\n",
        "En resumen, PEFT es excelente cuando la tarea es \"cercana\" al conocimiento ya codificado en el modelo. Cuando la tarea implica un **salto grande en distribución o en capacidades**, puede ser necesario un *fine-tuning* más intrusivo (o incluso reentrenamiento parcial/mayor) para introducir esas habilidades nuevas.\n"
      ],
      "metadata": {
        "id": "PSbTotIFUx70"
      }
    }
  ]
}
