{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255f746b",
   "metadata": {},
   "source": [
    "### **Respuestas de ejercicios 2**\n",
    "**Curso:** CC3S2 - NLP/LLMs  \n",
    "**Archivo base:** `Cargador_datos_NLP.ipynb`  \n",
    "\n",
    "\n",
    "> **Instrucciones**  \n",
    "> - Completa los ejercicios **sin pegar soluciones completas** desde otros repositorios.  \n",
    "> - Usa esta plantilla como **cuaderno de trabajo**, agrega celdas adicionales si lo necesitas.  \n",
    "> - Captura evidencias (shapes, fragmentos de tensores, tablas breves).  \n",
    "> - Mantén un estilo claro, comenta lo esencial y **no expongas credenciales**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344693da",
   "metadata": {},
   "source": [
    "#### **Verificación rápida de entorno**\n",
    "\n",
    "Ejecuta la celda y anota las versiones relevantes. Si hay warnings importantes, coméntalos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ejecuta y conserva la salida en el notebook\n",
    "import sys, platform, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Plataforma:\", platform.platform())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "# Si tienes torchtext / spacy / transformers instalados, descomenta para registrar versiones\n",
    "try:\n",
    "    import torchtext\n",
    "    print(\"torchtext:\", torchtext.__version__)\n",
    "except Exception as e:\n",
    "    print(\"torchtext: no disponible ->\", type(e).__name__)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"spacy:\", spacy.__version__)\n",
    "except Exception as e:\n",
    "    print(\"spacy: no disponible ->\", type(e).__name__)\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"transformers: no disponible ->\", type(e).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf3140",
   "metadata": {},
   "source": [
    "\n",
    "#### **(Opcional) Semillas de reproducibilidad**\n",
    "Usa esta celda si planeas comparar resultados entre corridas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fija semillas si lo requieres (opcional)\n",
    "import random, numpy as np, torch\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(\"Semillas fijadas:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a664b",
   "metadata": {},
   "source": [
    "#### **Parte B-Mini-labs con `Cargador_datos_NLP.ipynb`**\n",
    "\n",
    "> Meta: dominar tokenización, vocabularios, `collate_fn`, *padding*, BOS/EOS y batching.  \n",
    "> Notación: usa **PyTorch** (y torchtext/spacy si están disponibles en el entorno).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47210c",
   "metadata": {},
   "source": [
    "**Ejercicio 8 - Dataset mínimo y DataLoader**\n",
    "\n",
    "**Objetivo:** crear `CustomDataset` (devuelve cadenas) y `DataLoader`.  \n",
    "**Pasos sugeridos:**  \n",
    "1. Define 6-8 oraciones simples.  \n",
    "2. Implementa `__len__` y `__getitem__`.  \n",
    "3. Itera sobre 2 lotes y muestra su contenido.  \n",
    "**Entrega:** pantallazo de lotes.  \n",
    "**Mini-check:** `shuffle=True` debe cambiar el orden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementa un Dataset mínimo y un DataLoader\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, sentences: List[str]):\n",
    "        self.sentences = sentences\n",
    "    def __len__(self):\n",
    "        # TODO: retorna la cantidad de elementos\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, idx: int):\n",
    "        # TODO: retorna la cadena en la posición idx\n",
    "        return self.sentences[idx]\n",
    "\n",
    "sentences = [\n",
    "    \"Hola desde Lima\",\n",
    "    \"El NLP requiere buenos datos\",\n",
    "    \"El padding alinea tensores\",\n",
    "    \"spacy mejora la tokenización\",\n",
    "    \"Multi30k contiene pares de oraciones\",\n",
    "    \"HF integra tokenizadores\",\n",
    "    \"PyTorch DataLoader crea lotes\",\n",
    "    \"Bucketing reduce padding waste\"\n",
    "]\n",
    "\n",
    "ds = CustomTextDataset(sentences)\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dl):\n",
    "    print(f\"Lote {i}:\", batch)\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "assert len(ds) == len(sentences) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce8be8",
   "metadata": {},
   "source": [
    "**Ejercicio 9-Tokenizador + vocabulario**\n",
    "\n",
    "**Objetivo:** convertir a tensores de índices.  \n",
    "**Sugerencias:**  \n",
    "- Usa `get_tokenizer(\"basic_english\")` si no tienes spaCy.  \n",
    "- Construye el vocabulario con `build_vocab_from_iterator` (incluye tokens especiales).  \n",
    "- Devuelve `torch.tensor(ids)` desde el Dataset.  \n",
    "**Entrega:** 2 ejemplos (tensor + lista de tokens).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tokenizar y construir vocabulario\n",
    "import torch\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "try:\n",
    "    from torchtext.data.utils import get_tokenizer\n",
    "    from torchtext.vocab import build_vocab_from_iterator\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"torchtext no está disponible en este entorno\") from e\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "vocab = build_vocab_from_iterator(yield_tokens(ds), specials=specials, special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "UNK_IDX = vocab['<unk>']\n",
    "PAD_IDX = vocab['<pad>']\n",
    "BOS_IDX = vocab['<bos>']\n",
    "EOS_IDX = vocab['<eos>']\n",
    "\n",
    "def text_to_ids(text: str):\n",
    "    tokens = tokenizer(text)\n",
    "    return [BOS_IDX] + [vocab[token] for token in tokens] + [EOS_IDX], tokens\n",
    "\n",
    "# Muestra dos ejemplos\n",
    "for s in sentences[:2]:\n",
    "    ids, toks = text_to_ids(s)\n",
    "    print(\"Texto:\", s)\n",
    "    print(\"Tokens:\", toks)\n",
    "    print(\"IDs:\", ids, \"\\n\")\n",
    "\n",
    "assert isinstance(PAD_IDX, int) and PAD_IDX >= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac17532",
   "metadata": {},
   "source": [
    "**Ejercicio 10 - `collate_fn` con `pad_sequence` (batch_first=True)**\n",
    "\n",
    "**Objetivo:** homogeneizar longitudes del lote.  \n",
    "**Entrega:** `shape` y lote *padded* (vista parcial) e identificar `PAD_IDX`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b01e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementa collate_fn con pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def encode(texts):\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        ids, _ = text_to_ids(t)\n",
    "        out.append(torch.tensor(ids, dtype=torch.long))\n",
    "    return out\n",
    "\n",
    "def collate_fn_batch_first(batch):\n",
    "    seqs = encode(batch)\n",
    "    batch_padded = pad_sequence(seqs, batch_first=True, padding_value=PAD_IDX)\n",
    "    return batch_padded\n",
    "\n",
    "dl_bf = DataLoader(ds, batch_size=4, shuffle=False, collate_fn=collate_fn_batch_first)\n",
    "batch = next(iter(dl_bf))\n",
    "print(\"Shape (B, T):\", tuple(batch.shape))\n",
    "print(\"Batch (vista parcial):\\n\", batch[:2, :10])\n",
    "print(\"PAD_IDX:\", PAD_IDX)\n",
    "\n",
    "assert batch.dim() == 2 and batch.shape[0] == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381a795",
   "metadata": {},
   "source": [
    "**Ejercicio 11 -`batch_first=False`**\n",
    "\n",
    "**Objetivo:** comparar `(B, T)` vs `(T, B)` y anotar preferencia (1 línea).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementa collate_fn con batch_first=False\n",
    "def collate_fn_time_first(batch):\n",
    "    seqs = encode(batch)\n",
    "    batch_padded = pad_sequence(seqs, batch_first=False, padding_value=PAD_IDX)\n",
    "    return batch_padded\n",
    "\n",
    "dl_tf = DataLoader(ds, batch_size=4, shuffle=False, collate_fn=collate_fn_time_first)\n",
    "batch_tf = next(iter(dl_tf))\n",
    "print(\"Shape (T, B):\", tuple(batch_tf.shape))\n",
    "print(\"Vista parcial:\\n\", batch_tf[:10, :2])\n",
    "\n",
    "assert batch_tf.shape[1] == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfcb93",
   "metadata": {},
   "source": [
    "**Ejercicio 12-Multi30k: primer vistazo**\n",
    "\n",
    "**Objetivo:** cargar el `split='train'` y ver un ejemplo (DE, EN).  \n",
    "> **Nota:** en entornos sin internet, esta celda puede fallar. Comenta la carga y escribe una breve explicación si no puedes ejecutarla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: intenta cargar Multi30k (opcional según entorno)\n",
    "try:\n",
    "    from torchtext.datasets import Multi30k\n",
    "    train_iter = Multi30k(split='train', language_pair=('de','en'))\n",
    "    example = next(iter(train_iter))\n",
    "    print(\"Ejemplo DE:\", example[0][:120], \"...\")\n",
    "    print(\"Ejemplo EN:\", example[1][:120], \"...\")\n",
    "except Exception as e:\n",
    "    print(\"Multi30k no disponible en este entorno ->\", type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2f073",
   "metadata": {},
   "source": [
    "**Ejercicio 13- spaCy tokenizers por idioma**\n",
    "\n",
    "**Objetivo:** tokenizar con mejor calidad (DE/EN).  \n",
    "> **Nota:** requiere modelos spaCy (`de_core_news_sm`, `en_core_web_sm`). Si no puedes instalarlos, compara con `basic_english` y comenta diferencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tokenizar con spaCy si está disponible\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "        nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "        def spacy_tok_de(text): return [t.text for t in nlp_de(text)]\n",
    "        def spacy_tok_en(text): return [t.text for t in nlp_en(text)]\n",
    "        print(\"spaCy OK — muestra rápida:\")\n",
    "        print(\"DE:\", spacy_tok_de(\"Das ist ein kurzer Testsatz.\"))\n",
    "        print(\"EN:\", spacy_tok_en(\"This is a short test sentence.\"))\n",
    "    except OSError as oe:\n",
    "        print(\"Modelos spaCy no instalados:\", oe)\n",
    "except Exception as e:\n",
    "    print(\"spaCy no disponible ->\", type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d4092",
   "metadata": {},
   "source": [
    "**Ejercicio 14-Vocabularios por idioma + BOS/EOS**\n",
    "\n",
    "**Objetivo:** construir `vocab_transform` y aplicar `tensor_transform` para incluir BOS/EOS.  \n",
    "**Entrega:** un ejemplo con índices que incluya BOS/EOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ace34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: arma vocabs por idioma y transforma a tensores con BOS/EOS\n",
    "# Sugerencia: puedes reutilizar 'vocab' anterior para un solo idioma como ejemplo mínimo\n",
    "def tensor_transform_with_bos_eos(text: str):\n",
    "    ids, toks = text_to_ids(text)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "ex = tensor_transform_with_bos_eos(\"Transformers no requieren flip en src.\")\n",
    "print(\"Ejemplo IDs (con BOS/EOS):\", ex[:15], \"...\")\n",
    "assert ex[0].item() == BOS_IDX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00058648",
   "metadata": {},
   "source": [
    "**Ejercicio 15 -`collate_fn` (pares) + DataLoader train/validate**\n",
    "\n",
    "**Objetivo:** empaquetar `(src, tgt)` con *padding* y verificar *shapes*.  \n",
    "**Recomendación:** simula pares `(src, tgt)` con oraciones ES->EN si no tienes dataset real.  \n",
    "**Entrega:** `src.shape`, `tgt.shape` y 1 lote truncado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73060180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: simula pares (src, tgt) y arma un collate de traducción\n",
    "pairs = [\n",
    "    (\"Hola desde Lima\", \"Hello from Lima\"),\n",
    "    (\"El NLP requiere buenos datos\", \"NLP requires good data\"),\n",
    "    (\"El padding alinea tensores\", \"Padding aligns tensors\"),\n",
    "    (\"HF integra tokenizadores\", \"HF integrates tokenizers\"),\n",
    "    (\"DataLoader crea lotes\", \"DataLoader creates batches\"),\n",
    "    (\"spacy mejora la tokenización\", \"spacy improves tokenization\"),\n",
    "    (\"Bucketing reduce padding waste\", \"Bucketing reduces padding waste\"),\n",
    "    (\"Multi30k contiene pares\", \"Multi30k contains pairs\")\n",
    "]\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx): return self.pairs[idx]\n",
    "\n",
    "def encode_pair(src_t, tgt_t):\n",
    "    src_ids, _ = text_to_ids(src_t)\n",
    "    tgt_ids, _ = text_to_ids(tgt_t)\n",
    "    return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn_translation(batch):\n",
    "    src_list, tgt_list = [], []\n",
    "    for src_t, tgt_t in batch:\n",
    "        s, t = encode_pair(src_t, tgt_t)\n",
    "        src_list.append(s); tgt_list.append(t)\n",
    "    src_batch = pad_sequence(src_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "pair_ds = PairDataset(pairs)\n",
    "pair_dl = DataLoader(pair_ds, batch_size=4, shuffle=False, collate_fn=collate_fn_translation)\n",
    "src_b, tgt_b = next(iter(pair_dl))\n",
    "print(\"src.shape:\", tuple(src_b.shape), \"tgt.shape:\", tuple(tgt_b.shape))\n",
    "print(\"src (vista parcial):\\n\", src_b[:2, :12])\n",
    "print(\"tgt (vista parcial):\\n\", tgt_b[:2, :12])\n",
    "\n",
    "assert src_b.shape[0] == tgt_b.shape[0] == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed468a15",
   "metadata": {},
   "source": [
    "#### **Parte C-Retos**\n",
    "\n",
    "> En esta sección solo se entrega el andamiaje (sin soluciones). Completa según tu ritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300dc2d7",
   "metadata": {},
   "source": [
    "**Ejercicio 16 - *Bucketing Sampler* por longitud**\n",
    "\n",
    "**Objetivo:** reducir *padding waste*. Completa las funciones indicadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementa bucketing simple por longitud\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "def sequence_length(x: torch.Tensor) -> int:\n",
    "    # Longitud efectiva (sin PAD) si ya está *padded*; si no, usa len\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        # cuenta tokens distintos a PAD\n",
    "        return int((x != PAD_IDX).sum().item())\n",
    "    return len(x)\n",
    "\n",
    "def make_bins(indices: List[int], lengths: List[int], bin_size: int = 10) -> List[List[int]]:\n",
    "    # TODO: agrupa indices por rangos de longitud (0-9, 10-19, ...)\n",
    "    bins = {}\n",
    "    for idx, L in zip(indices, lengths):\n",
    "        b = (L // bin_size) * bin_size\n",
    "        bins.setdefault(b, []).append(idx)\n",
    "    return [bins[k] for k in sorted(bins.keys())]\n",
    "\n",
    "def bucket_sampler(dataset: Dataset, batch_size: int = 4, bin_size: int = 10) -> Iterable[List[int]]:\n",
    "    # TODO: devuelve listas de índices por lote, agrupadas por bins\n",
    "    indices = list(range(len(dataset)))\n",
    "    # nota: aquí medimos longitud de texto (aprox) antes de tokenizar, a modo de demo\n",
    "    lengths = [len(dataset[i][0]) if isinstance(dataset[i], tuple) else len(dataset[i]) for i in indices]\n",
    "    buckets = make_bins(indices, lengths, bin_size=bin_size)\n",
    "    for bucket in buckets:\n",
    "        # yieldea en trozos del tamaño batch\n",
    "        for i in range(0, len(bucket), batch_size):\n",
    "            yield bucket[i:i+batch_size]\n",
    "\n",
    "print(\"Base lista para bucketing. Completa mediciones y compara radio de padding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c064682-27ae-4ce1-8263-6c968bb7efa1",
   "metadata": {},
   "source": [
    "**Ejercicio 17 - Métrica de *padding waste* + logging por lote**\n",
    "\n",
    "**Objetivo:** instrumentar medición continua.  \n",
    "\n",
    "**Tareas:**\n",
    "1) En cada batch, calcula `pad_count` y `token_count`.  \n",
    "2) Acumula por época y registra p50/p90/p99 del *waste*.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa9363-48bb-48ca-b8a9-2cb46d9ef4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6570a",
   "metadata": {},
   "source": [
    "**Ejercicio 18 - Máscaras de atención**\n",
    "\n",
    "**Objetivo:** derivar `attention_mask` y máscara causal del *decoder*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1835e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: a partir de src_b / tgt_b, genera masks\n",
    "def make_attention_mask(batch_padded: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
    "    # 1 = token válido, 0 = PAD\n",
    "    return (batch_padded != pad_idx).to(torch.long)\n",
    "\n",
    "def make_causal_mask(T: int) -> torch.Tensor:\n",
    "    # Triangular superior en 0; permite ver solo pasado/presente\n",
    "    return torch.tril(torch.ones((T, T), dtype=torch.long))\n",
    "\n",
    "# Ejemplo con los lotes previos (si existen)\n",
    "try:\n",
    "    src_mask = make_attention_mask(src_b, PAD_IDX)\n",
    "    tgt_mask = make_attention_mask(tgt_b, PAD_IDX)\n",
    "    causal = make_causal_mask(tgt_b.shape[1])\n",
    "    print(\"src_mask:\", tuple(src_mask.shape), \"tgt_mask:\", tuple(tgt_mask.shape), \"causal:\", tuple(causal.shape))\n",
    "except NameError:\n",
    "    print(\"Ejecuta antes la celda del Ejercicio 15 para disponer de src_b/tgt_b.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e23c1-06b6-4f98-abe9-47e5dc208b4c",
   "metadata": {},
   "source": [
    "**Ejercicio 19 - *Loss* ignorando PAD + *teacher forcing* (bucle mínimo)**\n",
    "\n",
    "**Objetivo:** preparar un entrenamiento minimalista.  \n",
    "**Tareas:**\n",
    "1) Implementa una función de *loss* que ignore `PAD_IDX` (usar `ignore_index`).  \n",
    "2) Arma un bucle simulado de entrenamiento con *teacher forcing* (no requiere modelo complejo,  puede ser *dummy* que solo pruebe la forma).  \n",
    "\n",
    "\n",
    "**Ejercicio 20 - Sustituir TorchText por *tokenizador HF* (compatibilidad de checkpoint)**\n",
    "\n",
    "**Objetivo:** usar el *tokenizer* del modelo (p.ej., T5/mT5) para garantizar compatibilidad.  \n",
    "**Tareas:**\n",
    "1) Reemplaza `token_transform`/`vocab_transform` por `AutoTokenizer` del checkpoint.  \n",
    "2) Asegura `padding=True`, `truncation=True`, `return_tensors='pt'` y crea una `collate_fn` análoga.  \n",
    "\n",
    "\n",
    "**Ejercicio 21 - Portar a `datasets` (HF) + `map` + `DataCollatorWithPadding`**\n",
    "\n",
    "**Objetivo:** comparar pipelines.  \n",
    "**Tareas:**\n",
    "1) Carga un *split* compatible (o usa un dataset de texto de HF).  \n",
    "2) Aplica `map` con el tokenizador y usa `DataCollatorWithPadding`.  \n",
    "3) Compara tiempos/ergonomía vs TorchText.  \n",
    "\n",
    "\n",
    "**Ejercicio 22 - *Tests* mínimos (pytest) para el *pipeline* de datos**\n",
    "\n",
    "**Objetivo:** validar contratos.  \n",
    "**Tareas:** crea *tests* que verifiquen:  \n",
    "- (a) presencia de BOS/EOS,  \n",
    "- (b) `PAD_IDX` solo en cola del *padding*,  \n",
    "- (c) *shapes* esperados y `dtype=int64`,  \n",
    "- (d) reversa aplicada solo a `src`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f270b1-2f57-4d7c-bc7c-d1ded1fa3919",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872c110",
   "metadata": {},
   "source": [
    "#### **Entrega**\n",
    "- Evidencias mínimas por ejercicio (shapes, fragmentos de tensores/tablas).  \n",
    "- Comentarios breves explicando decisiones.  \n",
    "- Sin credenciales ni datos sensibles.  \n",
    "- `README.md` con qué funcionó y qué no (5-10 líneas).  \n",
    "- Roles documentados si trabajaste en equipo.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
