{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e6998a",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicios1 - Participación LLMs & NLP (torchtext + Hugging Face)**\n",
    "\n",
    "Cuaderno de **6 ejercicios cortos** (10-15 min c/u) que refuerzan:\n",
    "- *Introduccion_LLMs_y_Caso_de_Uso_Torchtext_HF* (torchtext + flujo clásico)\n",
    "- *Librerias_NLP* (panorama y Transformers de Hugging Face)\n",
    "\n",
    "> **Nota**: La primera ejecución puede descargar pesos de modelos. Mantén registro de versiones (`torch`, `transformers`, `torchtext`) para reproducibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b00009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de hardware y caché de Hugging Face (robusta)\n",
    "# Verifica GPU/CPU y configura caché local sin usar TRANSFORMERS_CACHE (deprecado).\n",
    "import os, sys, platform, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# (1) Caché local recomendado\n",
    "CACHE_BASE = Path(\"./hf_cache\").resolve()\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_BASE)                 # único root recomendado\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(CACHE_BASE / \"hub\")  # opcional\n",
    "for p in (CACHE_BASE, CACHE_BASE / \"hub\"):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Si alguien dejó TRANSFORMERS_CACHE en el entorno, lo quitamos para evitar FutureWarning\n",
    "if \"TRANSFORMERS_CACHE\" in os.environ:\n",
    "    os.environ.pop(\"TRANSFORMERS_CACHE\", None)\n",
    "    print(\"TRANSFORMERS_CACHE eliminado del entorno para evitar deprecación.\")\n",
    "\n",
    "print(\"HF_HOME:\", os.environ.get(\"HF_HOME\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.environ.get(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "# (2) Activar descargas rápidas solo si 'hf_transfer' está instalado\n",
    "try:\n",
    "    import hf_transfer  # noqa: F401\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "    print(\"HF_HUB_ENABLE_HF_TRANSFER: 1 (hf_transfer disponible)\")\n",
    "except Exception:\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "    print(\"HF_HUB_ENABLE_HF_TRANSFER: 0 (hf_transfer no instalado)\")\n",
    "\n",
    "# (3) Verificar dispositivo disponible\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        idx = torch.cuda.current_device()\n",
    "        device = f\"cuda:{idx}\"\n",
    "        print(\"CUDA disponible:\", True)\n",
    "        print(\"Cantidad de dispositivos CUDA:\", torch.cuda.device_count())\n",
    "        print(\"Nombre del dispositivo actual:\", torch.cuda.get_device_name(idx))\n",
    "        print(\"Capacidad de CUDA:\", torch.cuda.get_device_capability(idx))\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "        print(\"Apple MPS disponible:\", True)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"GPU no disponible; usando CPU.\")\n",
    "    print(\"Dispositivo seleccionado:\", device)\n",
    "except Exception as e:\n",
    "    print(\"Torch no disponible o la consulta de dispositivo falló:\", e)\n",
    "\n",
    "# (4) Silenciar advertencias deprecadas de torchtext 0.18\n",
    "try:\n",
    "    import torchtext\n",
    "    torchtext.disable_torchtext_deprecation_warning()\n",
    "    print(\"Advertencias deprecadas de torchtext silenciadas.\")\n",
    "except Exception as e:\n",
    "    print(\"No se pudo silenciar la advertencia de torchtext:\", e)\n",
    "\n",
    "# (5) Información del entorno\n",
    "print(\"Python:\", sys.version.split()[0], \"| Plataforma:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e78caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de hardware y caché de Hugging Face (robusta)\n",
    "# Verifica GPU/CPU y configura caché local sin usar TRANSFORMERS_CACHE (deprecado).\n",
    "import os, sys, platform, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# (1) Caché local recomendado\n",
    "CACHE_BASE = Path(\"./hf_cache\").resolve()\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_BASE)                 # único root recomendado\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(CACHE_BASE / \"hub\")  # opcional\n",
    "for p in (CACHE_BASE, CACHE_BASE / \"hub\"):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Si alguien dejó TRANSFORMERS_CACHE en el entorno, lo quitamos para evitar FutureWarning\n",
    "if \"TRANSFORMERS_CACHE\" in os.environ:\n",
    "    os.environ.pop(\"TRANSFORMERS_CACHE\", None)\n",
    "    print(\"TRANSFORMERS_CACHE eliminado del entorno para evitar deprecación.\")\n",
    "\n",
    "print(\"HF_HOME:\", os.environ.get(\"HF_HOME\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.environ.get(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "# (2) Activar descargas rápidas solo si 'hf_transfer' está instalado\n",
    "try:\n",
    "    import hf_transfer  # noqa: F401\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "    print(\"HF_HUB_ENABLE_HF_TRANSFER: 1 (hf_transfer disponible)\")\n",
    "except Exception:\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "    print(\"HF_HUB_ENABLE_HF_TRANSFER: 0 (hf_transfer no instalado)\")\n",
    "\n",
    "# (3) Verificar dispositivo disponible\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        idx = torch.cuda.current_device()\n",
    "        device = f\"cuda:{idx}\"\n",
    "        print(\"CUDA disponible:\", True)\n",
    "        print(\"Cantidad de dispositivos CUDA:\", torch.cuda.device_count())\n",
    "        print(\"Nombre del dispositivo actual:\", torch.cuda.get_device_name(idx))\n",
    "        print(\"Capacidad de CUDA:\", torch.cuda.get_device_capability(idx))\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "        print(\"Apple MPS disponible:\", True)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"GPU no disponible; usando CPU.\")\n",
    "    print(\"Dispositivo seleccionado:\", device)\n",
    "except Exception as e:\n",
    "    print(\"Torch no disponible o la consulta de dispositivo falló:\", e)\n",
    "\n",
    "# (4) Silenciar advertencias deprecadas de torchtext 0.18\n",
    "try:\n",
    "    import torchtext\n",
    "    torchtext.disable_torchtext_deprecation_warning()\n",
    "    print(\"Advertencias deprecadas de torchtext silenciadas.\")\n",
    "except Exception as e:\n",
    "    print(\"No se pudo silenciar la advertencia de torchtext:\", e)\n",
    "\n",
    "# (5) Información del entorno\n",
    "print(\"Python:\", sys.version.split()[0], \"| Plataforma:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc262847",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicio 1 - Mini-pipeline \"clásico\" con `EmbeddingBag` (torchtext)**\n",
    "\n",
    "**Objetivo:** tokenización -> vocabulario -> ids -> `EmbeddingBag` (sin entrenar).  \n",
    "**Entrega:** explica brevemente qué representan `ids`, `offsets` y por qué `out` es `(3, 16)`. Incluye un print del tamaño del vocabulario (`len(vocab.get_stoi())`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo: pipeline clásico mínimo con vocabulario -> ids -> EmbeddingBag\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Conjunto de textos de ejemplo\n",
    "texts = [\n",
    "    \"devops pipelines are reliable\",\n",
    "    \"security gates catch issues\",\n",
    "    \"infra as code is reproducible\",\n",
    "]\n",
    "\n",
    "# Tokenizador básico en inglés\n",
    "tok = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Función generadora de tokens\n",
    "def yield_tokens(data):\n",
    "    for t in data:\n",
    "        yield tok(t)\n",
    "\n",
    "# Construcción de vocabulario con un token especial <unk>\n",
    "vocab = build_vocab_from_iterator(yield_tokens(texts), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Conversión de textos a IDs (tensores de enteros)\n",
    "ids = [torch.tensor(vocab(tok(t)), dtype=torch.long) for t in texts]\n",
    "\n",
    "# Offsets para EmbeddingBag (marca dónde empieza cada frase)\n",
    "offsets = torch.tensor([0, len(ids[0]), len(ids[0]) + len(ids[1])])\n",
    "\n",
    "# Concatenación de todos los IDs en un solo tensor\n",
    "flat = torch.cat(ids)\n",
    "\n",
    "# Capa de embeddings con modo \"mean\" (media de embeddings por oración)\n",
    "emb = nn.EmbeddingBag(num_embeddings=len(vocab), embedding_dim=16, mode=\"mean\")\n",
    "out = emb(flat, offsets)\n",
    "\n",
    "# Salidas de depuración\n",
    "print(\"Tamaño stoi (vocabulario):\", len(vocab.get_stoi()))\n",
    "print(\"Longitudes de ids:\", [len(x) for x in ids])\n",
    "print(\"Offsets:\", offsets.tolist())\n",
    "print(\"Forma de salida:\", tuple(out.shape))  # esperado (3, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para completar\n",
    "# 1) En 3-4 líneas, explica qué representan 'ids' y 'offsets'.\n",
    "#    - 'ids' contiene los tensores con los índices de vocabulario para cada oración.\n",
    "#    - 'offsets' indica las posiciones iniciales de cada oración dentro del tensor concatenado.\n",
    "# 2) ¿Por qué la salida (Out) tiene forma (3, 16)?\n",
    "#    - Porque hay 3 oraciones de entrada y cada una se proyecta a un embedding de dimensión 16.\n",
    "# 3) Imprime las primeras 5 entradas de vocab.get_stoi() como evidencia.\n",
    "#    PISTA: list(vocab.get_stoi().items())[:5]\n",
    "# TODO: agrega aquí tu explicación y prints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e22c97",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicio 2 - Comparación de tokenización HF (BERT-m vs XLM-R)**\n",
    "\n",
    "**Objetivo:** observar diferencias de *subwords* en español.  \n",
    "**Entrega:** tabla breve (palabra -> fragmentación por modelo) y 3-4 líneas sobre impacto en longitud de secuencia/memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo: compara segmentacion de subpalabras en español\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Texto de ejemplo en español sobre Nginx\n",
    "text = \"Configurar Nginx como reverse proxy mejora seguridad y rendimiento del servicio.\"\n",
    "\n",
    "# Cargamos dos tokenizadores distintos\n",
    "t1 = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "t2 = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenización con cada modelo\n",
    "tokens1 = t1.tokenize(text)  # WordPiece (## para subpalabras)\n",
    "tokens2 = t2.tokenize(text)  # SentencePiece (▁ para inicio de palabra)\n",
    "\n",
    "# Mostramos resultados y longitudes\n",
    "print(\"Tokens BERT-m:\", tokens1, \"len:\", len(tokens1))\n",
    "print(\"Tokens XLM-R:\", tokens2, \"len:\", len(tokens2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para completar\n",
    "# Construye una pequeña tabla comparativa para 5-7 palabras interesantes (manual o programática).\n",
    "# Luego escribe 3-4 líneas (en español) sobre cómo las diferencias de subpalabras impactan\n",
    "# en la longitud de la secuencia y en el uso de memoria.\n",
    "# CONSEJO: elige palabras como \"Configurar\", \"Nginx\", \"rendimiento\", \"seguridad\", \"servicio\".\n",
    "# TODO: tu análisis / tabla aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb637057",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicio 3: Análisis de sentimiento multilingüe con `pipeline`**\n",
    "\n",
    "**Objetivo:** ejecutar un *pipeline* listo para usar y contar etiquetas.  \n",
    "**Entrega:** `Counter` de etiquetas y 2-3 líneas sobre coincidencias/discrepancias con tu intuición.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ce5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo: inferencia rápida y conteo de etiquetas\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Definimos un pipeline de análisis de sentimiento multilingüe\n",
    "clf = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Textos de prueba en diferentes idiomas\n",
    "texts = [\n",
    "    \"Excelente experiencia con el despliegue.\",\n",
    "    \"El build rompió producción, muy mal.\",\n",
    "    \"Service is okay, not great.\",\n",
    "    \"La latencia bajó notablemente, bien.\",\n",
    "    \"Terrible soporte al cliente.\",\n",
    "]\n",
    "\n",
    "# Ejecutamos la inferencia sobre los textos\n",
    "res = clf(texts)\n",
    "\n",
    "# Contamos cuántas veces aparece cada etiqueta\n",
    "cnt = Counter([r[\"label\"] for r in res])\n",
    "\n",
    "# Mostramos resultados brutos y conteo de etiquetas\n",
    "print(\"Resultados brutos:\", res)\n",
    "print(\"Conteo de etiquetas:\", cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para completar\n",
    "# Escribe 2-3 líneas comentando si los resultados coinciden con tu intuición.\n",
    "# Si hay discrepancias, da un ejemplo y una posible explicación.\n",
    "# TODO: tu comentario aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f089f20",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicio 4- Zero-shot para moderación ligera (MNLI)**\n",
    "\n",
    "**Objetivo:** etiquetar textos sin *finetuning* con `zero-shot-classification`.  \n",
    "**Entrega:** tabla (texto -> predicción -> score) + tu \"ground truth\" y un % de acierto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo: boceto rápido de moderación con zero-shot\n",
    "from transformers import pipeline\n",
    "\n",
    "# Definimos un pipeline de clasificación zero-shot con BART MNLI\n",
    "zsc = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Posibles etiquetas de moderación\n",
    "labels = [\"toxicity\", \"sexual\", \"violence\", \"insult\", \"safe\"]\n",
    "\n",
    "# Textos de prueba en español e inglés\n",
    "texts = [\n",
    "    \"Buen trabajo en el despliegue.\",\n",
    "    \"Eres un inútil total.\",\n",
    "    \"El informe incluye fotos violentas.\",\n",
    "    \"Contenido educativo para niños.\",\n",
    "    \"Comentarios groseros y ofensivos.\",\n",
    "    \"This is purely technical documentation.\",\n",
    "    \"Amenazas explícitas contra usuarios.\",\n",
    "    \"Chistes subidos de tono.\",\n",
    "]\n",
    "\n",
    "# Inferencia: seleccionamos la etiqueta más probable para cada texto\n",
    "pred = []\n",
    "for t in texts:\n",
    "    r = zsc(t, labels)\n",
    "    pred.append((t, r[\"labels\"][0], round(r[\"scores\"][0], 3)))\n",
    "\n",
    "# Mostramos los resultados (texto, etiqueta asignada, puntuación)\n",
    "for p in pred:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda  para completar\n",
    "# 1) Define tu lista manual de ground truth para los textos anteriores (elige la mejor clase por fila).\n",
    "# 2) Calcula la exactitud simple = correctos / 8.\n",
    "# 3) Muestra una tabla de 4 columnas: texto | predicción | puntaje | ground_truth | ¿correcto?\n",
    "# PISTA: construye una lista de diccionarios e imprime de forma legible (o usa pandas si está disponible).\n",
    "# TODO: tu evaluación aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a90070",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicio 5:  Resumen y control de longitud (DistilBART)**\n",
    "\n",
    "**Objetivo:** resumir un párrafo técnico y medir compresión.  \n",
    "**Entrega:** tabla con longitudes y *compression ratio (relación de comprensión)* + 3 viñetas sobre qué se gana/pierde al variar `max_length`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077151af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen con DistilBART usando safetensors (evita torch.load de .bin)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_id = \"sshleifer/distilbart-cnn-12-6\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "modelo = AutoModelForSeq2SeqLM.from_pretrained(model_id, use_safetensors=True)\n",
    "\n",
    "summ = pipeline(\"summarization\", model=modelo, tokenizer=tok)\n",
    "\n",
    "text = (\n",
    "  \"Implementar despliegues canarios reduce el riesgo al liberar cambios a un pequeño \"\n",
    "  \"porcentaje de usuarios. Combinado con métricas de latencia, error rate y saturación, \"\n",
    "  \"permite decisiones informadas sobre promoción o rollback. La observabilidad con trazas \"\n",
    "  \"distribuidas ayuda a localizar cuellos de botella. Feature flags facilitan activar o \"\n",
    "  \"desactivar funciones sin redeploy. La automatización en CI/CD asegura reproducibilidad \"\n",
    "  \"y tiempos de entrega más cortos, manteniendo la seguridad mediante escáneres en el pipeline.\"\n",
    ")\n",
    "\n",
    "s1 = summ(text, max_length=80, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "s2 = summ(text, max_length=120, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "def ratio(s, t): \n",
    "    return round(len(s)/len(t), 3)\n",
    "\n",
    "print(\"Longitud original:\", len(text))\n",
    "print(\"Longitud s1:\", len(s1), \"relación:\", ratio(s1, text))\n",
    "print(\"Longitud s2:\", len(s2), \"relación:\", ratio(s2, text))\n",
    "print(\"S1:\", s1, \"\\nS2:\", s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10378388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para completar\n",
    "# 1) Crea una pequeña tabla: [longitud_original, long_s1, long_s2, ratio_s1, ratio_s2].\n",
    "# 2) Escribe 3 viñetas (en español) describiendo los compromisos al aumentar max_length.\n",
    "# TODO: tu tabla y viñetas aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa5a50-4533-4778-bc9c-d42469dec53f",
   "metadata": {},
   "source": [
    "#### **Ejercicio 6 -Comparación de tokenizadores en español**\n",
    "\n",
    "El objetivo es observar cómo distintos modelos de HuggingFace tokenizan un mismo texto en español y analizar las diferencias en los sub-tokens generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88840bb-d604-49b4-be8b-b49076886df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Modelos a comparar\n",
    "models = [\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"xlm-roberta-base\"\n",
    "]\n",
    "\n",
    "text = \"La observabilidad en DevOps ayuda a detectar cuellos de botella en sistemas distribuidos.\"\n",
    "\n",
    "# TODO: itera sobre la lista de modelos, carga el tokenizer y muestra los tokens\n",
    "# Usa print para visualizar las diferencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e122f-0016-48fd-b62d-2ab9e5a5360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para completar\n",
    "# 1) Muestra el número de tokens generados por cada modelo.\n",
    "# 2) Compara cómo se segmenta la palabra \"observabilidad\".\n",
    "# 3) Escribe 3 viñetas (en español) sobre las ventajas y limitaciones de cada tokenizador.\n",
    "# TODO: tus resultados aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de45fc",
   "metadata": {},
   "source": [
    "\n",
    "#### **Recomendaciones de entrega**\n",
    "- Estructura de carpetas: `Participacion_LLMs_NLP` con `README.md` (hallazgos) y `out/` (salidas).\n",
    "- Documenta versiones y hardware; conserva logs de consola relevantes.\n",
    "- Si hay descargas lentas, precarga el modelo antes de la actividad grupal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
