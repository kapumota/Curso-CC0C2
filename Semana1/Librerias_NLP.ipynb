{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Explorando una librería de la IA generativa__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesamiento del lenguaje natural (NLP)\n",
    "\n",
    "El NLP es una rama de la inteligencia artificial enfocada en la interacción entre computadoras y lenguaje humano. Su objetivo es permitir que las máquinas comprendan, interpreten y generen textos de forma similar a los humanos. Entre sus aplicaciones se encuentran:\n",
    "\n",
    "- **Generación de contenido:** Creación de textos coherentes para artículos, historias o publicidad.\n",
    "- **Chatbots y asistentes virtuales:** Sistemas que interactúan en lenguaje natural, ofreciendo respuestas y soluciones a los usuarios.\n",
    "- **Análisis de sentimientos, traducción y clasificación:** Tareas que ayudan a extraer información y dar sentido a grandes volúmenes de texto.\n",
    "\n",
    "#### ¿Qué es la IA generativa?\n",
    "\n",
    "La IA generativa se caracteriza por su capacidad para crear contenido nuevo basándose en ejemplos previos. Un ejemplo ilustrativo es el proceso de alimentar a una computadora con numerosas pinturas y, a partir de ellas, permitir que genere una obra original. Entre sus aplicaciones destacan:\n",
    "\n",
    "La IA generativa está transformando múltiples industrias. Sus aplicaciones abarcan:\n",
    "\n",
    "#### 1. Arte y creatividad\n",
    "\n",
    "- **Arte generativo:** Artistas que emplean algoritmos de IA generativa pueden crear obras impresionantes al aprender de obras maestras existentes y producir piezas únicas inspiradas en ellas. Estas obras generadas por IA han ganado reconocimiento en el mundo del arte.\n",
    "- **Composición musical:** Proyectos en el ámbito de la IA generativa se han utilizado para componer música. Aprenden de un vasto conjunto de composiciones y pueden generar piezas originales en diversos estilos, desde clásico hasta jazz, revolucionando la industria musical.\n",
    "\n",
    "#### 2. Procesamiento del lenguaje natural (NLP)\n",
    "- **Generación de contenido:** Herramientas como el transformer generativo preentrenado (GPT) han demostrado su capacidad para generar textos coherentes y contextualizados. Pueden ayudar a creadores de contenido generando artículos, historias o textos publicitarios, siendo herramientas muy útiles.\n",
    "- **Chatbots y asistentes virtuales:** La IA generativa impulsa muchos de los chatbots y asistentes virtuales actuales. Estos agentes conversacionales basados en IA comprenden y generan respuestas similares a las humanas, mejorando la experiencia del usuario.\n",
    "- **Escritura de código:** Los modelos de IA generativa también pueden producir fragmentos de código basados en descripciones o requerimientos, agilizando el desarrollo de software.\n",
    "\n",
    "#### 3. Visión por computadora\n",
    "- **Síntesis de imágenes:** Modelos como el de análisis de datos con modelo de lenguaje para generación y exploración, conocido frecuentemente como DALL-E, pueden generar imágenes a partir de descripciones textuales. Esta tecnología tiene aplicaciones en diseño gráfico, publicidad y creación de contenido visual para marketing.\n",
    "- **Detección de deepfakes:** Con el avance de las técnicas de IA generativa, también ha aumentado la generación de contenido deepfake. En consecuencia, la IA generativa ahora participa en el desarrollo de herramientas y técnicas para detectar y combatir la difusión de información errónea mediante videos manipulados.\n",
    "\n",
    "#### 4. Avatares virtuales\n",
    "- **Entretenimiento:** La IA generativa se utiliza para crear avatares virtuales para juegos y entretenimiento. Estos avatares imitan expresiones y emociones humanas, aumentando el compromiso en entornos virtuales.\n",
    "- **Marketing:** Los influencers virtuales, impulsados por la IA generativa, están en auge en el marketing digital. Las marcas utilizan estas personalidades virtuales para promocionar sus productos y servicios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructuras neuronales detrás de la IA generativa\n",
    "\n",
    "Antes de contar con los poderosos transformers, que actúan como lectores súper rápidos y comprenden muchas palabras al mismo tiempo, existían otros métodos para que las computadoras generaran texto. Estos métodos fueron los bloques constructores que permitieron las increíbles capacidades actuales.\n",
    "\n",
    "#### Generación de texto antes de los transformers\n",
    "\n",
    "##### **1. Modelos de lenguaje n-gramas**\n",
    "Los modelos n-grama son como detectives del lenguaje. Predicen qué palabra viene a continuación en una oración basándose en las palabras previas. Por ejemplo, si dices \"El cielo es\", estos modelos adivinan que la siguiente palabra podría ser \"azul\".\n",
    "\n",
    "##### **2. Redes neuronales recurrentes (RNN)**\n",
    "\n",
    "Las redes neuronales recurrentes (RNN) están especialmente diseñadas para manejar datos secuenciales, lo que las convierte en una herramienta poderosa para tareas como el modelado del lenguaje y el pronóstico de series temporales. La esencia de su diseño radica en mantener una \"memoria\" o \"estado oculto\" a lo largo de la secuencia mediante bucles. Esto permite a las RNN reconocer y capturar las dependencias temporales inherentes a los datos secuenciales.\n",
    "\n",
    "- **Estado oculto:** Conocido como la \"memoria\" de la red, es un almacenamiento dinámico de información sobre entradas previas. Con cada nueva entrada, este estado se actualiza considerando tanto la entrada nueva como su valor anterior.\n",
    "- **Dependencia temporal:** Los bucles en las RNN facilitan la transferencia de información a través de los distintos pasos de la secuencia.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0J87EN/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\" width=\"50%\" height=\"70%\"> \n",
    "\n",
    "<div style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\">Fuente de la imagen</a></div>\n",
    "\n",
    "**Ilustración del funcionamiento de una RNN**: \n",
    "\n",
    "Considera una secuencia simple, por ejemplo la oración: \"I love RNNs\". La RNN interpreta esta oración palabra por palabra. Comenzando con la palabra \"I\", la RNN la procesa, genera una salida y actualiza su estado oculto. Al pasar a la palabra \"love\", la RNN la procesa junto al estado oculto actualizado, que ya contiene información sobre \"I\". \n",
    "\n",
    "Este patrón de procesamiento y actualización continúa hasta la última palabra. Al final, el estado oculto idealmente resume la información de toda la oración.\n",
    "\n",
    "##### **3. Memoria a largo plazo (LSTM) y unidades recurrentes con puertas (GRUs)**\n",
    "La memoria a largo plazo (LSTM) y las unidades recurrentes con puertas (GRUs) son variantes avanzadas de las RNN, diseñadas para superar las limitaciones de las RNN tradicionales y mejorar su capacidad para modelar datos secuenciales de manera efectiva. Procesan las secuencias elemento por elemento y mantienen un estado interno para recordar información pasada. \n",
    "Aunque fueron efectivas para muchas tareas, tuvieron dificultades con secuencias muy largas y dependencias a largo plazo.\n",
    "\n",
    "##### **4. Modelos Seq2seq con atención**\n",
    "\n",
    "- Los modelos de secuencia a secuencia (seq2seq), a menudo construidos con RNN o LSTM, fueron diseñados para tareas como la traducción, donde una secuencia de entrada se transforma en una secuencia de salida.\n",
    "- Se introdujo el mecanismo de atención para permitir que el modelo se \"concentre\" en las partes relevantes de la secuencia de entrada al generar la salida, mejorando significativamente el rendimiento en tareas como la traducción automática.\n",
    "\n",
    "> Aunque estos métodos representaron avances importantes en la generación de texto, la introducción de los transformers supuso un cambio de paradigma. Los transformers, con su mecanismo de autoatención, demostraron ser sumamente eficientes para capturar información contextual en secuencias largas, estableciendo nuevos estándares en diversas tareas de NLP.\n",
    "\n",
    "#### Transformers\n",
    "Propuesto en el artículo [Attention Is All You Need](), la arquitectura transformer reemplazó el procesamiento secuencial por procesamiento en paralelo. ¿El componente clave de su éxito? El mecanismo de atención, o más precisamente, la **autoatención**.\n",
    "\n",
    "Pasos clave incluyen:\n",
    "- **Tokenización:** El primer paso es descomponer una oración en tokens (palabras o subpalabras).\n",
    "- **Embedding:** Cada token se representa como un vector que captura su significado.\n",
    "- **Autoatención:** El modelo calcula puntuaciones que determinan la importancia de cada palabra en relación a las demás dentro de la secuencia. Estas puntuaciones se usan para ponderar los tokens y generar una nueva representación de la secuencia.Por ejemplo, en la oración \"Le dio un regalo porque ella le había ayudado\", comprender a quién se refiere \"ella\" requiere que el modelo preste atención a otras palabras. El transformer hace esto para cada palabra, considerando todo el contexto, lo que resulta muy eficaz para entender el significado.\n",
    "- **Redes neuronales feed-forward:** Después de la atención, cada posición se procesa de manera independiente a través de una red neuronal feed-forward.\n",
    "- **Secuencia de salida:** El o produce una secuencia de salida, que puede utilizarse para tareas como clasificación, traducción o generación de texto.\n",
    "- **Capas:** Los transformers son os profundos con múltiples capas de atención y redes feed-forward, lo que les permite aprender patrones complejos.\n",
    "\n",
    "La flexibilidad de esta arquitectura ha permitido utilizar los transformers más allá del NLP, aplicándolos también al procesamiento de imágenes y video. En NLP, os basados en transformers como BERT, GPT y sus variantes han establecido resultados de vanguardia en tareas que van desde la clasificación de textos hasta la traducción.\n",
    "\n",
    "##### **Modelos de lenguaje a gran escala (LLMs)**\n",
    "Los modelos de lenguaje a gran escala son como cerebros supercargados. Son programas de computadora masivos con muchas \"neuronas\" que aprenden de enormes cantidades de texto. Estos modelos se entrenan para tareas como comprender y generar texto, y se utilizan en muchas aplicaciones. Sin embargo, tienen una limitación: no son muy buenos para entender el contexto global o el significado profundo de las palabras. Funcionan bien para predicciones simples, pero tienen dificultades con textos más complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerías de NLP\n",
    "\n",
    "Entre las librerías más destacadas en el ámbito del NLP se encuentra la biblioteca **transformers** de Hugging Face. Esta herramienta de código abierto facilita:\n",
    "\n",
    "- **El acceso a modelos preentrenados:**  Como \"facebook/blenderbot-400M-distill\" y \"google/flan-t5-base\", que pueden ser utilizados para tareas de generación de texto, traducción, resumen y más.\n",
    "\n",
    "- **Procesamiento de texto:**  Con funcionalidades para tokenización y generación de embeddings, esenciales para transformar el texto en datos numéricos que los modelos pueden procesar.\n",
    "\n",
    "- **Integración con otros frameworks:**  Permite el uso conjunto con bibliotecas como TensorFlow y PyTorch, ampliando las posibilidades de implementación en proyectos de NLP.\n",
    "\n",
    "Otras librerías complementarias incluyen herramientas para la tokenización específica (por ejemplo, `sentencepiece`) y para la construcción de modelos personalizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación: Construyendo un chatbot simple con transformers\n",
    "Ahora construirás un chatbot simple utilizando la libreria `transformers` de Hugging Face, una herramienta de NLP de código abierto con muchas funcionalidades útiles.\n",
    "\n",
    "#### Paso 1: Instalación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar las librería necesarias\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "#!pip install torch\n",
    "#!pip install torchtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Importando las herramientas requeridas de la librería transformers\n",
    "En el siguiente script se inicializan variables utilizando dos clases fundamentales de la librería transformers:\n",
    "\n",
    "- `o` es una instancia de la clase `AutoForSeq2SeqLM`, la cual te permite interactuar con el o de lenguaje elegido.\n",
    "- `tokenizer` es una instancia de la clase `AutoTokenizer`, que facilita el procesamiento de tu entrada al convertir el texto en \"tokens\", la forma que tiene el o de interpretar el lenguaje.\n",
    "\n",
    "Se ha elegido el o \"facebook/blenderbot-400M-distill\" para este ejemplo, ya que está disponible de forma gratuita bajo una licencia de código abierto y funciona a un ritmo relativamente rápido. Para explorar una variedad de os y sus capacidades, visita la página de [os de Hugging Face](https://huggingface.co/s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Seleccionando el o. Se utilizará \"facebook/blenderbot-400M-distill\" en este ejemplo.\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Cargar el modelo y el tokenizer\n",
    "modelo = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la inicialización, configuremos la función de chat para permitir la interacción en tiempo real con el chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de chat\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Obtener la entrada del usuario\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Condiciones de salida\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenizar la entrada y generar la respuesta\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = modelo.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Mostrar la respuesta del chatbot\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Iniciar el chat\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has interactuado con tu chatbot. Al proporcionarle una solicitud, el chatbot utilizó el poder de la librería transformers y el modelo subyacente para generar una respuesta. Esto ejemplifica la destreza de los modelos basados en transformers para comprender y generar textos similares a los humanos a partir de un contexto dado. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3: Probando otro modelo de lenguaje y comparando la salida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes utilizar un modelo de lenguaje diferente, por ejemplo el modelo \"[flan-t5-base](https://huggingface.co/google/flan-t5-base)\" de Google, para crear un chatbot similar. Utiliza una función de chat parecida a la definida en el Paso 2 y compara las salidas de ambos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Seleccionar el modelo de Google flan-t5-base\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "modelo = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vamos a chatear con otro bot\n",
    "def chat_with_another_bot():\n",
    "    while True:\n",
    "        # Obtener la entrada del usuario\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Condiciones de salida\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenizar la entrada y generar la respuesta\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = modelo.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Mostrar la respuesta del chatbot\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Iniciar el chat\n",
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen muchos modelos de lenguaje disponibles en Hugging Face. En el siguiente ejercicio, compararás la salida para la misma entrada utilizando dos modelos diferentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crea un chatbot utilizando diferentes modelos de Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un chatbot simple utilizando la librería transformers de Hugging Face (https://huggingface.co/models). Ejecuta el código usando los siguientes modelos y compara la salida. Los modelos son \"[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\" y \"[facebook/bart-base](https://huggingface.co/facebook/bart-base)\".\n",
    "(Nota: Dependiendo del modelo seleccionado, es posible que notes diferencias en la salida del chatbot. Múltiples factores, como el entrenamiento y ajuste fino del modelo, influyen en el resultado.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega el código para el ejercicio aquí"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "prev_pub_hash": "5c5bec06088ad96b1ecbe1871624b6f4fcc99062c9772bf4e6ad46b1d556c1b8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
