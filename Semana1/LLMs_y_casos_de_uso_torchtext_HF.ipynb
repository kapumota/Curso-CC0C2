{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6682c9ce",
   "metadata": {},
   "source": [
    "### **Introducción a LLMs y casos de uso (torchtext + Hugging Face)**\n",
    "\n",
    "**Objetivos de aprendizaje**\n",
    "\n",
    "- Comprender qué son los *modelos fundacionales* (Foundation Models) y los *Large Language Models (LLMs)*.\n",
    "- Cargar y preprocesar texto con **torchtext** para un clasificador sencillo.\n",
    "- Consumir modelos preentrenados de **Hugging Face Transformers** para tareas prácticas: *zero-shot classification*, *traducción*, *resumen*, *NER* y *text-to-text*.\n",
    "- Comparar un flujo clásico (dataset+vocab+modelo pequeño) con uno de *foundation models* (pipelines de Transformers).\n",
    "- Plantear ejercicios interactivos para clase.\n",
    "\n",
    "> **Nota importante sobre torchtext**: Desde la versión 0.18 (abril 2024) el desarrollo de `torchtext` quedó congelado. Sigue funcionando, pero se recomienda preferir `datasets`/`tokenizers` de Hugging Face para nuevos proyectos. Este cuaderno incluye ambos enfoques para fines didácticos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a719f",
   "metadata": {},
   "source": [
    "#### **1. Instalación de dependencias**\n",
    "\n",
    "Ejecuta esta celda si estás en un entorno nuevo (por ejemplo, Google Colab). Si ya tienes las librerías, puedes omitirla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d855168",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#!pip -q install --upgrade torchtext==0.18.0 transformers datasets evaluate accelerate sentencepiece sacremoses\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "print('PyTorch:', torch.__version__, '| CUDA disponible:', torch.cuda.is_available(), '| Python:', platform.python_version())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656562d4",
   "metadata": {},
   "source": [
    "#### **2. ¿Qué son los modelos fundacionales (FMs) y los LLMs?**\n",
    "\n",
    "- **Modelos fundacionales (FMs)**: Modelos entrenados a gran escala con datos amplios (usualmente auto-supervisión) que pueden **adaptarse** a multitud de tareas. Ejemplos: BERT, T5, GPT, CLIP.\n",
    "- **LLMs**: Subconjunto de FMs centrado en lenguaje natural; suelen usar arquitecturas *Transformer* y tener billones/miles de millones de parámetros. Permiten *in-context learning*, *few-shot*, *zero-shot*.\n",
    "\n",
    "Ventajas: transferibilidad, cobertura de tareas, rendimiento *SOTA* en muchas benchmarks. Riesgos: sesgos, alucinaciones, coste computacional/energético, actualización de conocimiento.\n",
    "\n",
    "A continuación, veremos dos flujos de trabajo:\n",
    "\n",
    "1) **Clásico con torchtext** (dataset->vocab->modelo pequeño): útil para entender el pipeline y entrenar algo rápido.\n",
    "2) **Con Transformers** (modelos preentrenados + `pipeline`): útil para prototipado veloz y casos de uso reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079fe74",
   "metadata": {},
   "source": [
    "#### **3. Mini-clasificador con `torchtext` (AG_NEWS)**\n",
    "\n",
    "Entrenaremos un clasificador rápido de noticias con `EmbeddingBag`. Esto ilustra el flujo de preparación de datos, vocabulario y *mini-modelo*.\n",
    "\n",
    "**Advertencias**:\n",
    "- Para agilizar la práctica, usaremos un subconjunto pequeño del *train*.\n",
    "- Si tu versión de `torchtext` difiere, revisa la [documentación] y adapta los imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d743628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Silenciar el warning de deprecación de torchtext 0.18\n",
    "import warnings\n",
    "import torchtext\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=r\"torchtext(\\.|$)\")\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Carga del dataset\n",
    "train_iter = AG_NEWS(split='train')\n",
    "test_iter = AG_NEWS(split='test')\n",
    "\n",
    "# Construye vocabulario a partir del train\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Re-carga el iterador porque se consumió al construir el vocab\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  # etiquetas 1..4 -> 0..3\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (label, text) in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list, text_list, offsets\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_class = 4\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "modelo = TextClassifier(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "def train_func(dataloader):\n",
    "    modelo.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 200\n",
    "    optimizer = optim.SGD(modelo.parameters(), lr=4.0)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(1):\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            label, text, offsets = label.to(device), text.to(device), offsets.to(device)\n",
    "            pred = modelo(text, offsets)\n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_acc += (pred.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                print(f'  paso {idx:>4d} | acc acumulada: {total_acc/total_count:.3f}')\n",
    "        scheduler.step()\n",
    "\n",
    "def test_func(dataloader):\n",
    "    modelo.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for label, text, offsets in dataloader:\n",
    "            label, text, offsets = label.to(device), text.to(device), offsets.to(device)\n",
    "            pred = modelo(text, offsets)  # <-- corregido (antes: model)\n",
    "            total_acc += (pred.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    print(f'Acc. test: {total_acc/total_count:.3f}')\n",
    "\n",
    "# Subconjuntos pequeños para una demo rápida\n",
    "from itertools import islice\n",
    "train_subset = list(islice(AG_NEWS(split='train'), 0, 2000))\n",
    "test_subset  = list(islice(AG_NEWS(split='test'),  0, 1000))\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader  = DataLoader(test_subset,  batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print('Entrenando...')\n",
    "train_func(train_dataloader)\n",
    "print('Evaluando...')\n",
    "test_func(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b3c88",
   "metadata": {},
   "source": [
    "#### **4. Casos de uso con *Foundation Models* vía Hugging Face `pipeline`**\n",
    "\n",
    "La API `pipeline` ofrece acceso inmediato a tareas comunes sin escribir *boilerplate*.\n",
    "\n",
    "**4.1 Zero-shot classification**\n",
    "\n",
    "Permite clasificar texto en etiquetas que **no** se vieron en entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zero_shot = pipeline('zero-shot-classification', model='facebook/bart-large-mnli', device_map='auto' if torch.cuda.is_available() else None)\n",
    "texto = 'Este proyecto integra CI/CD con despliegues canary y observabilidad con métricas y logs estructurados.'\n",
    "etiquetas = ['DevOps', 'Ciberseguridad', 'Análisis Financiero', 'Educación']\n",
    "print(zero_shot(texto, candidate_labels=etiquetas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802a7b3",
   "metadata": {},
   "source": [
    "**4.2 Traducción**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6644798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "t2t = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",   # tiene safetensors\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "def es_en(txt):\n",
    "    return t2t(f\"translate Spanish to English: {txt}\", max_new_tokens=128)[0][\"generated_text\"]\n",
    "\n",
    "def en_es(txt):\n",
    "    return t2t(f\"translate English to Spanish: {txt}\", max_new_tokens=128)[0][\"generated_text\"]\n",
    "\n",
    "print(es_en(\"La ingeniería de IA combina ciencia de datos, MLOps y diseño de productos.\"))\n",
    "print(en_es(\"Large Language Models are transforming software development workflows.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834652d",
   "metadata": {},
   "source": [
    "**4.3 Resumen automático (*summarization*)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae249151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "t2t = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",           # suele venir en safetensors\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "def resumir_es(texto, max_new_tokens=80):\n",
    "    prompt = f\"Resumir en español en 1-2 frases: {texto}\"\n",
    "    return t2t(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "texto_largo = (\n",
    "    \"Los modelos de lenguaje grandes (LLMs) han habilitado nuevas capacidades en tareas de texto, \"\n",
    "    \"como resumen, respuesta a preguntas y generación de código. En entornos educativos, permiten \"\n",
    "    \"prototipos rápidos y evaluación automatizada, aunque plantean retos de honestidad académica, \"\n",
    "    \"sesgos y trazabilidad. La integración con MLOps y DevSecOps es clave para llevarlos a \"\n",
    "    \"producción de forma segura y escalable.\"\n",
    ")\n",
    "print(resumir_es(texto_largo, max_new_tokens=60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ec700",
   "metadata": {},
   "source": [
    "**4.4 NER (Reconocimiento de entidades)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d712d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (antes de importar transformers/pipeline)\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"  # quita barras de descarga del Hub\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"                  # silencia tqdm en transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging as hf_logging\n",
    "import torch\n",
    "\n",
    "# baja el ruido de logs de transformers\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# usa la tarea adecuada y la API nueva (sin grouped_entities)\n",
    "ner = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",  # reemplaza grouped_entities=True\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "texto = \"AWS Lambda se integra con Amazon API Gateway y DynamoDB para arquitecturas serverless.\"\n",
    "print(ner(texto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5ccc2",
   "metadata": {},
   "source": [
    "**4.5 Text-to-text con FLAN-T5 (instrucciones)**\n",
    "\n",
    "Ideal para plantillas tipo *prompt*: QA, explicación, reescritura, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c06c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = pipeline('text2text-generation', model='google/flan-t5-small', device_map='auto' if torch.cuda.is_available() else None)\n",
    "print(instruct('Explica brevemente qué es DevSecOps y por qué es importante.')[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cf739",
   "metadata": {},
   "source": [
    "### **5. Un vistazo a *tokenization* y *AutoModel***\n",
    "Para un mayor control, usa `AutoTokenizer`/`AutoModel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec9aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_id = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "inputs = tok('I love well-instrumented CI pipelines!', return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = mdl(**inputs)\n",
    "pred = outputs.logits.argmax(dim=-1).item()\n",
    "print('Predicción (0=NEG, 1=POS):', pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a1ce5-14b0-48c5-9767-ec88e77f2b43",
   "metadata": {},
   "source": [
    "#### **Ejercicios** \n",
    "\n",
    "**1) Clasificador torchtext (mini-experimento)**\n",
    "\n",
    "**Objetivo:** subir el tamaño a \\~10 k ejemplos y probar `EmbeddingBag` con `emsize=128`.\n",
    "\n",
    "**Qué hacer (sin detalles técnicos):**\n",
    "\n",
    "1. Entrena el clasificador con **10 k** ejemplos de entrenamiento.\n",
    "2. Prueba **3** tamaños de *batch*: **32**, **64**, **128** (con `emsize=128` fijo).\n",
    "3. Registra *accuracy* en test y tiempo de entrenamiento.\n",
    "\n",
    "**Entrega (pega esta tablita con tus números):**\n",
    "\n",
    "| batch\\_size | accuracy\\_test | tiempo\\_train (s) | notas |\n",
    "| ----------: | -------------: | ----------------: | ----- |\n",
    "|          32 |                |                   |       |\n",
    "|          64 |                |                   |       |\n",
    "|         128 |                |                   |       |\n",
    "\n",
    "\n",
    "**Pregunta guía:** ¿Cuál *batch\\_size* te dio mejor equilibrio entre estabilidad y tiempo?\n",
    "\n",
    "**2) Tokenización HF (subwords en español)**\n",
    "\n",
    "**Objetivo:** comparar cómo trocean palabras dos tokenizadores multilingües.\n",
    "**Modelos:** `bert-base-multilingual-cased` vs `xlm-roberta-base`.\n",
    "**Qué hacer:**\n",
    "\n",
    "1. Toma **un mismo texto corto en español** (2-3 oraciones).\n",
    "2. Tokeniza con ambos modelos.\n",
    "3. Cuenta cuántos **subwords** genera cada uno e identifica **5 palabras** donde difieran.\n",
    "\n",
    "**Entrega:**\n",
    "\n",
    "| Palabra/frase              | BERT-m-cased (subwords) | XLM-R-base (subwords) | Observación |\n",
    "| -------------------------- | ----------------------- | --------------------- | ----------- |\n",
    "| «arquitecturas serverless» |                         |                       |             |\n",
    "| «observabilidad»           |                         |                       |             |\n",
    "| …                          |                         |                       |             |\n",
    "\n",
    "**Pregunta guía:** ¿Cuál produce menos fragmentación en español técnico?\n",
    "\n",
    "\n",
    "**3) Zero-shot para moderación (conteo manual)**\n",
    "\n",
    "**Objetivo:** evaluar rápidamente un clasificador *zero-shot* con etiquetas de moderación.\n",
    "**Qué hacer:**\n",
    "\n",
    "1. Define **4 etiquetas**: por ejemplo **{\"permitido\", \"spam\", \"odio\", \"sensibles\"}**.\n",
    "2. Crea un **set de 20 textos** cortos (inventados o de clase) y márcalos manualmente con la etiqueta \"verdadera\".\n",
    "3. Pasa los 20 por el *zero-shot* y guarda la **etiqueta predicha**.\n",
    "4. Calcula **precisión aproximada**: TP/(TP+FP) para una etiqueta clave (p. ej., \"spam\").\n",
    "\n",
    "**Entrega (resumen):**\n",
    "\n",
    "| etiqueta | TP | FP | precisión ≈ TP/(TP+FP) | comentario |\n",
    "| -------- | -: | -: | ---------------------: | ---------- |\n",
    "| spam     |    |    |                        |            |\n",
    "\n",
    "**Pregunta guía:** ¿En qué tipo de texto falla más (ironía, jerga, abreviaturas)?\n",
    "\n",
    "\n",
    "**4) Traducción ES<->EN (errores típicos)**\n",
    "\n",
    "**Objetivo:** probar ida y vuelta en **5 oraciones técnicas** (DevOps/DevSecOps).\n",
    "**Qué hacer:**\n",
    "\n",
    "1. Traduce **ES->EN** y **EN->ES** las 5 oraciones.\n",
    "2. Señala **falsos cognados**, **términos mal traducidos** y **orden extraño**.\n",
    "3. Da una **corrección humana** breve para los fallos.\n",
    "\n",
    "**Entrega (por oración):**\n",
    "\n",
    "* **Original (ES):**\n",
    "* **Traducción (EN):**\n",
    "* **Errores detectados:**\n",
    "* **Corrección sugerida:**\n",
    "\n",
    "Repite para EN->ES.\n",
    "\n",
    "**5) Resumen automático (comparación ligera)**\n",
    "\n",
    "**Objetivo:** comparar dos *summarizers* si el entorno lo permite.\n",
    "**Modelos sugeridos:** `distilbart-cnn-12-6` vs `facebook/bart-large-cnn` (**si tu máquina los soporta**).\n",
    "**Si no cargan modelos grandes:** usa un **modelo text-to-text** (por ejemplo., FLAN-T5) como *fallback* y documenta la sustitución.\n",
    "\n",
    "**Qué hacer:**\n",
    "\n",
    "1. Elige un **texto fuente** (200-400 palabras).\n",
    "2. Genera dos resúmenes (o uno + *fallback*).\n",
    "3. Evalúa con **ROUGE** (si puedes) o, si no, usa una rúbrica simple:\n",
    "\n",
    "   * **Cobertura** (0-2): ¿incluye las ideas principales?\n",
    "   * **Coherencia** (0-2): ¿fluye y no repite?\n",
    "   * **Fidelidad** (0-2): ¿no inventa datos?\n",
    "\n",
    "**Entrega:**\n",
    "\n",
    "| sistema             | cobertura (0-2) | coherencia (0-2) | fidelidad (0-2) | notas |\n",
    "| ------------------- | --------------: | ---------------: | --------------: | ----- |\n",
    "| Modelo A            |                 |                  |                 |       |\n",
    "| Modelo B / Fallback |                 |                  |                 |       |\n",
    "\n",
    "**6) Prompting con FLAN-T5 (tres usos)**\n",
    "\n",
    "**Objetivo:** practicar instrucciones claras.\n",
    "\\*\\*Qué hacer con **un mismo párrafo técnico**:\n",
    "\n",
    "1. **Clasificación**: \"¿A qué tema pertenece? {DevOps, Seguridad, Observabilidad}\".\n",
    "2. **Extracción de claves**: \"Devuelve 5 términos clave en español\".\n",
    "3. **Reescritura**: \"Reescribe en **estilo académico breve** en 2-3 frases\".\n",
    "\n",
    "**Entrega (para cada prompt):**\n",
    "\n",
    "* **Entrada:** (párrafo)\n",
    "* **Salida:** (resultado del modelo)\n",
    "* **Breve nota:** ¿qué cambiarías en la instrucción para mejorar claridad?\n",
    "\n",
    "**7) Ética y riesgos**\n",
    "\n",
    "**Objetivo:** conectar con prácticas DevSecOps.\n",
    "**Qué cubrir (8-12 líneas):**\n",
    "\n",
    "* **Sesgos** (ejemplos y mitigaciones básicas: *prompting*, filtros, revisión humana).\n",
    "* **Privacidad** (datos personales, minimización, anonimización).\n",
    "* **Trazabilidad** (logs estructurados, *model cards*, versiones de modelo/datos).\n",
    "* **Despliegue seguro** (control de acceso, tests de *jailbreak*, monitoreo de salida).\n",
    "* **Ciclo de vida** (actualizaciones del modelo, *rollback*, auditoría).\n",
    "\n",
    "**Entrega:** un párrafo único, conciso, con 1-2 recomendaciones accionables para tu contexto de curso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e25c79-f1b3-43c2-9b63-6a26514a3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Introducción a LLMs y Casos de Uso (torchtext + HF).ipynb",
   "provenance": []
  },
  "created": "2025-09-06T03:26:05.426483Z",
  "custom": {
   "exercises_appended_at": "2025-09-06T04:18:40.337486Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
