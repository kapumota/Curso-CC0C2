{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bac78e-47d3-4da6-bca6-1c74ac4e55aa",
   "metadata": {},
   "source": [
    "### Regularización en modelos de lenguaje\n",
    "\n",
    "#### La funcion count\n",
    "\n",
    "Supongamos que tenemos el siguiente texto de ejemplo:\n",
    "\n",
    "`El gato come pescado. El gato duerme. El gato juega.`\n",
    "\n",
    "Si quisiéramos calcular la probabilidad condicional de la palabra `duerme` dado que la palabra anterior es `gato`, usaríamos `Count` de la siguiente manera:\n",
    "\n",
    "* `Count(\"El gato duerme\")` sería 1, porque esa secuencia de palabras aparece una vez en el texto.\n",
    "* `Count(\"El gato\")` sería 3, porque esa secuencia de palabras aparece tres veces en el texto.\n",
    "\n",
    "Entonces, la probabilidad condicional de que la palabra `duerme` siga a la palabra `gato` sería:\n",
    "\n",
    "$$P(\\text{\"duerme\"}∣\\text{\"El gato\"})= \\frac{\\text{Count(\"El gato\")}}{\\text{Count(\"El gato duerme\")}} = \\frac{1}{3}$$\n",
    " \n",
    "\n",
    "Esto significa que, en nuestro corpus de ejemplo, la probabilidad condicional de que `duerme` siga a `El gato` es de un tercio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517629c2-963e-4159-87aa-b0a3bf234da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Nuestro corpus de ejemplo\n",
    "corpus = \"El gato come pescado. El gato duerme. El gato juega.\"\n",
    "\n",
    "# Dividir el corpus en oraciones y luego en palabras\n",
    "sentences = corpus.split('. ')\n",
    "words = [word for sentence in sentences for word in sentence.split()]\n",
    "\n",
    "# Crear los contadores de bigramas y unigramas\n",
    "bigrams = Counter(zip(words[:-1], words[1:]))\n",
    "unigrams = Counter(words)\n",
    "\n",
    "# Calcular la probabilidad condicional\n",
    "def conditional_probability(wi, w_previous):\n",
    "    bigram_count = bigrams[(w_previous, wi)]\n",
    "    unigram_count = unigrams[w_previous]\n",
    "    return bigram_count / unigram_count if unigram_count > 0 else 0\n",
    "\n",
    "# Ejemplo: P(\"duerme\" | \"gato\")\n",
    "prob = conditional_probability(\"duerme\", \"gato\")\n",
    "print(\"P('duerme' | 'gato'):\", prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f406faf-1b8b-480f-81a1-0dad9acb5bc9",
   "metadata": {},
   "source": [
    "El código anterior crea contadores para bigramas (pares de palabras) y unigramas (palabras individuales) y proporciona una función para calcular la probabilidad condicional de una palabra dada su palabra anterior en el contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94206055-586c-4488-8ce6-bab7f267cad4",
   "metadata": {},
   "source": [
    "#### Probabilidad condicional de un bigrama\n",
    "\n",
    "La fórmula presentada en la imagen es una aproximación de la probabilidad condicional de una palabra en el contexto de la palabra que la precede inmediatamente, basada en la frecuencia de aparición de las palabras en un corpus dado. Esto se utiliza comúnmente en el modelado de lenguaje y se relaciona con el concepto de modelos de bigramas en procesamiento de lenguaje natural (NLP):\n",
    "\n",
    "La fórmula es:\n",
    "\n",
    "$$ P(w_i|w_{i -1}) \\approx \\frac{P(w_i, w_{i -1})}{P(w_{i -1})} = \\frac{\\text{Count}(w_i, w_{i -1})}{\\text{Count}(w_{i -1})}$$\n",
    "\n",
    "Esta fórmula supone que la probabilidad de una palabra depende solo de la palabra inmediatamente anterior, lo cual es una simplificación pero facilita enormemente los cálculos en el modelado de lenguaje.\n",
    "\n",
    "Vamos a presentar un ejemplo de código en Python que implementa esta fórmula para calcular la probabilidad condicional de un bigrama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74263dd2-d4a4-4743-894a-2f4c400f99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Supongamos que tenemos el siguiente corpus de ejemplo:\n",
    "corpus = \"el gato persigue a la rata el gato duerme el gato come\".split()\n",
    "\n",
    "# Calculamos las frecuencias de los bigramas (pares de palabras consecutivas)\n",
    "bigram_counts = Counter(zip(corpus[:-1], corpus[1:]))\n",
    "\n",
    "# Calculamos las frecuencias de las palabras individuales (unigramas)\n",
    "unigram_counts = Counter(corpus)\n",
    "\n",
    "# Definimos una función para calcular la probabilidad condicional de un bigrama\n",
    "def bigram_probability(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
    "\n",
    "# Ejemplo de uso: Probabilidad de 'gato' seguido por 'duerme'\n",
    "prob = bigram_probability('gato', 'duerme')\n",
    "print(f\"La probabilidad de 'duerme' dado 'gato' es: {prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d764e-2d1d-4070-84ae-7ad740bceaf8",
   "metadata": {},
   "source": [
    "Es importante destacar que este modelo es muy básico y asume que nuestro \"corpus\" es simplemente una cadena de texto. En aplicaciones reales de NLP, se llevaría a cabo una limpieza y normalización de texto más sofisticada, y se usarían corpus mucho más grandes para obtener estimaciones más precisas de las probabilidades. \n",
    "\n",
    "Además, los modelos más avanzados pueden utilizar técnicas como el suavizado de Laplace para manejar bigramas que nunca han sido vistos en el corpus (es decir, para evitar la división por cero o multiplicar la probabilidad por cero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5babfc-5a0c-4b14-b2fc-9cefc214ac9d",
   "metadata": {},
   "source": [
    "### Backoff para un modelo bigramas\n",
    "\n",
    "Los métodos de back-off son una técnica utilizada en el modelado de lenguaje para tratar con el problema de las palabras o secuencias de palabras (n-gramas) que no han sido vistas en el conjunto de datos de entrenamiento, también conocido como el problema de los n-gramas \"fuera del vocabulario\" o \"n-gramas raros\".\n",
    "\n",
    "En el contexto del modelado de lenguaje usando bigramas y trigramas, como en los modelos Markovianos que describimos en clase, la idea básica del back-off es retroceder a un modelo más simple cuando no hay suficiente información para tomar una decisión con el modelo más complejo. \n",
    "\n",
    "En el contexto del modelo de bigramas, si estamos intentando calcular la probabilidad de una palabra dado su predecesor como `P(\"duerme\"∣\n",
    "\"gato\")` podríamos encontrarnos con que el par `gato duerme` nunca ocurre en el corpus de entrenamiento. En este caso, el modelo de bigramas no tendría una estimación de la probabilidad porque el contador para ese bigrama sería cero. Aquí es donde aplicaríamos back-off: en lugar de utilizar la probabilidad de bigrama (que sería cero), retrocedemos a la probabilidad de unigrama, que es simplemente la probabilidad de la palabra `duerme`  sin tener en cuenta su contexto.\n",
    "\n",
    "En el contexto del modelo de trigramas el proceso es similar pero tiene un paso adicional. Si estamos buscando `P(\"duerme\"∣\"el gato\")` y no tenemos datos para ese trígrama, procederíamos de la siguiente manera:\n",
    "\n",
    "- Primero verificamos si existe información sobre el trígrama (`el gato duerme`).\n",
    "- Si no hay suficientes ocurrencias del trígrama, retrocedemos a un bigrama, reduciendo la ventana de contexto a una palabra (`gato duerme`).\n",
    "- Si también falta información sobre el bigrama, finalmente usamos el unigrama (`duerme`).\n",
    "\n",
    "Este enfoque es fundamentalmente recursivo y puede extenderse a n-gramas de cualquier longitud. El back-off a menudo se acompaña de suavizado, ya que al retroceder a n-gramas más cortos, se redistribuyen las probabilidades de manera que sumen 1. Un ejemplo de suavizado sería el suavizado de Laplace, donde se agrega una constante a todos los conteos para evitar probabilidades cero.\n",
    "\n",
    "Aquí hay una implementación conceptual simple de un método de back-off para un modelo de bigramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd3091-22a2-442f-851c-7d8f079713bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words):\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    if bigram_count > 0:\n",
    "        return bigram_count / unigram_counts.get(w1, 0)\n",
    "    else:\n",
    "        # Back-off to unigram probability\n",
    "        return unigram_counts.get(w2, 0) / total_words\n",
    "\n",
    "# Total de palabras en el corpus para calcular probabilidades de unigramas\n",
    "total_words = sum(unigram_counts.values())\n",
    "\n",
    "# Probabilidad con back-off\n",
    "prob_backoff = backoff_bigram_probability('gato', 'duerme', unigram_counts, bigram_counts, total_words)\n",
    "print(f\"Probabilidad de back-off para 'duerme' dado 'gato': {prob_backoff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7fbb6-0615-47e1-a387-bc0205d5149b",
   "metadata": {},
   "source": [
    "En este código, primero verificamos si tenemos un conteo para el bigrama específico. Si no, retrocedemos a la probabilidad de unigrama de la palabra `\"duerme\"`. El método de back-off garantiza que nunca tendremos una probabilidad de cero, lo cual es crucial para la generación de lenguaje y otras aplicaciones del modelado de lenguaje donde todas las posibles palabras siguientes deben tener alguna probabilidad no nula de ser elegidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a6e64-69da-4286-8850-12c0de013388",
   "metadata": {},
   "source": [
    "#### **Suavizado de Laplace**\n",
    "\n",
    "La ecuación siguiente presentada es una fórmula para calcular la probabilidad condicional de un bigrama con suavizado de Laplace, que es un tipo de suavizado utilizado en el modelado de lenguaje para manejar el problema de los bigramas que no aparecen en el conjunto de entrenamiento (es decir, con conteo cero). \n",
    "\n",
    "El suavizado de Laplace se aplica a la estimación de probabilidades para garantizar que no haya probabilidades condicionales iguales a cero, lo cual puede ser problemático en muchas aplicaciones de NLP:\n",
    "\n",
    "La fórmula es:\n",
    "\n",
    "$$ P(w_i|w_{i -1}) = \\frac{\\text{Count}(w_i, w_{i -1}) +\\beta}{\\text{Count}(w_{i -1}) +d\\cdot \\beta}$$\n",
    "\n",
    "- $\\beta$ es el factor de suavizado de Laplace, que es una constante positiva añadida a cada conteo para evitar probabilidades condicionales de cero.\n",
    "- $d$ es el tamaño del vocabulario, que es el número de palabras únicas en el conjunto de datos de entrenamiento.\n",
    "\n",
    "El término  $d\\cdot \\beta$ en el denominador asegura que el suavizado se aplique de manera uniforme a todas las posibles palabras que siguen a $w_{i−1}$\n",
    "\n",
    "El factor $\\beta$ es crucial porque:\n",
    "\n",
    "- Evita que las probabilidades condicionales sean cero cuando el conteo del bigrama es cero.\n",
    "- Distribuye una pequeña cantidad de probabilidad a todos los bigramas posibles, incluso a aquellos que no se han observado en el conjunto de datos.\n",
    "- El valor de $\\beta$ generalmente es menor que 1. Un valor común puede ser 1 (conocido como suavizado de Laplace), o un valor más pequeño (conocido como suavizado de Lidstone).\n",
    "- El efecto de $\\beta$ es más notable cuando hay pocos datos disponibles en grandes conjuntos de datos, su impacto es relativamente menor porque los conteos reales dominan la estimación de la probabilidad.\n",
    "- \n",
    "Aquí hay un ejemplo simple de cómo se podría implementar esta ecuación en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bc992-40e1-45c6-ad29-bc17e38c593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Inicializamos los conteos de bigramas y unigramas\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "\n",
    "# Asumimos que tenemos un conjunto de entrenamiento\n",
    "corpus = [\"el gato come pescado\", \"el gato come y duerme\", \"el perro come huesos\"]\n",
    "\n",
    "# Poblamos los conteos\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram_counts[(words[i], words[i+1])] += 1\n",
    "        unigram_counts[words[i]] += 1\n",
    "    unigram_counts[words[-1]] += 1  # Aseguramos contar la última palabra de cada oración\n",
    "\n",
    "# Establecemos el factor beta y el tamaño del vocabulario\n",
    "beta = 1\n",
    "vocab_size = len(unigram_counts)\n",
    "\n",
    "# Definimos la función para calcular la probabilidad con suavizado de Laplace\n",
    "def laplace_smoothing_bigram_probability(w1, w2, beta, vocab_size):\n",
    "    bigram_prob = (bigram_counts[(w1, w2)] + beta) / (unigram_counts[w1] + beta * vocab_size)\n",
    "    return bigram_prob\n",
    "\n",
    "# Ejemplo: probabilidad del bigrama \"gato come\"\n",
    "prob = laplace_smoothing_bigram_probability(\"gato\", \"come\", beta, vocab_size)\n",
    "print(f\"La probabilidad del bigrama 'gato come' con suavizado de Laplace es: {prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d3b9c-9b07-47de-8f0d-8358585ecbee",
   "metadata": {},
   "source": [
    "Este código cuenta los bigramas y unigramas en el corpus de ejemplo, y luego utiliza la fórmula de suavizado de Laplace para calcular la probabilidad de un bigrama dado. El suavizado asegura que incluso si un bigrama no está presente en el corpus (por ejemplo, `gato vuela`), todavía asignaría una probabilidad pequeña pero no cero a esa secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf6199-df0d-41ee-9a1e-94b74cd8ed89",
   "metadata": {},
   "source": [
    "#### **Implementación del modelo Skip-gram**\n",
    "\n",
    "La siguiente ecuación define formalmente el conjunto de todos los posibles `k-skip-n-grams` de una secuencia de palabras. Un `k-skip-n-gram` es una generalización de un n-gram, donde las palabras no necesitan ser consecutivas en la secuencia original; puede haber hasta `k` palabras omitidas entre cada par de palabras consecutivas en el n-grama.\n",
    "\n",
    "La ecuación es: \n",
    "\n",
    "$$S(k,n) = \\{w_{i_1}w_{i_2}\\dots w_{i_n}|\\sum_{j=2}^{n}(i_j -i_{j -1} -1)\\leq k; i_j \\geq i_{j -1} \\forall j  \\}  $$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $S(k,n)$ es el conjunto de todos los k-skip-n-gramas.\n",
    "- $k$ es el número máximo de palabras que se pueden omitir entre dos palabras dentro de un skip-grama.\n",
    "- $n$ es la longitud del n-gramas que estamos formando (no contando las omisiones).\n",
    "- $i_j$ es la posición de la palabra actual en la secuencia original y $i_{j -1}$ es la posición de la palabra anterior en el skip-gram.\n",
    "- $\\sum_{j=2}^{n}(i_j -i_{j -1} -1) $ es la suma de las palabras omitidas en el skip-grama completo. Si esta suma es menor o igual a $k$, entonces la secuencia es un k-skip-n-grama válido.\n",
    "- $w_{i_1}w_{i_2}\\dots w_{i_n}$  es una secuencia de palabras seleccionadas para formar un skip-grama.\n",
    "\n",
    "A continuación, te muestro cómo podríamos calcular skip-gramas en Python usando la definición proporcionada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688e3e6-c775-4174-a47c-fe0330f2f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_grams(sequence, n, k):\n",
    "    skip_grams = []\n",
    "    for i in range(len(sequence) - n + 1):\n",
    "        # Esta es la ventana inicial para el n-gram sin skips\n",
    "        base_gram = sequence[i:i+n]\n",
    "        skip_grams.append(base_gram)\n",
    "        for skip in range(1, k+1):\n",
    "            # Intenta generar skip-grams para esta base añadiendo palabras omitidas\n",
    "            for j in range(i+n, min(len(sequence), i+n+skip)):\n",
    "                for l in range(n-1):\n",
    "                    skip_gram = base_gram[:l+1] + [sequence[j]] + base_gram[l+1:]\n",
    "                    if skip_gram not in skip_grams:\n",
    "                        skip_grams.append(skip_gram)\n",
    "    return skip_grams\n",
    "\n",
    "# Ejemplo de uso\n",
    "sequence = [\"El\", \"gato\", \"come\", \"pescado\"]\n",
    "n = 2  # Queremos bigramas\n",
    "k = 1  # Permitimos un skip\n",
    "skip_grams = generate_skip_grams(sequence, n, k)\n",
    "for sg in skip_grams:\n",
    "    print(sg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bc276-bab5-4f70-ae2a-77e080a5f972",
   "metadata": {},
   "source": [
    "Este código generará todos los posibles skip-gramas de acuerdo con los parámetros `n` y `k` dados. En este caso, el parámetro `n` está configurado en 2 para bigramas y `k` está configurado en 1, lo que significa que se permitirá la omisión de una palabra entre las dos palabras del bigrama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce3955-0eb8-460b-bb5f-2d1f71e7530c",
   "metadata": {},
   "source": [
    "Vamos a extender la función `generate_skip_grams` para manejar trigramas con dos skips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b3e16-541f-4648-885d-475943dd144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_grams(sequence, n, k):\n",
    "    skip_grams = []\n",
    "    \n",
    "    def recurse_gram(base, start, depth):\n",
    "        if depth == n:\n",
    "            skip_grams.append(base)\n",
    "            return\n",
    "        for i in range(start, min(len(sequence), start + k + 1)):\n",
    "            recurse_gram(base + [sequence[i]], i + 1, depth + 1)\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        recurse_gram([sequence[i]], i + 1, 1)\n",
    "    \n",
    "    return skip_grams\n",
    "\n",
    "# Ejemplo de uso\n",
    "sequence = [\"El\", \"gato\", \"come\", \"pescado\"]\n",
    "n = 3  # Queremos trigramas\n",
    "k = 2  # Permitimos dos skips\n",
    "skip_grams = generate_skip_grams(sequence, n, k)\n",
    "for sg in skip_grams:\n",
    "    print(\" \".join(sg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0d54a-3ee4-496c-b661-d9a4714bb970",
   "metadata": {},
   "source": [
    "Este código imprimirá todos los posibles 2-skip-trigrams basados en la secuencia dada. La función `recurse_gram` se llama recursivamente para construir los skip-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d92fb-5b3e-4d74-aa65-04097a782c03",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "#### **Ejercicio 1: Análisis de frecuencia de bigramas**\n",
    "Dado un corpus de texto más grande (por ejemplo, un artículo, un capítulo de un libro o una colección de tuits), realiza lo siguiente:\n",
    "\n",
    "1. Limpia el texto eliminando signos de puntuación y convirtiendo todo a minúsculas.\n",
    "2. Divide el texto en palabras y calcula la frecuencia de todos los bigramas.\n",
    "3. Encuentra los 10 bigramas más frecuentes en el texto.\n",
    "4. Calcula la probabilidad condicional de la segunda palabra de cada bigrama dado la primera palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18099c-7892-4ddd-b1fc-3b2374a504c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356a834-3b42-40ed-850d-d436dec2df42",
   "metadata": {},
   "source": [
    "#### **Ejercicio 2: Suavizado de Laplace**\n",
    "Implementa una función de suavizado de Laplace que funcione para cualquier n-grama:\n",
    "\n",
    "1. Define una función para contar n-gramas de cualquier longitud en un corpus dado.\n",
    "2. Implementa el suavizado de Laplace en una función que calcule la probabilidad de un n-grama dado.\n",
    "3. Prueba tu función con un conjunto de datos de ejemplo y varios valores de $n$ y $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9425aea-268a-463e-9f48-ff48b025e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c662564-9add-44f1-9a1f-72652cb93ef2",
   "metadata": {},
   "source": [
    "#### **Ejercicio 3: Generación de texto con modelos Markovianos**\n",
    "\n",
    "Construye un modelo generativo de texto simple:\n",
    "\n",
    "1. Utiliza un conjunto de datos de texto para entrenar un modelo de bigramas con suavizado de Laplace.\n",
    "2. Escribe una función que genere texto de manera aleatoria, seleccionando la siguiente palabra en función de las probabilidades del modelo.\n",
    "3. Genera un párrafo de texto y evalúa su coherencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929e30b-5e5f-419c-96d1-a8cc441787e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44e00c-4048-4626-a0e2-026179eb62be",
   "metadata": {},
   "source": [
    "#### **Ejercicio 4: Implementación de backoff**\n",
    "Amplía el modelo de bigramas para implementar una estrategia de backoff:\n",
    "\n",
    "1. Implementa una función que calcule la probabilidad de un bigrama utilizando backoff al unigrama cuando no hay suficiente información.\n",
    "2. Asegúrate de que el suavizado de Laplace esté incluido en tu cálculo.\n",
    "3. Prueba tu modelo con secuencias de palabras que no estén en tu corpus y compara los resultados con y sin backoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6ddd0-b7f1-4477-b301-eadcd9cd687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3911d-2ca1-4e40-ab51-54ee59f7d658",
   "metadata": {},
   "source": [
    "#### **Ejercicio 5: Modelos Skip-gram**\n",
    "Practica con modelos skip-gram:\n",
    "\n",
    "1. Escribe una función que genere todos los posibles k-skip-n-gramas para un valor dado de $k$ y $n$.\n",
    "2. Calcula las frecuencias de estos k-skip-n-gramas en un corpus de texto.\n",
    "3. Analiza cómo la elección de $k$ afecta la cantidad y la variedad de skip-gramas generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15594668-bc54-49e1-a196-ae18a4f65622",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
