{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07dee1a-77b7-46c0-924a-7473351a5934",
   "metadata": {},
   "source": [
    "### Semántica de vectores y embeddings\n",
    "\n",
    "El estudio del significado de las palabras, conocido como *semántica léxica*, es una rama esencial de la lingüística que busca entender cómo las palabras transmiten significado y cómo se relacionan entre sí en un lenguaje. Uno de los enfoques modernos más influyentes en este campo es la **hipótesis distribucional**, que postula que las palabras que ocurren en contextos similares tienden a tener significados similares. Esta idea ha llevado al desarrollo de modelos semánticos vectoriales que representan el significado de las palabras basándose en sus distribuciones en grandes corpus de texto.\n",
    "\n",
    "\n",
    "**Hipótesis distribucional**\n",
    "\n",
    "Formulada en la década de 1950 por lingüistas como Joos, Harris y Firth, la hipótesis distribucional establece que el significado de una palabra está estrechamente ligado a su contexto lingüístico. En otras palabras, si dos palabras aparecen en contextos similares, es probable que compartan aspectos de significado. Por ejemplo, las palabras *oculist y *eye-doctor* tienden a aparecer cerca de términos como *eye* o *examined*, lo que sugiere que comparten un significado relacionado con la atención ocular.\n",
    "\n",
    "Esta hipótesis ha sido fundamental para el desarrollo de técnicas que permiten aprender *embeddings* o representaciones vectoriales de palabras directamente de grandes conjuntos de datos textuales. Estas representaciones capturan relaciones semánticas y son ampliamente utilizadas en aplicaciones de procesamiento del lenguaje natural (NLP), como el análisis de sentimientos, la traducción automática y la respuesta a preguntas.\n",
    "\n",
    "\n",
    "**Semántica léxica**\n",
    "\n",
    "La semántica léxica se centra en cómo las palabras transmiten significado y cómo se relacionan semánticamente entre sí. Uno de los desafíos clave en este campo es encontrar formas efectivas de representar el significado de las palabras de manera que refleje sus relaciones y matices semánticos.\n",
    "\n",
    "Tradicionalmente, en modelos *n*-grama y aplicaciones clásicas de NLP, las palabras se representaban simplemente como cadenas de caracteres o índices en un vocabulario. Sin embargo, este enfoque es limitado, ya que no captura las relaciones semánticas entre palabras ni permite inferir similitudes o diferencias de significado.\n",
    "\n",
    "La semántica léxica busca ir más allá de estas limitaciones al considerar aspectos como la sinonimia, la polisemia, las connotaciones y las relaciones semánticas más amplias entre palabras y conceptos.\n",
    "\n",
    "\n",
    "**Lemas y sentidos**\n",
    "\n",
    "Un aspecto fundamental en el estudio del significado de las palabras es la distinción entre *lemas* y *formas de palabras*. El lema es la forma base o de cita de una palabra, que representa su entrada en un diccionario. Por ejemplo, *ratón* es el lema tanto para *ratón* como para *ratones*. De manera similar, *cantar* es el lema para *canto*, *canté*, *cantado*, etc.\n",
    "\n",
    "Las palabras pueden tener múltiples significados o *sentidos*, fenómeno conocido como polisemia. Por ejemplo, el lema *mouse* puede referirse al pequeño roedor o al dispositivo de entrada utilizado en computadoras. La capacidad de una palabra para tener múltiples sentidos puede generar ambigüedad en la comunicación y es un área de interés en tareas de NLP como la *desambiguación de sentidos de palabras*, que busca determinar el significado específico de una palabra en un contexto dado.\n",
    "\n",
    "\n",
    "**Sinonimia**\n",
    "\n",
    "La *sinonimia* es la relación entre palabras que tienen significados idénticos o muy similares. Aunque es raro encontrar sinónimos perfectos, existen palabras que pueden sustituirse entre sí en ciertos contextos sin alterar significativamente el significado de una oración. Ejemplos de sinónimos incluyen:\n",
    "\n",
    "- *Sofá* y *diván*\n",
    "- *Automóvil* y *coche*\n",
    "- *Alegre* y *feliz*\n",
    "\n",
    "La sinonimia es esencial para tareas de NLP que implican reconocimiento de parafraseo y expansión de consultas en motores de búsqueda. Sin embargo, es importante reconocer que incluso los sinónimos pueden tener diferencias sutiles en connotación, registro o uso que afectan su idoneidad en distintos contextos.\n",
    "\n",
    "\n",
    "**Similaridad de palabras**\n",
    "\n",
    "Más allá de la sinonimia, muchas palabras comparten grados de similitud sin ser completamente intercambiables. La *similaridad de palabras* es una medida que refleja cuánto comparten en común dos términos en términos de significado o características semánticas.\n",
    "\n",
    "Por ejemplo, *gato* y *perro* no son sinónimos, pero son semánticamente similares al ser ambos animales domésticos y mascotas comunes. Esta similitud puede cuantificarse mediante escalas o modelos que asignan puntuaciones basadas en juicios humanos o análisis computacionales.\n",
    "\n",
    "El conjunto de datos *SimLex-999* es un ejemplo de recursos que miden la similitud entre pares de palabras en una escala de 0 a 10. Algunos ejemplos de este conjunto incluyen:\n",
    "\n",
    "| Palabra 1  | Palabra 2    | SimLex-999 |\n",
    "|------------|--------------|------------|\n",
    "| vanish     | disappear    | 9.8        |\n",
    "| belief     | impression   | 5.95       |\n",
    "| muscle     | bone         | 3.65       |\n",
    "| modest     | flexible     | 0.98       |\n",
    "| hole       | agreement    | 0.3        |\n",
    "\n",
    "\n",
    "La capacidad de evaluar y modelar la similitud de palabras es crucial para aplicaciones como la traducción automática y la recuperación de información, donde es importante reconocer términos relacionados semánticamente.\n",
    "\n",
    "\n",
    "**Relación de palabras**\n",
    "\n",
    "Las palabras pueden estar relacionadas de diversas maneras más allá de la similitud directa. Las *relaciones de palabras* o *asociaciones de palabras* incluyen conexiones temáticas, funcionales o contextuales entre términos. Por ejemplo:\n",
    "\n",
    "- *Café* y *taza*: aunque no son similares, están relacionados porque uno suele contener al otro.\n",
    "- *Escalpelo* y *cirujano*: relacionados por la herramienta y el profesional que la utiliza.\n",
    "\n",
    "Estas relaciones se exploran mediante conceptos como *campos semánticos*, que agrupan palabras dentro de un dominio o tema específico, como términos médicos, culinarios o arquitectónicos. Los *modelos de tópicos*, como la asignación de Dirichlet Latente (LDA), aprovechan estas relaciones para identificar y agrupar automáticamente temas en grandes conjuntos de textos.\n",
    "\n",
    "**Marcos semánticos y roles**\n",
    "\n",
    "Los *marcos semánticos* son estructuras que representan eventos o situaciones típicas, incluyendo los participantes y roles involucrados. Por ejemplo, el marco de una *transacción comercial* incluye roles como comprador, vendedor, bienes y dinero.\n",
    "\n",
    "Las palabras pueden activar o evocan estos marcos y roles semánticos. Por ejemplo:\n",
    "\n",
    "- *Comprar*: enfoca el evento desde la perspectiva del comprador.\n",
    "- *Vender*: lo hace desde la perspectiva del vendedor.\n",
    "- *Pagar*: destaca el aspecto monetario de la transacción.\n",
    "\n",
    "Comprender estos marcos y roles es esencial para tareas como la comprensión del lenguaje natural y la generación de parafraseos. Permite reconocer que oraciones como *\"Ana compró un libro a Luis\"* y *\"Luis vendió un libro a Ana\"* describen el mismo evento desde diferentes perspectivas, lo cual es crucial para aplicaciones de resumen automático y traducción.\n",
    "\n",
    "\n",
    "**Connotaciones**\n",
    "\n",
    "Las *connotaciones* se refieren a los significados adicionales o emociones asociadas que una palabra evoca más allá de su definición literal. Incluyen aspectos afectivos, evaluativos y culturales que influyen en cómo se percibe un término.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "- *Positivo*: *maravilloso*, *excelente*, *felicidad*.\n",
    "- *Negativo*: *terrible*, *horrible*, *tristeza*.\n",
    "\n",
    "Incluso palabras con significados similares pueden tener connotaciones diferentes:\n",
    "\n",
    "- *Ahorrativo* vs. *tacaño*: ambos implican gastar poco dinero, pero el primero tiene una connotación positiva y el segundo negativa.\n",
    "- *Curioso* vs. *entrometido*: ambos relacionados con el deseo de saber, pero con connotaciones distintas.\n",
    "\n",
    "Las connotaciones son especialmente importantes en el análisis de sentimientos y opinión, donde se busca identificar la actitud o emoción expresada en un texto. Modelos que capturan estas dimensiones afectivas ayudan a entender mejor las percepciones y reacciones humanas ante ciertos estímulos lingüísticos.\n",
    "\n",
    "\n",
    "**Dimensiones afectivas**\n",
    "\n",
    "Investigaciones tempranas en psicología identificaron tres dimensiones principales que capturan el significado afectivo de las palabras:\n",
    "\n",
    "1. **Valencia**: grado de agradabilidad o desagrado que provoca una palabra.\n",
    "2. **Activación**: nivel de excitación o intensidad emocional asociada.\n",
    "3. **Dominancia**: sensación de control o influencia que ejerce el estímulo.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "- *Feliz*: alta valencia, alta activación, alta dominancia.\n",
    "- *Calma*: alta valencia, baja activación, moderada dominancia.\n",
    "- *Enojado*: baja valencia, alta activación, alta dominancia.\n",
    "- *Asustado*: baja valencia, alta activación, baja dominancia.\n",
    "\n",
    "En inglés la misma figura se repite, palabras como *happy* o *satisfied* son altas en valencia, mientras que *unhappy* o *annoyed* son bajas en valencia. *Excited* es alta en activación, mientras que *calm* es baja en activación. *Controlling* es alta en dominancia, mientras que *awed* o *influenced* son bajas en dominancia. Cada palabra se representa así por tres números, correspondientes a su valor en cada una de las tres dimensiones:\n",
    "\n",
    "| Palabra     | Valencia | Activación | Dominancia |\n",
    "|-------------|----------|------------|------------|\n",
    "| happy       | 7.96     | 6.46       | 7.09       |\n",
    "| satisfied   | 7.63     | 5.06       | 6.35       |\n",
    "| annoyed     | 2.93     | 5.81       | 5.32       |\n",
    "| heartbreak  | 2.45     | 5.65       | 3.58       |\n",
    "\n",
    "Osgood y colaboradores notaron que al usar estos 3 números para representar el significado de una palabra, el modelo estaba representando cada palabra como un punto en un espacio tridimensional, un vector cuyas tres dimensiones correspondían a la calificación de la palabra en las tres escalas. Esta idea revolucionaria de que el significado de las palabras podría representarse como un punto en el espacio (por ejemplo, que parte del significado de *heartbreak* puede representarse como el punto `[2.45, 5.65, 3.58])` fue la primera expresión de los modelos de *semántica vectorial*.\n",
    "\n",
    "**Semántica vectorial y aprendizaje de representaciones**\n",
    "\n",
    "La idea de representar el significado de las palabras como puntos en un espacio multidimensional es fundamental en la *semántica vectorial*. Al aprender automáticamente representaciones vectoriales de palabras a partir de grandes corpus de texto, los modelos pueden capturar relaciones semánticas y sintácticas de manera eficiente.\n",
    "\n",
    "Este enfoque es parte del *aprendizaje de representaciones*, donde en lugar de diseñar manualmente características lingüísticas, los algoritmos aprenden directamente de los datos. Modelos como *Word2Vec* y *GloVe* generan *embeddings* estáticos que asignan a cada palabra un vector fijo en un espacio de alta dimensionalidad.\n",
    "\n",
    "Estos *embeddings* son la base para modelos más avanzados como *BERT* y *GPT*, que generan *embeddings* contextuales o dinámicos. Estos modelos tienen en cuenta el contexto circundante para asignar representaciones más precisas a palabras polisémicas, mejorando significativamente el rendimiento en diversas tareas de NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1047e0-bae7-4585-95e8-b30e52ef121e",
   "metadata": {},
   "source": [
    "#### **Ejemplos**\n",
    "\n",
    "Conceptos de la hipótesis distribucional y la semántica vectorial usando embeddings de palabras entrenados a partir de textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb333939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Texto de ejemplo para generar los embeddings\n",
    "texto = \"\"\"\n",
    "La similitud de estos dos mamíferos es uno de muchos ejemplos de evolución paralela o convergente,\n",
    "en la cual contextos o entornos particulares llevan a la evolución de estructuras muy similares en diferentes especies.\n",
    "El papel del contexto también es importante en la similitud de un tipo de organismo menos biológico: la palabra.\n",
    "Las palabras que ocurren en contextos similares tienden a tener significados similares.\n",
    "Este vínculo entre la similitud en cómo las palabras se distribuyen y la similitud en lo que significan se llama la hipótesis distribucional.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocesamiento del texto\n",
    "def preprocess_text(texto):\n",
    "    texto = texto.lower().replace('\\n', ' ')\n",
    "    words = texto.split()\n",
    "    return words\n",
    "\n",
    "words = preprocess_text(texto)\n",
    "\n",
    "# Crear un diccionario de palabras y sus índices\n",
    "vocab = set(words)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\n",
    "# Crear ejemplos de ventanas de contexto para entrenar el modelo\n",
    "def create_context_target_pairs(words, window_size=2):\n",
    "    context_target_pairs = []\n",
    "    for i, word in enumerate(words):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(words), i + window_size + 1)\n",
    "        context = [words[j] for j in range(start, end) if j != i]\n",
    "        target = word\n",
    "        context_target_pairs.append((context, target))\n",
    "    return context_target_pairs\n",
    "\n",
    "context_target_pairs = create_context_target_pairs(words)\n",
    "\n",
    "# Modelo de Word2Vec basado en skip-gram\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        embeds = self.embeddings(input_word)  # Obtenemos los embeddings para todas las palabras del contexto\n",
    "        embed_mean = embeds.mean(dim=0).unsqueeze(0)  # Calculamos la media de los embeddings\n",
    "        out = self.linear(embed_mean)  # Pasamos el embedding medio a través de la capa lineal\n",
    "        return out\n",
    "\n",
    "# Hiperparámetros\n",
    "embedding_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Inicializar el modelo\n",
    "modelo = Word2Vec(vocab_size, embedding_dim)\n",
    "\n",
    "# Función de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=0.001)\n",
    "\n",
    "# Crear datos de entrenamiento en formato tensor\n",
    "def prepare_data(context_target_pairs, word_to_ix):\n",
    "    data = []\n",
    "    for context, target in context_target_pairs:\n",
    "        target_ix = word_to_ix[target]\n",
    "        context_ix = [word_to_ix[word] for word in context]\n",
    "        data.append((context_ix, target_ix))\n",
    "    return data\n",
    "\n",
    "train_data = prepare_data(context_target_pairs, word_to_ix)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(modelo, train_data, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context_ix, target_ix in train_data:\n",
    "            # Los tensores de contexto se mantienen como long para usar en embeddings\n",
    "            context_tensor = torch.tensor(context_ix, dtype=torch.long)\n",
    "            target_tensor = torch.tensor([target_ix], dtype=torch.long)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = modelo(context_tensor)  # Pasamos todo el contexto\n",
    "            loss = criterion(output, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoca {epoch+1}/{epochs}, Loss: {total_loss:.4f}')\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model(modelo, train_data)\n",
    "\n",
    "# Obtener las representaciones de palabras (embeddings)\n",
    "def get_word_embedding(word, modelo, word_to_ix):\n",
    "    word_ix = word_to_ix[word]\n",
    "    word_tensor = torch.tensor([word_ix], dtype=torch.long)\n",
    "    embedding = modelo.embeddings(word_tensor)\n",
    "    return embedding\n",
    "\n",
    "# Ejemplo: Obtener la representación vectorial de una palabra\n",
    "word = \"similitud\"\n",
    "embedding = get_word_embedding(word, modelo, word_to_ix)\n",
    "print(f\"Embedding de '{word}': {embedding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c6eef",
   "metadata": {},
   "source": [
    "Uso del modelo de BERT para desambiguar el significado de la palabra \"mouse\", que puede referirse a un animal o a un dispositivo de control, según el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4689db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Inicializar el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "modelo = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Función para desambiguar el sentido de la palabra \"mouse\"\n",
    "def disambiguate_sense(sentence):\n",
    "    # Tokenizar la oración de entrada con un token [MASK] para el \"mouse\"\n",
    "    masked_sentence = sentence.replace(\"mouse\", \"[MASK]\")\n",
    "    \n",
    "    # Tokenizar la oración\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    \n",
    "    # Predecir la palabra en la posición del token [MASK]\n",
    "    with torch.no_grad():\n",
    "        outputs = modelo(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Obtener la predicción más probable para el token [MASK]\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    predicted_token_id = logits[0, mask_token_index, :].argmax(dim=1)\n",
    "    predicted_token = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "    return predicted_token\n",
    "\n",
    "# Ejemplos de contextos\n",
    "context_animal = \"The mouse is a small, furry animal.\"\n",
    "context_device = \"I use a mouse to control the cursor on my computer.\"\n",
    "\n",
    "# Desambiguación de sentidos para la palabra \"mouse\"\n",
    "sense_animal = disambiguate_sense(context_animal)\n",
    "sense_device = disambiguate_sense(context_device)\n",
    "\n",
    "print(f\"Sentido de 'mouse' en el contexto de un animal: {sense_animal}\")\n",
    "print(f\"Sentido de 'mouse' en el contexto de un dispositivo: {sense_device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ba1b4",
   "metadata": {},
   "source": [
    "Ejemplo de relación de sinonimia entre palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed08ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Asegurarse de que los recursos de NLTK están descargados\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Función para encontrar sinónimos de una palabra\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    \n",
    "    # Buscar conjuntos de sinónimos (synsets) de la palabra\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())  # Añadir sinónimos\n",
    "    \n",
    "    return synonyms\n",
    "\n",
    "# Función para verificar si dos palabras son sinónimas\n",
    "def are_synonyms(word1, word2):\n",
    "    synonyms_word1 = get_synonyms(word1)\n",
    "    return word2 in synonyms_word1\n",
    "\n",
    "# Función para encontrar el grado de sinonimia entre dos palabras\n",
    "def synonym_similarity(word1, word2):\n",
    "    # Obtener los synsets (conjuntos de sinónimos) de ambas palabras\n",
    "    synsets_word1 = wn.synsets(word1)\n",
    "    synsets_word2 = wn.synsets(word2)\n",
    "    \n",
    "    # Si no hay synsets para alguna palabra, devolver 0\n",
    "    if not synsets_word1 or not synsets_word2:\n",
    "        return 0\n",
    "    \n",
    "    # Calcular la máxima similitud de los synsets\n",
    "    max_similarity = 0\n",
    "    for synset1 in synsets_word1:\n",
    "        for synset2 in synsets_word2:\n",
    "            similarity = synset1.wup_similarity(synset2)  # Similaridad de Wu-Palmer\n",
    "            if similarity and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    \n",
    "    return max_similarity\n",
    "\n",
    "# Ejemplos de uso:\n",
    "\n",
    "# 1. Encontrar sinónimos de una palabra\n",
    "word = \"car\"\n",
    "synonyms_car = get_synonyms(word)\n",
    "print(f\"Sinónimos de '{word}': {synonyms_car}\")\n",
    "\n",
    "# 2. Verificar si dos palabras son sinónimas\n",
    "word1 = \"car\"\n",
    "word2 = \"automobile\"\n",
    "if are_synonyms(word1, word2):\n",
    "    print(f\"'{word1}' y '{word2}' son sinónimos.\")\n",
    "else:\n",
    "    print(f\"'{word1}' y '{word2}' NO son sinónimos.\")\n",
    "\n",
    "# 3. Grado de sinonimia entre dos palabras\n",
    "similarity = synonym_similarity(word1, word2)\n",
    "print(f\"El grado de sinonimia entre '{word1}' y '{word2}' es: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc42a1",
   "metadata": {},
   "source": [
    "Calculamos la similaridad entre palabras y explora las relaciones semánticas utilizando embeddings basados en BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Inicializamos el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "modelo = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Función para obtener los embeddings de una palabra\n",
    "def get_word_embedding(word):\n",
    "    # Tokenizamos la palabra\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    \n",
    "    # Obtenemos los embeddings de la última capa de BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = modelo(**inputs)\n",
    "    # Usamos el embedding del token [CLS] (primer token) como la representación de la palabra\n",
    "    embedding = outputs.last_hidden_state[0][1]  # [0][1] asume una única palabra\n",
    "    return embedding\n",
    "\n",
    "# Función para calcular la similaridad coseno entre dos embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "# Función para calcular la similaridad entre dos palabras\n",
    "def word_similarity(word1, word2):\n",
    "    embedding1 = get_word_embedding(word1)\n",
    "    embedding2 = get_word_embedding(word2)\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    return similarity\n",
    "\n",
    "# Ejemplos de cálculo de similaridad\n",
    "word_pairs = [\n",
    "    (\"vanish\", \"disappear\"),\n",
    "    (\"belief\", \"impression\"),\n",
    "    (\"muscle\", \"bone\"),\n",
    "    (\"modest\", \"flexible\"),\n",
    "    (\"hole\", \"agreement\"),\n",
    "]\n",
    "\n",
    "print(\"Similaridad entre palabras:\")\n",
    "for word1, word2 in word_pairs:\n",
    "    sim = word_similarity(word1, word2)\n",
    "    print(f\"'{word1}' y '{word2}': Similaridad = {sim:.2f}\")\n",
    "\n",
    "# Relación semántica entre palabras de un mismo campo (hospital)\n",
    "semantic_field_words = [\"surgeon\", \"scalpel\", \"nurse\", \"anesthetic\", \"hospital\"]\n",
    "\n",
    "print(\"\\nRelación semántica en el campo 'hospital':\")\n",
    "for i in range(len(semantic_field_words)):\n",
    "    for j in range(i + 1, len(semantic_field_words)):\n",
    "        word1 = semantic_field_words[i]\n",
    "        word2 = semantic_field_words[j]\n",
    "        sim = word_similarity(word1, word2)\n",
    "        print(f\"'{word1}' y '{word2}': Similaridad = {sim:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d097f8",
   "metadata": {},
   "source": [
    "Ejemplo de marcos semánticos y roles como las connotaciones afectivas.  Este código primero parafrasea una oración basada en un marco semántico (como una transacción comercial) y luego calcula la similitud afectiva entre las palabras de la oración, usando las dimensiones de valencia, activación, y dominancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos un marco semántico simple para una transacción comercial\n",
    "transaction_frame = {\n",
    "    \"buy\": {\"agent\": \"buyer\", \"theme\": \"good\", \"recipient\": \"seller\"},\n",
    "    \"sell\": {\"agent\": \"seller\", \"theme\": \"good\", \"recipient\": \"buyer\"},\n",
    "    \"pay\": {\"agent\": \"payer\", \"theme\": \"money\", \"recipient\": \"seller\"}\n",
    "}\n",
    "\n",
    "# Función para parafrasear una oración dada un marco semántico\n",
    "def paraphrase_transaction(sentence, verb, new_verb):\n",
    "    # Extraemos los roles semánticos de ambos verbos\n",
    "    old_roles = transaction_frame[verb]\n",
    "    new_roles = transaction_frame[new_verb]\n",
    "    \n",
    "    # Simulación de parafraseo (en la práctica necesitaríamos análisis sintáctico)\n",
    "    # Ejemplo: Sam bought the book from Ling -> Ling sold the book to Sam\n",
    "    agent = sentence.split()[0]  # Primera palabra (suponemos que es el agente)\n",
    "    recipient = sentence.split()[-1]  # Última palabra (suponemos que es el recipiente)\n",
    "    theme = sentence.split()[2]  # Tercer palabra (suponemos que es el bien)\n",
    "    \n",
    "    # Parafraseamos la oración\n",
    "    new_sentence = f\"{recipient} {new_verb} the {theme} to {agent}\"\n",
    "    return new_sentence\n",
    "\n",
    "# Connotación: Valencia, Activación, Dominancia\n",
    "connotation_data = {\n",
    "    \"happy\": (7.96, 6.46, 7.09),\n",
    "    \"satisfied\": (7.63, 5.06, 6.35),\n",
    "    \"annoyed\": (2.93, 5.81, 5.32),\n",
    "    \"heartbreak\": (2.45, 5.65, 3.58),\n",
    "    \"buy\": (5.0, 4.5, 6.0),  # Valores ficticios\n",
    "    \"sell\": (5.0, 4.5, 6.0),  # Valores ficticios\n",
    "    \"Sam\": (6.0, 5.0, 5.5),   # Valores ficticios\n",
    "    \"Ling\": (6.0, 5.0, 5.5),  # Valores ficticios\n",
    "    \"book\": (4.0, 3.0, 2.5)   # Valores ficticios\n",
    "}\n",
    "\n",
    "# Función para obtener el vector de connotación de una palabra\n",
    "def get_connotation_vector(word):\n",
    "    return connotation_data.get(word, None)\n",
    "\n",
    "# Función para calcular la distancia entre connotaciones (en el espacio tridimensional)\n",
    "def connotation_similarity(word1, word2):\n",
    "    vector1 = get_connotation_vector(word1)\n",
    "    vector2 = get_connotation_vector(word2)\n",
    "    \n",
    "    if vector1 and vector2:\n",
    "        # Calculamos la distancia euclidiana en el espacio de valencia, activación, y dominancia\n",
    "        dist = np.linalg.norm(np.array(vector1) - np.array(vector2))\n",
    "        return dist\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Combinación de ambos ejemplos\n",
    "\n",
    "# 1. Parafraseo basado en marco semántico\n",
    "sentence = \"Sam bought the book from Ling\"\n",
    "paraphrased_sentence = paraphrase_transaction(sentence, \"buy\", \"sell\")\n",
    "print(f\"Oración original: {sentence}\")\n",
    "print(f\"Paráfrasis: {paraphrased_sentence}\")\n",
    "\n",
    "# 2. Cálculo de la similitud afectiva (connotación)\n",
    "words_in_sentence = [\"Sam\", \"bought\", \"book\", \"Ling\"]\n",
    "word_pairs = [(\"Sam\", \"Ling\"), (\"buy\", \"sell\"), (\"book\", \"bought\")]\n",
    "\n",
    "print(\"\\nSimilaridad afectiva entre las palabras de la oración:\")\n",
    "for word1, word2 in word_pairs:\n",
    "    similarity = connotation_similarity(word1, word2)\n",
    "    if similarity is not None:\n",
    "        print(f\"Distancia entre '{word1}' y '{word2}' en el espacio de connotación: {similarity:.2f}\")\n",
    "    else:\n",
    "        print(f\"No se encontró connotación para una de las palabras: '{word1}' o '{word2}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06224740-beae-4a17-8298-8775a2384e8e",
   "metadata": {},
   "source": [
    "#### **Ejemplo completo  - version sin usar gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc82c66-b68e-45d5-8d27-1dfab05ca181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de bibliotecas necesarias\n",
    "!pip install nltk matplotlib scikit-learn --quiet\n",
    "\n",
    "# Importación de bibliotecas\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Descarga de recursos de NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Configuración del stemmer para español\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"\"\"\n",
    "El asfalto por el que Los Ángeles es famoso se encuentra principalmente en sus autopistas. Pero en medio de la ciudad hay otro parche de asfalto, los pozos de alquitrán de La Brea, y este asfalto conserva millones de huesos fósiles de la última de las Edades de Hielo del Pleistoceno. Uno de estos fósiles es el Smilodon, o tigre dientes de sable, instantáneamente reconocible por sus largos caninos. Hace unos cinco millones de años, un tigre dientes de sable completamente diferente llamado Thylacosmilus vivió en Argentina y otras partes de América del Sur.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocesamiento del texto\n",
    "# 1. Tokenización de oraciones\n",
    "oraciones = sent_tokenize(texto, language='spanish')\n",
    "\n",
    "# 2. Tokenización de palabras\n",
    "palabras = [word_tokenize(oracion, language='spanish') for oracion in oraciones]\n",
    "\n",
    "# 3. Normalización (minúsculas)\n",
    "palabras_normalizadas = [[palabra.lower() for palabra in oracion] for oracion in palabras]\n",
    "\n",
    "# 4. Eliminación de stopwords y caracteres no alfabéticos\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "palabras_filtradas = [[palabra for palabra in oracion if palabra.isalpha() and palabra not in stop_words] for oracion in palabras_normalizadas]\n",
    "\n",
    "# 5. Stemming usando SnowballStemmer\n",
    "palabras_stemmizadas = [[stemmer.stem(palabra) for palabra in oracion] for oracion in palabras_filtradas]\n",
    "\n",
    "# Vectorización del texto usando CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform([' '.join(sum(palabras_stemmizadas, []))])\n",
    "\n",
    "# Exploración de vecinos semánticos\n",
    "palabra_objetivo = 'asfalt'\n",
    "indice_palabra = vectorizer.vocabulary_.get(palabra_objetivo)\n",
    "if indice_palabra is not None:\n",
    "    similitudes = cosine_similarity(X.T)\n",
    "    vecinos = np.argsort(-similitudes[indice_palabra])[1:6]\n",
    "    print(f\"\\nPalabras más similares a '{palabra_objetivo}':\")\n",
    "    for idx in vecinos:\n",
    "        palabra = list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(idx)]\n",
    "        print(f\"- {palabra} (similitud: {similitudes[indice_palabra, idx]:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nLa palabra '{palabra_objetivo}' no está en el vocabulario.\")\n",
    "\n",
    "# Mostrar cómo el stemmer transforma 'pozo'\n",
    "palabra_original = 'pozo'\n",
    "palabra_stemmizada = stemmer.stem(palabra_original)\n",
    "print(f\"\\nLa palabra original '{palabra_original}' se stemmiza como '{palabra_stemmizada}'\")\n",
    "\n",
    "# Cálculo de similitud entre dos palabras usando las formas stemmizadas\n",
    "palabra1 = 'asfalt'\n",
    "palabra2 = palabra_stemmizada  # Forma stemmizada de 'pozo'\n",
    "\n",
    "# Verificar que las palabras estén en el vocabulario\n",
    "if palabra1 in vectorizer.vocabulary_ and palabra2 in vectorizer.vocabulary_:\n",
    "    idx1 = vectorizer.vocabulary_.get(palabra1)\n",
    "    idx2 = vectorizer.vocabulary_.get(palabra2)\n",
    "    similitud = cosine_similarity(X.T[idx1], X.T[idx2])[0][0]\n",
    "    print(f\"\\nSimilitud entre '{palabra1}' y '{palabra2}': {similitud:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nUna de las palabras '{palabra1}' o '{palabra2}' no está en el vocabulario.\")\n",
    "\n",
    "# Visualización de embeddings\n",
    "# Lista actualizada de palabras para visualizar (formas stemmizadas)\n",
    "palabras_para_visualizar = ['asfalt', 'poz', 'alquitr', 'fósil', 'ciudad', 'smilodon', 'tigr', 'canin', 'hiel', 'américa', 'argentin', 'sur']\n",
    "\n",
    "# Verificar que las palabras estén en el vocabulario del modelo\n",
    "palabras_para_visualizar = [palabra for palabra in palabras_para_visualizar if palabra in vectorizer.vocabulary_]\n",
    "\n",
    "# Comprobar si hay suficientes palabras para PCA\n",
    "if len(palabras_para_visualizar) > 1:\n",
    "    vectores = X.toarray()[:, [vectorizer.vocabulary_[palabra] for palabra in palabras_para_visualizar]].T\n",
    "\n",
    "    # Verificar que haya más muestras que el número de componentes solicitados (al menos 2)\n",
    "    if vectores.shape[0] > 1 and vectores.shape[1] > 1:\n",
    "        pca = PCA(n_components=2)\n",
    "        vectores_reducidos = pca.fit_transform(vectores)\n",
    "\n",
    "        # Gráfica\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        for i, palabra in enumerate(palabras_para_visualizar):\n",
    "            plt.scatter(vectores_reducidos[i, 0], vectores_reducidos[i, 1])\n",
    "            plt.annotate(palabra, xy=(vectores_reducidos[i, 0] + 0.01, vectores_reducidos[i, 1] + 0.01))\n",
    "\n",
    "        plt.title('Visualización de Embeddings de Palabras')\n",
    "        plt.xlabel('Componente Principal 1')\n",
    "        plt.ylabel('Componente Principal 2')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No hay suficientes características o muestras para aplicar PCA.\")\n",
    "else:\n",
    "    print(\"No hay suficientes palabras en el vocabulario para aplicar PCA.\")\n",
    "\n",
    "\n",
    "# Análisis de relaciones entre palabras (analogías)\n",
    "resultado = cosine_similarity(X.T[idx1], X.T[idx2])\n",
    "print(\"\\nResultado de la similitud entre 'asfalt' y 'pozo':\")\n",
    "print(f\"- Similitud: {resultado[0][0]:.4f}\")\n",
    "\n",
    "# Incorporación de connotaciones y sentimiento\n",
    "# Creación de un léxico de sentimientos ficticio\n",
    "sentiment_lexicon = {\n",
    "    'tigr': {'valencia': 6.0, 'activación': 7.0, 'dominancia': 6.5},\n",
    "    'hiel': {'valencia': 4.5, 'activación': 3.0, 'dominancia': 4.0},\n",
    "    'ciudad': {'valencia': 5.5, 'activación': 5.0, 'dominancia': 5.5},\n",
    "    'fósil': {'valencia': 5.0, 'activación': 4.0, 'dominancia': 5.0},\n",
    "    'smilodon': {'valencia': 6.5, 'activación': 6.5, 'dominancia': 7.0},\n",
    "    'américa': {'valencia': 5.5, 'activación': 5.0, 'dominancia': 6.0},\n",
    "    'argentin': {'valencia': 6.0, 'activación': 5.5, 'dominancia': 6.5},\n",
    "}\n",
    "\n",
    "# Función para obtener connotaciones\n",
    "def obtener_connotaciones(palabra):\n",
    "    return sentiment_lexicon.get(palabra, {'valencia': None, 'activación': None, 'dominancia': None})\n",
    "\n",
    "# Análisis de connotaciones\n",
    "print(\"\\nConnotaciones de las palabras seleccionadas:\")\n",
    "for palabra in palabras_para_visualizar:\n",
    "    connotaciones = obtener_connotaciones(palabra)\n",
    "    print(f\"- '{palabra}': {connotaciones}\")\n",
    "\n",
    "# Aplicación de la hipótesis distribucional\n",
    "# Construcción de la matriz de co-ocurrencia\n",
    "# Creación del vocabulario\n",
    "vocabulario = set()\n",
    "for oracion in palabras_stemmizadas:\n",
    "    for palabra in oracion:\n",
    "        vocabulario.add(palabra)\n",
    "vocabulario = list(vocabulario)\n",
    "\n",
    "# Inicialización de la matriz de co-ocurrencia\n",
    "co_ocurrencia = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Ventana de contexto\n",
    "ventana = 2\n",
    "\n",
    "# Población de la matriz\n",
    "for oracion in palabras_stemmizadas:\n",
    "    for i, palabra in enumerate(oracion):\n",
    "        contexto = oracion[max(i - ventana, 0): i] + oracion[i + 1: i + ventana + 1]\n",
    "        for palabra_contexto in contexto:\n",
    "            co_ocurrencia[palabra][palabra_contexto] += 1\n",
    "\n",
    "# Conversión a matriz numpy\n",
    "matriz_co_ocurrencia = np.zeros((len(vocabulario), len(vocabulario)))\n",
    "for i, palabra in enumerate(vocabulario):\n",
    "    for j, palabra_contexto in enumerate(vocabulario):\n",
    "        matriz_co_ocurrencia[i, j] = co_ocurrencia[palabra][palabra_contexto]\n",
    "\n",
    "# Definición de la función obtener_vector\n",
    "def obtener_vector(palabra):\n",
    "    idx = vocabulario.index(palabra)\n",
    "    return matriz_co_ocurrencia[idx].reshape(1, -1)\n",
    "\n",
    "# Cálculo de similitud entre palabras basado en co-ocurrencia\n",
    "palabra1 = 'tigr'\n",
    "palabra2 = 'smilodon'\n",
    "\n",
    "# Verificar que las palabras estén en el vocabulario\n",
    "if palabra1 in vocabulario and palabra2 in vocabulario:\n",
    "    vector1 = obtener_vector(palabra1)\n",
    "    vector2 = obtener_vector(palabra2)\n",
    "    \n",
    "    similitud = cosine_similarity(vector1, vector2)[0][0]\n",
    "    print(f\"\\nSimilitud basada en co-ocurrencia entre '{palabra1}' y '{palabra2}': {similitud:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nUna de las palabras '{palabra1}' o '{palabra2}' no está en el vocabulario.\")\n",
    "\n",
    "# Modelos de tópicos y campos semánticos\n",
    "lda_model = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Mostrar los tópicos\n",
    "print(\"\\nTópicos identificados por el modelo LDA:\")\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Tópico {idx}: {', '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f0ade-6b46-42fc-aa0f-86d89d5bf515",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "##### **Ejercicio 1: Modificación de Stopwords**\n",
    "\n",
    "Modifica el conjunto de stopwords para incluir palabras adicionales que puedan ser irrelevantes en un contexto específico.\n",
    "   - **Instrucción**: Agrega al conjunto de stopwords palabras que consideres innecesarias para el análisis de texto proporcionado (por ejemplo, nombres de lugares o artículos).\n",
    "   - **Tarea**: Justifica por qué las palabras que añadiste deben ser consideradas como stopwords.\n",
    "\n",
    "##### **Ejercicio 2: Cambiar el algoritmo de Stemming**\n",
    "Compara diferentes algoritmos de stemming y su impacto en el modelo Word2Vec.\n",
    "   - **Instrucción**: Modifica el código para usar el algoritmo PorterStemmer en lugar de SnowballStemmer.\n",
    "   - **Tarea**: Entrena el modelo Word2Vec nuevamente con los resultados de PorterStemmer y compara los resultados de palabras similares para la palabra objetivo \"asfalto\".\n",
    "   - **Pregunta**: ¿Qué diferencias encuentras en los vecinos semánticos al cambiar de stemmer?\n",
    "\n",
    "##### **Ejercicio 3: Entrenar Word2Vec con parámetros diferentes**\n",
    "Explora el impacto de cambiar los hiperparámetros en el entrenamiento de Word2Vec.\n",
    "   - **Instrucción**: Cambia los parámetros `window`, `min_count`, y `vector_size` en el modelo Word2Vec.\n",
    "   - **Tarea**: Experimenta con al menos 3 combinaciones diferentes de parámetros. Mide el impacto en la calidad de las palabras más similares a \"tigre\" y \"hielo\".\n",
    "   - **Pregunta**: ¿Cómo afectan los cambios de parámetros al número y calidad de vecinos semánticos?\n",
    "\n",
    "##### **Ejercicio 4: Añadir nuevas palabras al análisis de similitud**\n",
    "Evalua el impacto de añadir nuevas palabras al texto original.\n",
    "   - **Instrucción**: Añade nuevas oraciones sobre temas relacionados (por ejemplo, \"glaciares\", \"animales prehistóricos\", etc.) al texto original.\n",
    "   - **Tarea**: Entrena nuevamente el modelo Word2Vec e identifica las palabras más similares a \"glaciar\" y \"mamífero\".\n",
    "   - **Pregunta**: ¿Cómo influye la adición de nuevo contenido en las similitudes calculadas?\n",
    "\n",
    "##### **Ejercicio 5: Similitud basada en co-ocurrencia**\n",
    "Aplica la hipótesis distribucional con una ventana de contexto más grande.\n",
    "   - **Instrucción**: Aumenta el tamaño de la ventana de contexto de 2 a 5 en el cálculo de la matriz de co-ocurrencia.\n",
    "   - **Tarea**: Calcula la similitud de co-ocurrencia entre las palabras \"smilodon\" y \"américa\" con la nueva ventana.\n",
    "   - **Pregunta**: ¿Cómo cambian las similitudes entre las palabras al aumentar el tamaño de la ventana de contexto?\n",
    "\n",
    "##### **Ejercicio 6: Análisis de tópicos con LDA**\n",
    "Cambia el número de tópicos en el modelo LDA.\n",
    "   - **Instrucción**: Entrena el modelo LDA con un número diferente de tópicos, por ejemplo, `num_topics=3`.\n",
    "   - **Tarea**: Identifica los nuevos tópicos generados y compara los resultados con los tópicos originales.\n",
    "   - **Pregunta**: ¿Cómo cambia la clasificación de palabras entre los tópicos cuando aumentas o disminuyes el número de temas?\n",
    "\n",
    "##### **Ejercicio 7: Visualización avanzada de embeddings**\n",
    "Amplia la visualización de embeddings con más palabras clave.\n",
    "   - **Instrucción**: Añade más palabras al conjunto de palabras a visualizar (por ejemplo, \"autopista\", \"fósil\", \"animales\").\n",
    "   - **Tarea**: Genera una nueva visualización con estas palabras adicionales.\n",
    "   - **Pregunta**: ¿Qué patrones interesantes observas al agregar más palabras a la visualización?\n",
    "\n",
    "##### **Ejercicio 8: Incorporación de sentimientos reales**\n",
    "Añade un análisis de sentimiento real usando un diccionario de sentimientos.\n",
    "   - **Instrucción**: Utiliza una biblioteca externa como `VADER` para hacer un análisis de sentimiento real en lugar del léxico ficticio de sentimientos.\n",
    "   - **Tarea**: Aplica el análisis de sentimiento al texto original y compara los resultados con el léxico ficticio.\n",
    "   - **Pregunta**: ¿Cómo varían los resultados al usar un análisis de sentimiento basado en datos reales?\n",
    "\n",
    "##### **Ejercicio 9: Analogías avanzadas**\n",
    "Construye analogías semánticas más complejas.\n",
    "   - **Instrucción**: Usa diferentes combinaciones de palabras para crear analogías (por ejemplo, \"smilodon\" es a \"fósil\" como \"pozo\" es a \"ciudad\").\n",
    "   - **Tarea**: Explora varias combinaciones y discute las analogías generadas.\n",
    "   - **Pregunta**: ¿Qué tan efectivas son las analogías semánticas creadas por el modelo Word2Vec?\n",
    "\n",
    "##### **Ejercicio 10: Ajuste de parámetros en LDA**\n",
    "Mejora la coherencia de los temas generados por LDA.\n",
    "   - **Instrucción**: Ajusta el número de `passes` en el entrenamiento del modelo LDA, aumentándolo a 20.\n",
    "   - **Tarea**: Reentrena el modelo y compara la calidad de los tópicos generados con los anteriores.\n",
    "   - **Pregunta**: ¿La mayor cantidad de iteraciones mejora los resultados de los temas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eaceb0-795a-4cd4-86db-ee84e11dff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9babf-7b4a-4a44-b160-eea850407408",
   "metadata": {},
   "source": [
    "### Semántica de vectores \n",
    "\n",
    "La *semántica de vectores* es actualmente la forma estándar de representar el significado de las palabras en el procesamiento del lenguaje natural. Esta técnica nos permite modelar muchos aspectos del significado de las palabras y es fundamental para diversas aplicaciones en el campo de la inteligencia artificial y la lingüística computacional.\n",
    "\n",
    "\n",
    "Las raíces de la semántica de vectores se remontan a la década de 1950, cuando dos grandes ideas convergieron para sentar las bases de esta metodología. Por un lado, Charles Osgood, en 1957, introdujo la idea de representar la connotación de una palabra como un punto en un espacio tridimensional. Esta representación buscaba capturar las dimensiones afectivas del significado, como la valencia (agradabilidad), la activación (intensidad emocional) y la dominancia (grado de control).\n",
    "\n",
    "Por otro lado, lingüistas como Martin Joos (1950), Zellig Harris (1954) y J.R. Firth (1957) propusieron que el significado de una palabra podía definirse por su distribución en el uso del lenguaje, es decir, por las palabras que la rodean en diferentes contextos o sus entornos gramaticales. Esta idea se resume en la famosa cita de Firth: *\"You shall know a word by the company it keeps\"* (\"Conocerás una palabra por la compañía que mantiene\"). La hipótesis distribucional sugiere que dos palabras que ocurren en distribuciones muy similares (cuyas palabras vecinas son similares) tienden a tener significados similares.\n",
    "\n",
    "\n",
    "**Ejemplo práctico: Descubriendo el significado de 'ongchoi'**\n",
    "\n",
    "Para ilustrar cómo funciona esta idea en la práctica, consideremos el ejemplo de la palabra *ongchoi*, un préstamo reciente del cantonés que puede ser desconocido para muchos hablantes. Supongamos que encontramos *ongchoi* en los siguientes contextos:\n",
    "\n",
    "1. *Ongchoi is delicious sautéed with garlic.*  \n",
    "   (*Ongchoi es delicioso salteado con ajo.*)\n",
    "\n",
    "2. *Ongchoi is superb over rice.*  \n",
    "   (*Ongchoi es magnífico sobre arroz.*)\n",
    "\n",
    "3. *...ongchoi leaves with salty sauces...*  \n",
    "   (*...hojas de ongchoi con salsas saladas...*)\n",
    "\n",
    "Aunque no conocemos el significado exacto de *ongchoi*, podemos observar las palabras que la rodean: *delicious*, *sautéed*, *garlic*, *superb*, *over rice*, *leaves*, *salty sauces*. Si además hemos visto estas palabras en contextos con otras palabras conocidas, como:\n",
    "\n",
    "4. *...spinach sautéed with garlic over rice...*  \n",
    "   (*...espinacas salteadas con ajo sobre arroz...*)\n",
    "\n",
    "5. *...chard stems and leaves are delicious...*  \n",
    "   (*...los tallos y hojas de acelga son deliciosos...*)\n",
    "\n",
    "6. *...collard greens and other salty leafy greens*  \n",
    "   (*...col rizada y otras verduras de hoja verde saladas...*)\n",
    "\n",
    "Podemos inferir que *ongchoi* es probablemente una verdura de hoja verde similar a la espinaca, la acelga o la col rizada. Este proceso de deducción se basa en la observación de que *ongchoi* comparte contextos con palabras que conocemos y que pertenecen a una categoría semántica específica.\n",
    "\n",
    "\n",
    "**Representación de palabras en un espacio semántico multidimensional**\n",
    "\n",
    "La semántica de vectores formaliza esta idea al representar cada palabra como un punto en un espacio semántico multidimensional. Este espacio se deriva de las distribuciones de las palabras vecinas, es decir, de cómo y con qué frecuencia una palabra aparece en compañía de otras en grandes corpus de texto.\n",
    "\n",
    "Los vectores utilizados para representar palabras en este espacio se conocen como *embeddings*. Aunque el término *embedding* se aplica a veces más estrictamente a vectores densos como los generados por modelos como *word2vec*, también se utiliza para referirse a vectores dispersos como los obtenidos mediante métodos de *tf-idf* (Term Frequency-Inverse Document Frequency) o *PPMI* (Positive Pointwise Mutual Information). La palabra \"embedding\" proviene de su sentido matemático de una función que mapea un espacio o estructura en otro.\n",
    "\n",
    "**Vectores dispersos y vectores densos**\n",
    "\n",
    "- **Vectores dispersos**: Métodos como *tf-idf* generan vectores muy largos donde la mayoría de las entradas son ceros. Esto se debe a que una palabra individual solo co-ocurre con un subconjunto pequeño del vocabulario total. Aunque estos vectores pueden capturar información útil, su alta dimensionalidad y dispersión los hacen menos eficientes computacionalmente.\n",
    "\n",
    "- **Vectores densos**: Modelos como *word2vec* producen vectores más cortos y densos, donde todas o la mayoría de las dimensiones tienen valores distintos de cero. Estos vectores densos son capaces de capturar relaciones semánticas más profundas entre palabras y son más manejables en términos computacionales.\n",
    "\n",
    "**Visualización de embeddings y análisis de sentimientos**\n",
    "\n",
    "Una aplicación práctica de los embeddings es en el análisis de sentimientos. Al entrenar un modelo de embeddings en un corpus etiquetado con sentimientos positivos y negativos, es posible proyectar las palabras en un espacio vectorial donde las palabras con connotaciones similares se agrupan.\n",
    "\n",
    "Imaginemos una visualización donde hemos reducido un espacio de 60 dimensiones a dos dimensiones para facilitar la interpretación. En esta representación, podríamos observar regiones distintas que contienen:\n",
    "\n",
    "- **Palabras positivas**: *excellent*, *wonderful*, *amazing*.\n",
    "- **Palabras negativas**: *terrible*, *awful*, *horrible*.\n",
    "- **Palabras funcionales neutras**: *the*, *and*, *but*.\n",
    "\n",
    "Esta agrupación refleja cómo las palabras con significados similares se sitúan cerca unas de otras en el espacio vectorial, mientras que las palabras con significados opuestos o no relacionados están más alejadas.\n",
    "\n",
    "\n",
    "**Ventajas de la semántica de vectores en aplicaciones de NLP**\n",
    "\n",
    "El modelo de similitud de palabras de grano fino que ofrece la semántica de vectores tiene un enorme poder en aplicaciones de NLP. Por ejemplo:\n",
    "\n",
    "- **Generalización**: Los clasificadores de sentimientos tradicionales dependen de que las mismas palabras aparezcan en los conjuntos de entrenamiento y prueba. Sin embargo, al utilizar embeddings, el clasificador puede reconocer palabras no vistas anteriormente si están cercanas en el espacio vectorial a palabras conocidas.\n",
    "\n",
    "- **Aprendizaje no supervisado**: Los modelos semánticos vectoriales pueden aprenderse automáticamente del texto sin necesidad de etiquetas manuales, lo que permite aprovechar grandes cantidades de datos no etiquetados.\n",
    "\n",
    "- **Captura de relaciones semánticas**: Los embeddings pueden capturar relaciones complejas entre palabras, como sinonimia, antonimia y analogías.\n",
    "\n",
    "\n",
    "**Modelos comúnmente usados: tf-idf y word2vec**\n",
    "\n",
    "1. **tf-idf (Term Frequency-Inverse Document Frequency)**: Es un método que asigna pesos a las palabras basándose en su frecuencia en un documento y en el corpus completo. El objetivo es resaltar palabras que son importantes en un documento pero que no son comunes en todo el corpus. Aunque útil, este método resulta en vectores muy largos y dispersos.\n",
    "\n",
    "2. **word2vec**: Es una familia de modelos que utiliza redes neuronales para aprender embeddings de palabras. Hay dos arquitecturas principales:\n",
    "\n",
    "   - **CBOW (Continuous Bag-of-Words)**: Predice una palabra basándose en su contexto.\n",
    "   - **Skip-Gram**: Predice el contexto de una palabra dada.\n",
    "\n",
    "Estos modelos generan vectores cortos y densos que capturan propiedades semánticas útiles y son eficientes computacionalmente.\n",
    "\n",
    "\n",
    "**Medición de la Similitud Semántica: El Coseno**\n",
    "\n",
    "Para calcular la similitud semántica entre dos palabras, oraciones o documentos utilizando embeddings, una medida comúnmente utilizada es la *similitud del coseno*. Esta medida calcula el coseno del ángulo entre dos vectores en el espacio vectorial y proporciona un valor entre -1 y 1:\n",
    "\n",
    "$ \\text{Similitud del coseno} = \\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\, ||\\vec{B}||}$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\vec{A} \\cdot \\vec{B}$ es el producto punto de los vectores.\n",
    "- $||\\vec{A}||$ y $||\\vec{B}||$ son las normas (longitudes) de los vectores.\n",
    "\n",
    "Una similitud de coseno cercana a 1 indica que los vectores apuntan en la misma dirección (altamente similares), mientras que una cercana a 0 indica que son ortogonales (no relacionados).\n",
    "\n",
    "**Nota**\n",
    "\n",
    "A medida que el campo del procesamiento del lenguaje natural continúa avanzando, técnicas más sofisticadas como los embeddings contextuales (por ejemplo, *BERT* y *GPT*) están llevando la semántica de vectores a nuevos niveles. Estos modelos tienen en cuenta el contexto completo de una palabra en una oración, lo que permite una comprensión aún más profunda y matizada del lenguaje natural. Esto abre las puertas a aplicaciones más avanzadas y precisas, desde asistentes virtuales más inteligentes hasta sistemas de traducción y resumen que capturan las sutilezas del lenguaje humano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bc12a-84c9-4a42-b813-a48348a2d02e",
   "metadata": {},
   "source": [
    "#### **Ejemplos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c7d00",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Visualización y proyectando las palabras en un espacio bidimensional con colores para cada categoría (palabras positivas, negativas y neutras).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79953bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lista de palabras agrupadas por categoría\n",
    "positive_words = [\"good\", \"very good\", \"nice\", \"fantastic\", \"wonderful\", \"terrific\", \"amazing\", \"incredibly good\"]\n",
    "negative_words = [\"bad\", \"worst\", \"worse\", \"dislike\", \"incredibly bad\", \"not good\"]\n",
    "neutral_words = [\"to\", \"by\", \"is\", \"with\", \"are\", \"that\", \"now\", \"than\", \"you\", \"i\", \"a\", \"the\", \"it's\"]\n",
    "\n",
    "# Coordenadas aproximadas para las palabras en 2D (basadas en la imagen proporcionada)\n",
    "positive_coords = [\n",
    "    (3, -6), (4, -5), (2, -5), (5, -6), (6, -6), (5, -7), (4, -7), (7, -5)\n",
    "]\n",
    "\n",
    "negative_coords = [\n",
    "    (9, 1), (10, 0), (9, 0), (8, 1), (9, 2), (7, 0)\n",
    "]\n",
    "\n",
    "neutral_coords = [\n",
    "    (-8, 3), (-7, 3), (-5, 2), (-6, 1), (-5, 3), (-7, 2), (-8, 2), (-7, 1),\n",
    "    (-6, 2), (-5, 1), (-8, 1), (-6, 3), (-7, 0)\n",
    "]\n",
    "\n",
    "# Crear el gráfico\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Graficar palabras positivas (verde)\n",
    "for word, (x, y) in zip(positive_words, positive_coords):\n",
    "    plt.scatter(x, y, color='green')\n",
    "    plt.text(x + 0.1, y, word, fontsize=12, color='green')\n",
    "\n",
    "# Graficar palabras negativas (rojo)\n",
    "for word, (x, y) in zip(negative_words, negative_coords):\n",
    "    plt.scatter(x, y, color='red')\n",
    "    plt.text(x + 0.1, y, word, fontsize=12, color='red')\n",
    "\n",
    "# Graficar palabras neutrales (azul)\n",
    "for word, (x, y) in zip(neutral_words, neutral_coords):\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x + 0.1, y, word, fontsize=12, color='blue')\n",
    "\n",
    "# Configurar el gráfico\n",
    "plt.grid(True)\n",
    "plt.xlim(-9, 11)\n",
    "plt.ylim(-8, 4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382714b9",
   "metadata": {},
   "source": [
    "Obtenemos los embeddings de BERT para las palabras y luego los proyectamos en un espacio bidimensional para visualización. Finalmente, graficamos las palabras según su categoría (positiva, negativa, neutral) y calculamos la similitud coseno entre algunas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Inicializamos el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Lista de palabras de ejemplo (positivas, negativas, neutrales)\n",
    "words = [\"good\", \"bad\", \"wonderful\", \"amazing\", \"terrific\", \"nice\", \"worse\", \"worst\", \"dislike\", \n",
    "         \"fantastic\", \"incredibly bad\", \"incredibly good\", \"not good\", \"very good\", \"to\", \"is\", \n",
    "         \"now\", \"with\", \"are\", \"by\", \"than\"]\n",
    "\n",
    "# Función para obtener embeddings de una palabra\n",
    "def get_word_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    # Usamos el embedding del token [CLS] para representar la palabra\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "# Obtenemos los embeddings para las palabras\n",
    "embeddings = np.array([get_word_embedding(word) for word in words])\n",
    "\n",
    "# Usamos t-SNE para reducir las dimensiones a 2D para visualización\n",
    "# Se establece perplexity=5 para evitar el error (debe ser menor que 21, el número de palabras)\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Diccionario de colores para cada categoría de palabras\n",
    "colors = {\n",
    "    \"positive\": \"green\",\n",
    "    \"negative\": \"red\",\n",
    "    \"neutral\": \"blue\"\n",
    "}\n",
    "\n",
    "# Definimos las categorías para las palabras\n",
    "word_categories = {\n",
    "    \"good\": \"positive\", \"bad\": \"negative\", \"wonderful\": \"positive\", \"amazing\": \"positive\", \n",
    "    \"terrific\": \"positive\", \"nice\": \"positive\", \"worse\": \"negative\", \"worst\": \"negative\", \n",
    "    \"dislike\": \"negative\", \"fantastic\": \"positive\", \"incredibly bad\": \"negative\", \n",
    "    \"incredibly good\": \"positive\", \"not good\": \"negative\", \"very good\": \"positive\", \n",
    "    \"to\": \"neutral\", \"is\": \"neutral\", \"now\": \"neutral\", \"with\": \"neutral\", \"are\": \"neutral\", \n",
    "    \"by\": \"neutral\", \"than\": \"neutral\"\n",
    "}\n",
    "\n",
    "# Plot de las palabras en el espacio 2D con colores correspondientes\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=colors[word_categories[word]])\n",
    "    plt.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, word, fontsize=12)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Función para calcular la similaridad coseno entre dos palabras\n",
    "def cosine_similarity_word(word1, word2):\n",
    "    emb1 = get_word_embedding(word1)\n",
    "    emb2 = get_word_embedding(word2)\n",
    "    return cosine_similarity([emb1], [emb2])[0][0]\n",
    "\n",
    "# Ejemplo de similaridad coseno entre algunas palabras\n",
    "word1 = \"good\"\n",
    "word2 = \"fantastic\"\n",
    "similarity = cosine_similarity_word(word1, word2)\n",
    "print(f\"Similaridad coseno entre '{word1}' y '{word2}': {similarity:.2f}\")\n",
    "\n",
    "word1 = \"good\"\n",
    "word2 = \"bad\"\n",
    "similarity = cosine_similarity_word(word1, word2)\n",
    "print(f\"Similaridad coseno entre '{word1}' y '{word2}': {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada30e7a-7011-49c0-becf-92cac5f41942",
   "metadata": {},
   "source": [
    "##### **Ejemplo -versión sin usar gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e148626-b84f-46b6-ae0a-d15d39a0eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Definición de los ejemplos proporcionados\n",
    "sentences = [\n",
    "    \"Ongchoi is delicious sauteed with garlic.\",\n",
    "    \"Ongchoi is superb over rice.\",\n",
    "    \"...ongchoi leaves with salty sauces...\",\n",
    "    \"...spinach sauteed with garlic over rice...\",\n",
    "    \"...chard stems and leaves are delicious...\",\n",
    "    \"...collard greens and other salty leafy greens\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Eliminamos signos de puntuación y convertimos a minúsculas\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Tokenizamos\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Eliminamos stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "# Unimos los tokens para utilizar el vectorizador TF-IDF\n",
    "joined_sentences = [' '.join(sentence) for sentence in preprocessed_sentences]\n",
    "\n",
    "# Construcción del modelo TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(joined_sentences)\n",
    "\n",
    "# Obtenemos los nombres de las palabras (vocabulario)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convertimos la matriz TF-IDF a un DataFrame para visualizarla\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vocab)\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# Construcción de la matriz PPMI\n",
    "# Construimos un vocabulario indexado\n",
    "word2index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "# Inicializamos la matriz de co-ocurrencia\n",
    "co_occurrence = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "# Definimos una ventana de contexto (tamaño 2 para este ejemplo)\n",
    "window_size = 2\n",
    "\n",
    "for sentence in preprocessed_sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        word_idx = word2index[word]\n",
    "        # Definimos el contexto (palabras alrededor de la palabra actual)\n",
    "        start = max(0, idx - window_size)\n",
    "        end = min(len(sentence), idx + window_size + 1)\n",
    "        context = sentence[start:idx] + sentence[idx+1:end]\n",
    "        for context_word in context:\n",
    "            context_idx = word2index[context_word]\n",
    "            co_occurrence[word_idx][context_idx] += 1\n",
    "\n",
    "# Calculamos la PMI\n",
    "total_co_occurrences = np.sum(co_occurrence)\n",
    "word_counts = np.sum(co_occurrence, axis=1)\n",
    "ppmi = np.maximum(np.log2((co_occurrence * total_co_occurrences) / (word_counts[:, None] * word_counts[None, :] + 1e-8)), 0)\n",
    "\n",
    "# Convertimos la matriz PPMI a un DataFrame\n",
    "ppmi_df = pd.DataFrame(ppmi, index=vocab, columns=vocab)\n",
    "print(\"\\nMatriz PPMI:\")\n",
    "print(ppmi_df)\n",
    "\n",
    "# En lugar de Word2Vec, usamos TruncatedSVD para reducir la dimensionalidad de las palabras\n",
    "# Obtenemos los vectores de palabras desde la matriz TF-IDF\n",
    "vectors = tfidf_matrix.T.toarray()  # Transponemos para obtener las palabras como filas\n",
    "\n",
    "# Reducimos la dimensionalidad a 2D usando TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "vectors_2d = svd.fit_transform(vectors)\n",
    "\n",
    "# Creamos un DataFrame para facilitar la visualización\n",
    "embedding_df = pd.DataFrame(vectors_2d, index=vocab, columns=['x', 'y'])\n",
    "\n",
    "# Graficamos los embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=embedding_df, x='x', y='y')\n",
    "\n",
    "# Añadimos etiquetas a cada punto\n",
    "for word, pos in embedding_df.iterrows():\n",
    "    plt.annotate(word, pos + 0.02, fontsize=12)\n",
    "plt.title(\"Visualización de Embeddings (TF-IDF + SVD) en 2D\")\n",
    "plt.show()\n",
    "\n",
    "# Cálculo de similitud de coseno usando los embeddings generados por SVD\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "word_pairs = [\n",
    "    ('ongchoi', 'spinach'),\n",
    "    ('ongchoi', 'garlic'),\n",
    "    ('ongchoi', 'delicious'),\n",
    "    ('ongchoi', 'salty'),\n",
    "    ('ongchoi', 'collard'),\n",
    "    ('ongchoi', 'rice')\n",
    "]\n",
    "\n",
    "print(\"\\nSimilitudes de Coseno (TF-IDF + SVD):\")\n",
    "for w1, w2 in word_pairs:\n",
    "    if w1 in vocab and w2 in vocab:\n",
    "        vec1 = vectors_2d[word2index[w1]]\n",
    "        vec2 = vectors_2d[word2index[w2]]\n",
    "        sim = cosine_similarity(vec1, vec2)\n",
    "        print(f\"Sim({w1}, {w2}) = {sim:.4f}\")\n",
    "    else:\n",
    "        print(f\"Una de las palabras '{w1}' o '{w2}' no está en el vocabulario.\")\n",
    "\n",
    "# Aplicación en clasificación de sentimientos (opcional)\n",
    "# Asignamos etiquetas de sentimiento a ciertas palabras\n",
    "sentiment_words = {\n",
    "    'delicious': 'positive',\n",
    "    'superb': 'positive',\n",
    "    'salty': 'negative',\n",
    "    'garlic': 'neutral',\n",
    "    'rice': 'neutral',\n",
    "    'ongchoi': 'unknown'\n",
    "}\n",
    "\n",
    "# Calculamos el centroide de las palabras positivas y negativas\n",
    "positive_vectors = np.array([vectors_2d[word2index[word]] for word, sentiment in sentiment_words.items() if sentiment == 'positive'])\n",
    "negative_vectors = np.array([vectors_2d[word2index[word]] for word, sentiment in sentiment_words.items() if sentiment == 'negative'])\n",
    "\n",
    "positive_centroid = np.mean(positive_vectors, axis=0)\n",
    "negative_centroid = np.mean(negative_vectors, axis=0)\n",
    "\n",
    "# Calculamos la similitud de 'ongchoi' con los centroides de sentimiento\n",
    "sim_positive = cosine_similarity(vectors_2d[word2index['ongchoi']], positive_centroid)\n",
    "sim_negative = cosine_similarity(vectors_2d[word2index['ongchoi']], negative_centroid)\n",
    "\n",
    "print(f\"\\nSimilitud de 'ongchoi' con sentimientos positivos: {sim_positive:.4f}\")\n",
    "print(f\"Similitud de 'ongchoi' con sentimientos negativos: {sim_negative:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9916e-8a8e-4631-b074-983088930936",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "##### **Ejercicio 1: Modificar la ventana de contexto en PPMI**\n",
    "Explora cómo el tamaño de la ventana de contexto afecta la matriz de co-ocurrencia y la matriz PPMI.\n",
    "   - **Instrucción**: Modifica el tamaño de la ventana de contexto de 2 a 4 en el cálculo de la matriz de co-ocurrencia.\n",
    "   - **Tarea**: Genera nuevamente la matriz PPMI y visualiza las diferencias.\n",
    "   - **Pregunta**: ¿Cómo afecta el cambio en la ventana de contexto la matriz de co-ocurrencia y la matriz PPMI? ¿Se observan similitudes diferentes entre las palabras?\n",
    "\n",
    "##### **Ejercicio 2: Añadir nuevas oraciones al modelo**\n",
    "Evalua el impacto de añadir más datos al modelo TF-IDF y Word2Vec.\n",
    "   \n",
    "   - **Instrucción**: Añade al menos 3 oraciones adicionales sobre vegetales o alimentos a la lista de oraciones originales.\n",
    "   - **Tarea**: Preprocesa las nuevas oraciones, genera la matriz TF-IDF y entrena el modelo Word2Vec nuevamente. Visualiza los embeddings y calcula la similitud de \"ongchoi\" con las nuevas palabras.\n",
    "   - **Pregunta**: ¿Cómo cambian los embeddings y las similitudes de coseno cuando añades nuevas oraciones?\n",
    "\n",
    "##### **Ejercicio 3: Exploración de parámetros en Word2Vec**\n",
    "Comprende cómo los parámetros del modelo Word2Vec afectan los resultados.\n",
    "   - **Instrucción**: Experimenta con diferentes valores de `vector_size` y `window` en el entrenamiento de Word2Vec.\n",
    "   - **Tarea**: Cambia el tamaño del vector a 100 y la ventana de contexto a 5. Vuelve a entrenar el modelo y calcula las nuevas similitudes de coseno entre \"ongchoi\" y las demás palabras.\n",
    "   - **Pregunta**: ¿Cómo afectan estos parámetros a la calidad de las similitudes?\n",
    "\n",
    "##### **Ejercicio 4: Aplicar el modelo PPMI en nuevas oraciones**\n",
    "\n",
    "Aplica el concepto de matriz PPMI a un conjunto de oraciones completamente nuevo.\n",
    "   - **Instrucción**: Cambia completamente el conjunto de oraciones, utilizando un tema diferente (por ejemplo, animales o deportes).\n",
    "   - **Tarea**: Preprocesa las nuevas oraciones, calcula la matriz PPMI y genera la visualización de las palabras.\n",
    "   - **Pregunta**: ¿Cómo cambian las relaciones semánticas entre las palabras en un tema diferente?\n",
    "\n",
    "##### **Ejercicio 5: Visualización avanzada con PCA en lugar de TruncatedSVD**\n",
    "Compara diferentes métodos de reducción de dimensionalidad para visualizar embeddings.\n",
    "   - **Instrucción**: Usa PCA en lugar de TruncatedSVD para reducir los vectores de Word2Vec a 2 dimensiones.\n",
    "   - **Tarea**: Implementa PCA, genera una nueva visualización y compara los resultados con los obtenidos usando TruncatedSVD.\n",
    "   - **Pregunta**: ¿Qué diferencias encuentras en la distribución de las palabras en el espacio 2D utilizando PCA vs TruncatedSVD?\n",
    "\n",
    "##### **Ejercicio 6: Análisis de similitud de coseno con nuevas palabras**\n",
    "\n",
    "Evalua cómo cambian las similitudes de coseno al introducir nuevas palabras en el modelo Word2Vec.\n",
    "   - **Instrucción**: Añade nuevas palabras relacionadas con \"ongchoi\" (como \"broccoli\", \"lettuce\", etc.) y reentrena el modelo Word2Vec.\n",
    "   - **Tarea**: Calcula la similitud de coseno entre \"ongchoi\" y las nuevas palabras.\n",
    "   - **Pregunta**: ¿Cómo afecta la adición de nuevas palabras al cálculo de similitud?\n",
    "\n",
    "##### **Ejercicio 7: Análisis de sentimiento avanzado**\n",
    "Evalua el impacto de añadir más palabras al análisis de sentimiento.\n",
    "   \n",
    "   - **Instrucción**: Añade al menos 3 nuevas palabras al diccionario de sentimientos (por ejemplo, \"tasty\", \"bitter\", \"spicy\").\n",
    "   - **Tarea**: Calcula la similitud de \"ongchoi\" con los centroides de palabras positivas y negativas, e interpreta los resultados.\n",
    "   - **Pregunta**: ¿Cómo cambia la relación de \"ongchoi\" con los sentimientos positivos y negativos después de agregar más palabras?\n",
    "\n",
    "##### **Ejercicio 8: Visualización de la matriz PPMI**\n",
    "\n",
    "Crea una visualización de calor para la matriz PPMI.\n",
    "   - **Instrucción**: Usa **seaborn** para generar un **heatmap** de la matriz PPMI.\n",
    "   - **Tarea**: Visualiza la matriz PPMI utilizando un heatmap e interpreta las relaciones entre las palabras basándote en las co-ocurrencias.\n",
    "   - **Pregunta**: ¿Qué patrones interesantes observas en el heatmap de la matriz PPMI?\n",
    "\n",
    "\n",
    "##### **Ejercicio 9: Aplicación de TF-IDF a un corpus grande**\n",
    "\n",
    "Analiza el rendimiento del modelo TF-IDF en un corpus más extenso.\n",
    "   - **Instrucción**: Aplica el modelo TF-IDF a un corpus de noticias o reseñas de productos.\n",
    "   - **Tarea**: Preprocesa el texto, genera la matriz TF-IDF y analiza qué términos tienen el mayor valor de TF-IDF.\n",
    "   - **Pregunta**: ¿Qué términos son más relevantes en este nuevo corpus, según el modelo TF-IDF?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90862e5-c1fa-4baa-b560-63a95cfc1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3d64a-89a5-4e42-a160-f13307304c8e",
   "metadata": {},
   "source": [
    "### Palabras y vectores\n",
    "\n",
    "Los modelos vectoriales o distribucionales de significado son fundamentales en el procesamiento del lenguaje natural y la recuperación de información. Estos modelos se basan en la **matriz de coocurrencia**, que es una representación cuantitativa de la frecuencia con la que las palabras coocurren en un corpus de texto. En este contexto, dos matrices destacan por su relevancia: la **matriz término-documento** y la **matriz término-término**. Ambas permiten representar palabras y documentos en espacios vectoriales, facilitando así el análisis y comparación de su contenido semántico.\n",
    "\n",
    "#### Vectores y documentos\n",
    "\n",
    "La **matriz término-documento** es una estructura donde cada fila representa una palabra del vocabulario y cada columna representa un documento dentro de una colección. Cada celda de la matriz indica el número de veces que una palabra específica aparece en un documento particular. Por ejemplo, en una matriz que incluye cuatro obras de Shakespeare (*As You Like It*, *Twelfth Night*, *Julius Caesar* y *Henry V*), podríamos observar que la palabra *fool* aparece 58 veces en *Twelfth Night*.\n",
    "\n",
    "**Ejemplo de matriz término-documento:**\n",
    "\n",
    "```plaintext\n",
    "Palabra    | As You Like It | Twelfth Night | Julius Caesar | Henry V\n",
    "-----------|----------------|---------------|---------------|---------\n",
    "battle     |       1        |       0       |       7       |    13\n",
    "good       |     114        |      80       |      62       |    89\n",
    "fool       |      36        |      58       |       1       |     4\n",
    "wit        |      20        |      15       |       2       |     3\n",
    "```\n",
    "\n",
    "Esta matriz fue introducida por primera vez en el **modelo de espacio vectorial** de recuperación de información propuesto por Salton en 1971. En este modelo, cada documento se representa como un vector de conteos (una columna de la matriz), lo que permite comparar documentos basándose en la similitud de sus vectores.\n",
    "\n",
    "Un **vector** en álgebra lineal es esencialmente una lista ordenada de números que pueden representar magnitudes en diferentes dimensiones. En nuestro ejemplo, el documento *As You Like It* se representa por el vector `[1, 114, 36, 20]`, y *Julius Caesar* por `[7, 62, 1, 2]`. Cada componente del vector corresponde al conteo de una palabra específica en el documento.\n",
    "\n",
    "Un **espacio vectorial** es un conjunto de vectores que pueden sumarse entre sí y multiplicarse por escalares, cumpliendo ciertas propiedades matemáticas. La **dimensión** del espacio vectorial viene dada por la cantidad de componentes en los vectores; en nuestro ejemplo simplificado, trabajamos con vectores de dimensión 4, pero en aplicaciones reales, la dimensión suele ser igual al tamaño del vocabulario `|V|`, que puede ser del orden de decenas de miles de palabras.\n",
    "\n",
    "La posición de cada número en el vector corresponde a una dimensión específica que representa una palabra. Por ejemplo, la primera dimensión podría representar la frecuencia de *battle* en el documento. Al comparar los vectores de dos documentos, podemos observar similitudes y diferencias en su contenido temático. Documentos con vectores similares tienden a compartir temas o palabras comunes.\n",
    "\n",
    "Visualmente, aunque es difícil representar espacios de alta dimensión, podemos simplificar y considerar solo dos dimensiones para facilitar la comprensión. Si elegimos las dimensiones correspondientes a *battle* y *fool*, podemos representar los documentos en un plano y observar cómo se agrupan según sus contenidos.\n",
    "\n",
    "**Representación en 2D (battle vs. fool):**\n",
    "\n",
    "```plaintext\n",
    "Documento         | battle | fool\n",
    "------------------|--------|-----\n",
    "As You Like It    |   1    | 114\n",
    "Twelfth Night     |   0    |  80\n",
    "Julius Caesar     |   7    |  62\n",
    "Henry V           |  13    |  89\n",
    "```\n",
    "\n",
    "En esta representación, observamos que *As You Like It* y *Twelfth Night* están más cercanos entre sí en el espacio vectorial, lo que indica una mayor similitud en términos de las palabras *battle* y *fool*.\n",
    "\n",
    "#### Recuperación de información\n",
    "\n",
    "La **recuperación de información** (IR) es el proceso de encontrar documentos relevantes en una colección basada en una consulta del usuario. En el contexto del modelo de espacio vectorial, tanto los documentos como las consultas se representan como vectores en un espacio de `|V|` dimensiones. La relevancia de un documento con respecto a una consulta se determina calculando la similitud entre sus vectores correspondientes.\n",
    "\n",
    "Una medida común de similitud es el **coseno del ángulo** entre los dos vectores, conocido como **similitud del coseno**. Este método evalúa la orientación relativa de los vectores, independientemente de su magnitud, lo que es útil cuando la frecuencia absoluta de palabras puede variar significativamente entre documentos.\n",
    "\n",
    "Además, se utilizan técnicas de **ponderación de términos**, como el **tf-idf** (frecuencia de término-inversa de la frecuencia en documentos), para ajustar la importancia de las palabras en el vector. Esta ponderación considera no solo cuántas veces aparece una palabra en un documento, sino también en cuántos documentos de la colección aparece, dando más peso a palabras que son distintivas de ciertos documentos.\n",
    "\n",
    "La eficiencia en el almacenamiento y manipulación de estos vectores es crucial, especialmente porque suelen ser **dispersos** (contienen muchos ceros). Por lo tanto, se emplean estructuras de datos y algoritmos optimizados para matrices dispersas.\n",
    "\n",
    "##### Palabras como vectores: dimensiones de documentos\n",
    "\n",
    "No solo los documentos pueden representarse como vectores; las palabras también pueden ser representadas en un espacio vectorial utilizando las frecuencias con las que aparecen en distintos documentos. En este caso, cada palabra se asocia con un **vector de palabras** (un vector fila en la matriz término-documento), donde cada componente del vector corresponde a un documento de la colección.\n",
    "\n",
    "Por ejemplo, la palabra *fool* se representa por el vector `[36, 58, 1, 4]`, donde cada número indica la frecuencia de *fool* en los documentos *As You Like It*, *Twelfth Night*, *Julius Caesar* y *Henry V*, respectivamente. Al igual que con los documentos, las palabras que tienen vectores similares tienden a tener significados o usos similares, ya que aparecen en contextos similares (en este caso, documentos).\n",
    "\n",
    "**Vectores de palabras:**\n",
    "\n",
    "```plaintext\n",
    "Palabra   | As You Like It | Twelfth Night | Julius Caesar | Henry V\n",
    "----------|----------------|---------------|---------------|---------\n",
    "battle    |       1        |       0       |       7       |    13\n",
    "good      |      114       |      80       |       62      |     89\n",
    "fool      |      36        |      58       |       1       |     4\n",
    "wit       |     20         |      15       |      2        |    3\n",
    "```\n",
    "\n",
    "Al analizar estos vectores, podemos inferir relaciones semánticas entre las palabras. Por ejemplo, *fool* y *wit* pueden estar más relacionados entre sí que con *battle*, dado que comparten patrones de frecuencia similares en los mismos documentos.\n",
    "\n",
    "##### Palabras como vectores: dimensiones de palabras\n",
    "\n",
    "Otra forma de representar palabras en un espacio vectorial es mediante la **matriz término-término**, también conocida como matriz palabra-palabra o matriz término-contexto. En esta matriz, tanto las filas como las columnas representan palabras, y cada celda registra la frecuencia con la que dos palabras coocurren en un cierto contexto dentro del corpus de entrenamiento.\n",
    "\n",
    "El contexto puede definirse de diversas maneras, pero es común utilizar una ventana de palabras alrededor de la palabra objetivo. Por ejemplo, una ventana de $\\pm 4$ palabras incluye las cuatro palabras anteriores y las cuatro siguientes a la palabra objetivo. Esta técnica captura relaciones sintácticas y semánticas más locales entre las palabras.\n",
    "\n",
    "**Ejemplos de palabras en sus ventanas:**\n",
    "\n",
    "1. *is traditionally followed by **cherry** pie, a traditional dessert*\n",
    "2. *often mixed, such as **strawberry** rhubarb pie. Apple pie*\n",
    "3. *computer peripherals and personal **digital** assistants. These devices usually*\n",
    "4. *a computer. This includes **information** available on the internet*\n",
    "\n",
    "A partir de estos ejemplos, podemos construir una matriz de coocurrencia palabra-palabra contando cuántas veces cada palabra aparece en el contexto de otra. Esto nos proporciona una representación más rica y detallada de las relaciones entre palabras.\n",
    "\n",
    "**Matriz de coocurrencia palabra-palabra:**\n",
    "\n",
    "```plaintext\n",
    "Palabra       | pie | sugar | computer | data\n",
    "--------------|-----|-------|----------|------\n",
    "cherry        |442  |  25   |     2    |   8\n",
    "strawberry    | 60  |  19   |     0    |   0\n",
    "digital       |  5  |   4   |   1670   |  1683\n",
    "information   |  5  |  13   |   3325   | 3982\n",
    "```\n",
    "\n",
    "En esta matriz, observamos que *cherry* y *strawberry* tienen patrones de coocurrencia similares, coocurriendo frecuentemente con *pie* y *sugar*. Esto sugiere que estas palabras están relacionadas semánticamente, posiblemente dentro del campo de la gastronomía o frutas. Por otro lado, *digital* e *information* coocurren con *computer* y *data*, indicando una relación en el ámbito tecnológico.\n",
    "\n",
    "La representación de palabras en un espacio vectorial basado en coocurrencias captura la hipótesis distribucional de la lingüística: \"Las palabras que aparecen en contextos similares tienden a tener significados similares\". Este enfoque permite a los modelos de lenguaje aprender relaciones semánticas y sintácticas a partir de datos sin etiquetar.\n",
    "\n",
    "#### Dimensionalidad y representaciones dispersas\n",
    "\n",
    "El tamaño del vocabulario `|V|` influye directamente en la dimensionalidad de los vectores en estas matrices. En aplicaciones prácticas, `|V|` puede oscilar entre 10,000 y 50,000 palabras, utilizando las más frecuentes en el corpus de entrenamiento. Más allá de este punto, mantener palabras adicionales suele ofrecer rendimientos decrecientes en términos de mejora del modelo.\n",
    "\n",
    "Debido a que la mayoría de las combinaciones de palabras no coocurren en el corpus, las matrices término-documento y término-término son **dispersas**; es decir, contienen muchos ceros. Esto tiene implicaciones importantes para el almacenamiento y procesamiento de estos datos. Se utilizan estructuras de datos especializadas y algoritmos eficientes para manejar matrices dispersas, reduciendo el consumo de memoria y acelerando los cálculos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1a172",
   "metadata": {},
   "source": [
    "**Código de la matriz término-documento y la matriz término-término (coocurrencia)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ejemplo 1: matriz término-documento\n",
    "\n",
    "# Definimos la matriz término-documento (conteos de palabras en documentos)\n",
    "# Las palabras son: battle, good, fool, wit\n",
    "# Los documentos son: As You Like It, Twelfth Night, Julius Caesar, Henry V\n",
    "\n",
    "term_document_matrix = np.array([\n",
    "    [1, 0, 7, 13],  # battle\n",
    "    [114, 80, 62, 89],  # good\n",
    "    [36, 58, 1, 4],  # fool\n",
    "    [20, 15, 2, 3]  # wit\n",
    "])\n",
    "\n",
    "# Nombres de los documentos\n",
    "documents = [\"As You Like It\", \"Twelfth Night\", \"Julius Caesar\", \"Henry V\"]\n",
    "\n",
    "# Nombres de las palabras\n",
    "terms = [\"battle\", \"good\", \"fool\", \"wit\"]\n",
    "\n",
    "# Mostramos la matriz término-documento\n",
    "print(\"Matriz Término-Documento:\")\n",
    "print(term_document_matrix)\n",
    "\n",
    "# Calculamos la similaridad coseno entre los documentos\n",
    "similarity_matrix = cosine_similarity(term_document_matrix.T)\n",
    "\n",
    "print(\"\\nMatriz de Similitud Coseno entre Documentos:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Similitud coseno entre dos documentos específicos\n",
    "doc1 = 0  # As You Like It\n",
    "doc2 = 1  # Twelfth Night\n",
    "\n",
    "similarity = cosine_similarity(term_document_matrix[:, doc1].reshape(1, -1),\n",
    "                               term_document_matrix[:, doc2].reshape(1, -1))\n",
    "print(f\"\\nSimilaridad coseno entre '{documents[doc1]}' y '{documents[doc2]}': {similarity[0][0]:.2f}\")\n",
    "\n",
    "# Ejemplo 2: matriz término-término (Coocurrencia)\n",
    "\n",
    "# Definimos un pequeño corpus de ejemplo\n",
    "corpus = [\n",
    "    \"cherry pie is delicious with sugar\",\n",
    "    \"strawberry pie is also very tasty with sugar\",\n",
    "    \"digital information is stored in computer systems\",\n",
    "    \"computers process digital data\"\n",
    "]\n",
    "\n",
    "# Función para generar la matriz de coocurrencia término-término\n",
    "def generate_cooccurrence_matrix(corpus, window_size=2):\n",
    "    cooccurrence = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, word in enumerate(words):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    cooccurrence[word][words[j]] += 1\n",
    "\n",
    "    return cooccurrence\n",
    "\n",
    "# Generamos la matriz de coocurrencia\n",
    "cooccurrence_matrix = generate_cooccurrence_matrix(corpus, window_size=2)\n",
    "\n",
    "# Mostramos la matriz de coocurrencia\n",
    "print(\"\\nMatriz de Coocurrencia Término-Término:\")\n",
    "for word, neighbors in cooccurrence_matrix.items():\n",
    "    print(f\"{word}: {dict(neighbors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654e7c3",
   "metadata": {},
   "source": [
    "**Ejemplo completo con scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11041e8-7a08-43a1-8c39-92dfeb97321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy pandas scikit-learn matplotlib nltk\n",
    "# Importamos las bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Corpus de ejemplo\n",
    "documents = {\n",
    "    'As You Like It': \"All the world's a stage, and all the men and women merely players...\",\n",
    "    'Twelfth Night': \"If music be the food of love, play on...\",\n",
    "    'Julius Caesar': \"Beware the Ides of March...\",\n",
    "    'Henry V': \"Once more unto the breach, dear friends, once more...\"\n",
    "}\n",
    "\n",
    "# Construcción de la matriz Término-Documento\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents.values()) # X matriz dispersa\n",
    "\n",
    "td_matrix = pd.DataFrame(X.toarray(), index=documents.keys(), columns=vectorizer.get_feature_names_out())\n",
    "td_matrix = td_matrix.T  # Transponemos para tener términos como filas y documentos como columnas\n",
    "print(td_matrix)\n",
    "\n",
    "#Representación de documentos como vectores\n",
    "doc_vectors = X.toarray()\n",
    "as_you_like_it_vector = doc_vectors[0]\n",
    "print(as_you_like_it_vector)\n",
    "\n",
    "# Cálculo de similitud entre documentos\n",
    "cosine_similarities = cosine_similarity(X)\n",
    "cosine_sim_df = pd.DataFrame(cosine_similarities, index=documents.keys(), columns=documents.keys())\n",
    "print(cosine_sim_df)\n",
    "\n",
    "# Construccion de la matriz Término-Término\n",
    "def build_cooccurrence_matrix(corpus, window_size=2):\n",
    "    vocab = set()\n",
    "    for text in corpus:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        vocab.update(tokens)\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    cooccurrence_matrix = np.zeros((len(vocab), len(vocab)), dtype=np.int32)\n",
    "    \n",
    "    for text in corpus:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_idx = vocab_index[token]\n",
    "            start = max(i - window_size, 0)\n",
    "            end = min(i + window_size + 1, len(tokens))\n",
    "            context = tokens[start:i] + tokens[i+1:end]\n",
    "            for context_word in context:\n",
    "                context_idx = vocab_index[context_word]\n",
    "                cooccurrence_matrix[token_idx][context_idx] += 1\n",
    "    return pd.DataFrame(cooccurrence_matrix, index=vocab, columns=vocab)\n",
    "\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(documents.values(), window_size=2)\n",
    "print(cooccurrence_matrix)\n",
    "\n",
    "# Representación de palabras como vectores\n",
    "word_vector = cooccurrence_matrix.loc['music'].values\n",
    "print(word_vector)\n",
    "\n",
    "# Ponderación TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_X = tfidf_vectorizer.fit_transform(documents.values())\n",
    "tfidf_matrix = pd.DataFrame(tfidf_X.toarray(), index=documents.keys(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix = tfidf_matrix.T  # Transponemos para tener términos como filas\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Cálculo de similitud con TF-IDF\n",
    "tfidf_cosine_similarities = cosine_similarity(tfidf_X)\n",
    "tfidf_cosine_sim_df = pd.DataFrame(tfidf_cosine_similarities, index=documents.keys(), columns=documents.keys())\n",
    "print(tfidf_cosine_sim_df)\n",
    "\n",
    "# Visualización de vectores en 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Para documentos\n",
    "pca = PCA(n_components=2)\n",
    "doc_vectors_2d = pca.fit_transform(tfidf_X.toarray())\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, doc in enumerate(documents.keys()):\n",
    "    plt.scatter(doc_vectors_2d[i,0], doc_vectors_2d[i,1])\n",
    "    plt.text(doc_vectors_2d[i,0]+0.01, doc_vectors_2d[i,1]+0.01, doc)\n",
    "plt.title('Representación de Documentos en 2D')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c18393-0a6f-4e1e-a1c8-f7680526ece1",
   "metadata": {},
   "source": [
    "Aplicación a corpus más grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7b744-1ede-4774-9177-ba1a93dd5c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "nltk.download('shakespeare')\n",
    "\n",
    "plays = shakespeare.fileids()\n",
    "documents_large = {play: shakespeare.raw(play) for play in plays}\n",
    "\n",
    "# Construcción de la matriz Término-Documento con el corpus completo\n",
    "vectorizer_large = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_large = vectorizer_large.fit_transform(documents_large.values())\n",
    "\n",
    "# Cálculo de similitud\n",
    "cosine_similarities_large = cosine_similarity(X_large)\n",
    "cosine_sim_df_large = pd.DataFrame(cosine_similarities_large, index=documents_large.keys(), columns=documents_large.keys())\n",
    "\n",
    "# Mostrar las similitudes\n",
    "print(cosine_sim_df_large)\n",
    "\n",
    "# Visualización de documentos en 2D\n",
    "pca_large = PCA(n_components=2)\n",
    "doc_vectors_2d_large = pca_large.fit_transform(X_large.toarray())\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "for i, doc in enumerate(documents_large.keys()):\n",
    "    plt.scatter(doc_vectors_2d_large[i,0], doc_vectors_2d_large[i,1])\n",
    "    plt.text(doc_vectors_2d_large[i,0]+0.01, doc_vectors_2d_large[i,1]+0.01, doc)\n",
    "plt.title('Representación de Obras de Shakespeare en 2D')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb196e0e-e437-4b0c-bc01-939e538c4396",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "##### **Ejercicio 1: Modificar la ventana de Co-ocurrencia**\n",
    "   - **Objetivo**: Evaluar cómo cambia la matriz de co-ocurrencia al ajustar la ventana de contexto.\n",
    "   - **Instrucción**: Cambia el tamaño de la ventana de contexto de 2 a 5 en la función `build_cooccurrence_matrix`.\n",
    "   - **Tarea**: Vuelve a generar la matriz de co-ocurrencia y analiza las diferencias en las co-apariciones de palabras.\n",
    "   - **Pregunta**: ¿Cómo afecta el tamaño de la ventana a la matriz de co-ocurrencia?\n",
    "\n",
    "##### **Ejercicio 2: Comparación entre CountVectorizer y TfidfVectorizer**\n",
    "   - **Objetivo**: Entender las diferencias entre CountVectorizer y TfidfVectorizer.\n",
    "   - **Instrucción**: Usa **CountVectorizer** en lugar de **TfidfVectorizer** en la segunda parte del código que analiza el corpus completo de Shakespeare.\n",
    "   - **Tarea**: Compara las similitudes de coseno generadas por CountVectorizer y TfidfVectorizer.\n",
    "   - **Pregunta**: ¿Cómo difieren las similitudes entre documentos cuando se utiliza frecuencia de término en lugar de TF-IDF?\n",
    "\n",
    "##### **Ejercicio 3: Visualización de la matriz de Co-ocurrencia**\n",
    "   - **Objetivo**: Visualizar cómo las palabras se relacionan en el corpus a través de co-ocurrencias.\n",
    "   - **Instrucción**: Usa **seaborn** para generar un **heatmap** de la matriz de co-ocurrencia.\n",
    "   - **Tarea**: Visualiza la matriz de co-ocurrencia y analiza las relaciones entre las palabras.\n",
    "   - **Pregunta**: ¿Qué patrones de co-ocurrencia observas entre las palabras más comunes?\n",
    "\n",
    "##### **Ejercicio 4: Añadir stopwords personalizadas**\n",
    "   - **Objetivo**: Explorar el impacto de personalizar las stopwords.\n",
    "   - **Instrucción**: Añade stopwords personalizadas al `TfidfVectorizer` (palabras que consideres irrelevantes para el análisis).\n",
    "   - **Tarea**: Modifica las stopwords y vuelve a generar la matriz TF-IDF. Analiza cómo cambian las similitudes de coseno.\n",
    "   - **Pregunta**: ¿Cómo cambia la similitud de coseno entre documentos cuando añades tus propias stopwords?\n",
    "\n",
    "##### **Ejercicio 5: Clustering de documentos**\n",
    "   - **Objetivo**: Agrupar documentos en clústeres basados en su similitud.\n",
    "   - **Instrucción**: Usa **KMeans** para agrupar los documentos basados en los vectores TF-IDF generados.\n",
    "   - **Tarea**: Agrupa los documentos en 3 clústeres y analiza qué obras se agrupan juntas.\n",
    "   - **Pregunta**: ¿Qué obras de Shakespeare se agrupan en el mismo clúster? ¿Por qué crees que sucede esto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad024fd4-8e1b-47a9-ae51-eb3de632e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5d486-2dd0-4928-9d27-036c583ffb81",
   "metadata": {},
   "source": [
    "#### Cálculo de similitud entre palabras\n",
    "\n",
    "Con las representaciones vectoriales de palabras, podemos cuantificar la similitud semántica entre ellas calculando medidas de similitud entre sus vectores. Una vez más, la **similitud del coseno** es una elección común, ya que mide la similitud en dirección entre dos vectores, ignorando las diferencias en magnitud.\n",
    "\n",
    "La **ponderación de las celdas** en las matrices puede mejorar la calidad de las medidas de similitud. En lugar de utilizar los conteos brutos de coocurrencia, se pueden aplicar técnicas como **tf-idf** o medidas basadas en la entropía para dar más peso a las coocurrencias que son más informativas o distintivas.\n",
    "\n",
    "\n",
    "Este enfoque se basa en la idea fundamental de que el contexto en el que aparecen las palabras es clave para comprender su significado. Al aprovechar esta información, los modelos vectoriales nos permiten construir representaciones ricas y útiles que son esenciales para el procesamiento del lenguaje natural en la era de los datos masivos.\n",
    "\n",
    "##### **Coseno para medir la similitud**\n",
    "\n",
    "Para medir la similitud entre dos palabras, usamos el **coseno** del ángulo entre sus vectores, una métrica común en PLN (Procesamiento del Lenguaje Natural). El **producto punto** o **producto interno** entre dos vectores \\(\\mathbf{v}\\) y \\(\\mathbf{w}\\) se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Producto punto} = \\mathbf{v} \\cdot \\mathbf{w} = \\sum_{i=1}^{n} v_i w_i\n",
    "$$\n",
    "\n",
    "Este producto es alto cuando ambos vectores tienen valores grandes en las mismas dimensiones, y es 0 cuando son ortogonales. Sin embargo, el producto punto tiene un inconveniente: **favorece vectores largos**, que tienden a tener valores más altos debido a la frecuencia. La longitud del vector se define como:\n",
    "\n",
    "$$\n",
    "|\\mathbf{v}| = \\sqrt{\\sum_{i=1}^{n} v_i^2}\n",
    "$$\n",
    "\n",
    "Para corregir esto, **normalizamos** el producto punto dividiéndolo por las longitudes de los vectores, obteniendo el **coseno** del ángulo entre ellos:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| |\\mathbf{b}|}\n",
    "$$\n",
    "\n",
    "La similitud coseno entre dos vectores \\(\\mathbf{v}\\) y \\(\\mathbf{w}\\) se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{sim}(\\mathbf{v}, \\mathbf{w}) = \\cos(\\theta) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{|\\mathbf{v}| |\\mathbf{w}|}\n",
    "$$\n",
    "\n",
    "El valor del coseno varía entre 1 (vectores en la misma dirección), 0 (vectores ortogonales), y -1 (vectores opuestos). En frecuencias crudas, el coseno va de 0 a 1.\n",
    "\n",
    "##### Ejemplo de cálculo:\n",
    "\n",
    "Dada la siguiente tabla con conteos simples:\n",
    "\n",
    "| Palabra       | pie  | computer | data |\n",
    "|---------------|------|----------|------|\n",
    "| cherry        | 442  | 2        | 8    |\n",
    "| digital       | 5    | 1670     | 1683 |\n",
    "| information   | 5    | 3325     | 3982 |\n",
    "\n",
    "Se concluye que el modelo concluye que *information* está más cerca de *digital* que de *cherry*, lo cual es razonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b1496-e6cb-48b2-a608-cef913c02b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Definimos los vectores de las palabras de la tabla\n",
    "cherry = np.array([442, 2, 8])\n",
    "digital = np.array([5, 1670, 1683])\n",
    "information = np.array([5, 3325, 3982])\n",
    "\n",
    "# Función para calcular la similitud de coseno entre dos vectores\n",
    "def cosine_similarity_vectors(vec1, vec2):\n",
    "    sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "    return sim\n",
    "\n",
    "# Cálculo de la similitud de coseno entre las palabras\n",
    "sim_cherry_information = cosine_similarity_vectors(cherry, information)\n",
    "sim_digital_information = cosine_similarity_vectors(digital, information)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f\"Similitud de coseno entre 'cherry' y 'information': {sim_cherry_information:.4f}\")\n",
    "print(f\"Similitud de coseno entre 'digital' y 'information': {sim_digital_information:.4f}\")\n",
    "\n",
    "# Decisión del modelo\n",
    "if sim_digital_information > sim_cherry_information:\n",
    "    print(\"'information' está más cerca de 'digital' que de 'cherry'.\")\n",
    "else:\n",
    "    print(\"'information' está más cerca de 'cherry' que de 'digital'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fcd16-3ba7-44ca-8e44-2ec69daef118",
   "metadata": {},
   "source": [
    "#### **TF-IDF: Ponderación de términos en el vector**\n",
    "\n",
    "Las matrices de coocurrencia tradicionales representan cada celda mediante frecuencias, ya sea de palabras con documentos o de palabras con otras palabras. Sin embargo, la **frecuencia cruda** no es la mejor medida para capturar la asociación entre palabras. Esto se debe a que:\n",
    "\n",
    "- **Sesgo**: Las frecuencias crudas están sesgadas hacia palabras muy comunes.\n",
    "- **Falta de discriminación**: No distinguen bien entre palabras informativas y no informativas.\n",
    "\n",
    "Por ejemplo, si queremos identificar contextos compartidos por *cherry* y *strawberry* pero no por *digital* e *information*, las palabras comunes como *the*, *it* o *they* no nos ayudarán, ya que aparecen frecuentemente con todo tipo de palabras y no son informativas sobre ninguna en particular.\n",
    "\n",
    "Es paradójico que:\n",
    "\n",
    "- Las palabras que ocurren frecuentemente cerca (por ejemplo, *pie* cerca de *cherry*) son más importantes.\n",
    "- Las palabras demasiado frecuentes y ubicuas (como *the* o *good*) son poco importantes.\n",
    "\n",
    "**¿Cómo equilibramos estas restricciones conflictivas?**\n",
    "\n",
    "Hay dos soluciones comunes:\n",
    "\n",
    "1. **Ponderación tf-idf**: Usada cuando las dimensiones son documentos.\n",
    "2. **PPMI (Positive Pointwise Mutual Information)**: Usada cuando las dimensiones son palabras.\n",
    "\n",
    "#### **1. Frecuencia del término (tf)**\n",
    "\n",
    "Es la frecuencia de la palabra *t* en el documento *d*. Se puede definir como:\n",
    "\n",
    "$$\n",
    "\\text{tf}_{t,d} = \\text{count}(t, d)\n",
    "$$\n",
    "\n",
    "Sin embargo, es común utilizar una **ponderación logarítmica** para reducir el impacto de frecuencias muy altas:\n",
    "\n",
    "$$\n",
    "\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "0 & \\text{si } \\text{count}(t, d) = 0 \\\\\n",
    "1 + \\log_{10}(\\text{count}(t, d)) & \\text{si } \\text{count}(t, d) > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Intuición**: Una palabra que aparece 100 veces en un documento no es 100 veces más relevante que una que aparece una vez.\n",
    "\n",
    "#### **2. Frecuencia Inversa de Documento (idf)**\n",
    "\n",
    "Se utiliza para dar mayor peso a palabras que ocurren en pocos documentos:\n",
    "\n",
    "$$\n",
    "\\text{idf}_{t} = \\log\\left(\\frac{N}{\\text{df}_{t}}\\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $N$ es el número total de documentos.\n",
    "- $\\text{df}_{t}$ es el número de documentos donde aparece el término $t$.\n",
    "\n",
    "**Intuición**: Palabras que son raras en la colección pero frecuentes en un documento específico son más informativas sobre ese documento.\n",
    "\n",
    "#### **Ponderación tf-idf**\n",
    "\n",
    "El valor tf-idf para la palabra $t$ en el documento $d$ es:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_{t}\n",
    "$$\n",
    "\n",
    "Esto equilibra la frecuencia de término con su rareza en el conjunto de documentos.\n",
    "\n",
    "\n",
    "**Nota:** En aplicaciones prácticas, es común realizar preprocesamientos adicionales como:\n",
    "\n",
    "- **Eliminación de palabras vacías** (stopwords).\n",
    "- **Stemming** o **lematización** para reducir las palabras a su forma raíz.\n",
    "- **Normalización** de los vectores tf-idf.\n",
    "\n",
    "Estas técnicas pueden mejorar aún más la calidad de los resultados en el procesamiento del lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f0526-f75c-400f-b28d-a3a8f5f4c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Definimos un conjunto de documentos de ejemplo\n",
    "documentos = {\n",
    "    'Doc1': 'Este es un documento de ejemplo con palabras repetidas palabras palabras',\n",
    "    'Doc2': 'Este documento es otro ejemplo con diferentes palabras',\n",
    "    'Doc3': 'Las palabras comunes no son muy informativas en este documento',\n",
    "    'Doc4': 'Un documento adicional para el cálculo de tf-idf',\n",
    "}\n",
    "\n",
    "# Preprocesamiento básico: tokenización y conteo de frecuencias\n",
    "def preprocessar_documentos(documentos):\n",
    "    vocabulario = set()\n",
    "    terminos_por_documento = {}\n",
    "    for doc_id, texto in documentos.items():\n",
    "        # Convertimos el texto a minúsculas y lo tokenizamos por espacios\n",
    "        terminos = texto.lower().split()\n",
    "        vocabulario.update(terminos)\n",
    "        terminos_por_documento[doc_id] = terminos\n",
    "    return list(vocabulario), terminos_por_documento  # Convertimos el vocabulario a una lista\n",
    "\n",
    "vocabulario, terminos_por_documento = preprocessar_documentos(documentos)\n",
    "\n",
    "# Construcción de la matriz término-documento con conteos crudos\n",
    "def construir_matriz_td(vocabulario, terminos_por_documento):\n",
    "    matriz_td = pd.DataFrame(0, index=vocabulario, columns=terminos_por_documento.keys())\n",
    "    for doc_id, terminos in terminos_por_documento.items():\n",
    "        for termino in terminos:\n",
    "            matriz_td.at[termino, doc_id] += 1\n",
    "    return matriz_td\n",
    "\n",
    "matriz_td = construir_matriz_td(vocabulario, terminos_por_documento)\n",
    "print(\"Matriz Término-Documento con conteos crudos:\")\n",
    "print(matriz_td)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d4378",
   "metadata": {},
   "source": [
    "Otro ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de la tabla proporcionada\n",
    "words = [\"Romeo\", \"salad\", \"Falstaff\", \"forest\", \"battle\", \"wit\", \"fool\", \"good\", \"sweet\"]\n",
    "df = [1, 2, 4, 12, 21, 34, 36, 37, 37]  # Frecuencia de documentos (df)\n",
    "idf = [1.57, 1.27, 0.967, 0.489, 0.246, 0.037, 0.012, 0, 0]  # Valores de idf proporcionados\n",
    "\n",
    "# Supongamos que tenemos un documento con las siguientes frecuencias de término (tf)\n",
    "tf_document = {\n",
    "    \"Romeo\": 3,\n",
    "    \"salad\": 2,\n",
    "    \"Falstaff\": 1,\n",
    "    \"forest\": 4,\n",
    "    \"battle\": 5,\n",
    "    \"wit\": 7,\n",
    "    \"fool\": 8,\n",
    "    \"good\": 2,\n",
    "    \"sweet\": 3\n",
    "}\n",
    "\n",
    "# Función para calcular tf-idf\n",
    "def calculate_tf_idf(tf_document, words, idf):\n",
    "    tf_idf_scores = {}\n",
    "    \n",
    "    for word in words:\n",
    "        tf = tf_document.get(word, 0)  # Obtener tf para la palabra (si no está en el documento, es 0)\n",
    "        idf_value = idf[words.index(word)]  # Obtener idf de la palabra\n",
    "        tf_idf = tf * idf_value  # Calcular tf-idf\n",
    "        tf_idf_scores[word] = tf_idf\n",
    "    \n",
    "    return tf_idf_scores\n",
    "\n",
    "# Calculamos los valores tf-idf para el documento\n",
    "tf_idf_scores = calculate_tf_idf(tf_document, words, idf)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\"Valores TF-IDF para el documento:\")\n",
    "for word, score in tf_idf_scores.items():\n",
    "    print(f\"{word}: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d772d-0d87-46a9-8c16-74f1c2059c39",
   "metadata": {},
   "source": [
    "#### **Información mutua puntual (PMI)**\n",
    "\n",
    "Una función de ponderación alternativa a tf-idf, **PPMI** (Información Mutua Puntual Positiva), se utiliza para matrices término-término, cuando las dimensiones del vector corresponden a palabras en lugar de documentos. PPMI se basa en la intuición de que la mejor manera de ponderar la asociación entre dos palabras es preguntar cuánto más las dos palabras coocurren en nuestro corpus de lo que hubiéramos esperado a priori que aparecieran por casualidad.\n",
    "\n",
    "La **información mutua puntual** es una de las conceptos más importantes en NLP. Es una medida de cuán a menudo dos eventos $x$ e $y$ ocurren, en comparación con lo que esperaríamos si fueran independientes:\n",
    "\n",
    "$$\n",
    "\\text{I}(x, y) = \\log_2\\left(\\frac{P(x, y)}{P(x) P(y)}\\right)\n",
    "$$\n",
    "\n",
    "La información mutua puntual entre una palabra objetivo $w$ y una palabra de contexto $c$  se define entonces como:\n",
    "\n",
    "$$\n",
    "\\text{PMI}(w, c) = \\log_2\\left(\\frac{P(w, c)}{P(w) P(c)}\\right)\n",
    "$$\n",
    "\n",
    "La razón nos da una estimación de cuánto más las dos palabras coocurren de lo que esperamos por casualidad. \n",
    "\n",
    "Los valores de PMI varían desde menos infinito hasta más infinito. Pero los valores de PMI negativos (que implican que las cosas coocurren menos a menudo de lo que esperaríamos por casualidad) tienden a ser poco confiables a menos que nuestros corpus sean enormes. \n",
    "\n",
    "Por esta razón, es más común usar **PMI positiva** (llamada PPMI) que reemplaza todos los valores de PMI negativos con cero:\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = \\max\\left(\\text{PMI}(w, c), 0\\right)\n",
    "$$\n",
    "\n",
    "Más formalmente, asumamos que tenemos una matriz de coocurrencia $F$ con $W$ filas (palabras) y $C$ columnas (contextos), donde $f_{ij}$ da el número de veces que la palabra $w_i$ ocurre con el contexto $c_j$. Esto puede convertirse en una matriz PPMI donde $\\text{PPMI}_{ij}$ da el valor PPMI de la palabra $w_i$ con el contexto $c_j$:\n",
    "\n",
    "Aquí están las ecuaciones en LaTeX basadas en el gráfico:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{f_{ij}}{\\sum_{i=1}^{W}\\sum_{j=1}^{C} f_{ij}}, \\quad p_{i*} = \\frac{\\sum_{j=1}^{C} f_{ij}}{\\sum_{i=1}^{W}\\sum_{j=1}^{C} f_{ij}}, \\quad p_{*j} = \\frac{\\sum_{i=1}^{W} f_{ij}}{\\sum_{i=1}^{W}\\sum_{j=1}^{C} f_{ij}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PPMI_{ij} = \\max \\left( \\log_2 \\frac{p_{ij}}{p_{i*} p_{*j}}, 0 \\right)\n",
    "$$\n",
    "\n",
    "Estas ecuaciones corresponden a las probabilidades conjuntas y marginales, y el cálculo de la **PPMI** (Positive Pointwise Mutual Information).\n",
    "\n",
    "PMI tiene el problema de estar sesgada hacia eventos infrecuentes; las palabras muy raras tienden a tener valores de PMI muy altos. Una forma de reducir este sesgo hacia eventos de baja frecuencia es cambiar ligeramente el cálculo para $P(c)$, utilizando una función diferente $P_{\\alpha(c)}$ que eleva la probabilidad de la palabra de contexto a la potencia de $\\alpha$:\n",
    "\n",
    "\n",
    "$$\n",
    "PPMI_{\\alpha}(w, c) = \\max \\left( \\log_2 \\frac{P(w, c)}{P(w) P_{\\alpha}(c)}, 0 \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\alpha}(c) = \\frac{count(c)^{\\alpha}}{\\sum_{c} count(c)^{\\alpha}}\n",
    "$$\n",
    "\n",
    "donde:  **count(c)** representa el número de veces que el contexto $c$ aparece en el corpus (o conjunto de datos). Es una frecuencia de aparición, es decir, cuántas veces se observa el contexto $c$ en relación a la palabra objetivo o co-ocurrencias en el conjunto de datos.\n",
    "\n",
    "El parámetro  $\\alpha$ se utiliza para suavizar estas frecuencias, reduciendo el impacto de contextos muy frecuentes. Cuando $\\alpha = 1$, no hay suavización, y $P_{\\alpha}(c)$ es simplemente la distribución de frecuencias normalizada de $c$. Cuando $\\alpha$ es menor que 1, las frecuencias altas se reducen más drásticamente, lo que ayuda a mitigar el efecto de contextos muy comunes en el cálculo de probabilidades. \n",
    "\n",
    "Este tipo de suavización es común en tareas de procesamiento de lenguaje natural, donde se busca controlar la influencia de palabras o contextos extremadamente comunes (como preposiciones o artículos) que pueden no ser tan informativos en ciertos análisis.\n",
    "Estas ecuaciones corresponden a una versión suavizada de la **PPMI** utilizando un parámetro $\\alpha$ para ajustar la probabilidad de la co-ocurrencia.\n",
    "\n",
    "Se ha encontrado que un ajuste de $\\alpha = 0.75$ mejoraba el rendimiento de los embeddings en una amplia gama de tareas. Esto funciona porque elevar el conteo a $\\alpha = 0.75$ aumenta la probabilidad asignada a contextos raros, y por lo tanto reduce su PMI $(P^\\alpha(c) > P(c)$ cuando $c$ es raro).\n",
    "\n",
    "Otra posible solución es el **suavizado de Laplace**: antes de calcular PMI, se agrega una pequeña constante $k$ (valores de 0.1-3 son comunes) a cada uno de los conteos, reduciendo (descontando) todos los valores no cero. Cuanto mayor sea $k$, más se descuentan los conteos no cero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27290e8b-e6be-47aa-914b-8b730c6bd62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import pprint\n",
    "\n",
    "# Definición de un corpus de ejemplo como una lista de oraciones\n",
    "corpus = [\n",
    "    \"el gato negro\",\n",
    "    \"el perro marrón\",\n",
    "    \"el gato y el perro\",\n",
    "    \"el perro negro y el gato marrón\",\n",
    "    \"el gato juega con el perro\"\n",
    "]\n",
    "\n",
    "# Tokenización del corpus y construcción del vocabulario\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "vocab = sorted(vocab)  # Ordenamos para mantener consistencia\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "# Inicialización de la matriz de co-ocurrencia vacía\n",
    "cooccurrence_matrix = np.zeros((len(vocab), len(vocab)), dtype=np.float64)\n",
    "\n",
    "# Conteo de co-ocurrencias\n",
    "for sentence in tokenized_corpus:\n",
    "    for word1, word2 in product(sentence, repeat=2):\n",
    "        if word1 != word2:\n",
    "            idx1, idx2 = word_to_id[word1], word_to_id[word2]\n",
    "            cooccurrence_matrix[idx1, idx2] += 1\n",
    "\n",
    "# Cálculo del total de co-ocurrencias\n",
    "total_cooccurrences = np.sum(cooccurrence_matrix)\n",
    "\n",
    "# Cálculo de la probabilidad conjunta p_{ij}\n",
    "p_ij = cooccurrence_matrix / total_cooccurrences\n",
    "\n",
    "# Cálculo de las probabilidades marginales p_{i*} (suma de filas) y p_{*j} (suma de columnas)\n",
    "p_i = np.sum(cooccurrence_matrix, axis=1) / total_cooccurrences\n",
    "p_j = np.sum(cooccurrence_matrix, axis=0) / total_cooccurrences\n",
    "\n",
    "# Evitamos divisiones por cero usando un pequeño valor epsilon\n",
    "epsilon = 1e-10  # Pequeño valor para evitar log(0)\n",
    "\n",
    "# Cálculo de la matriz PMI\n",
    "pmi_matrix = np.log2(np.maximum(p_ij / (p_i[:, None] * p_j[None, :]), epsilon))\n",
    "\n",
    "# Aplicación de PPMI (reemplazamos valores negativos por cero)\n",
    "ppmi_matrix = np.maximum(pmi_matrix, 0)\n",
    "\n",
    "# Definición del valor de alpha para el ajuste\n",
    "alpha = 0.75\n",
    "\n",
    "# Cálculo de count(c)^{alpha}\n",
    "context_counts = np.sum(cooccurrence_matrix, axis=0)\n",
    "context_counts_alpha = context_counts ** alpha\n",
    "\n",
    "# Cálculo de P_alpha(c)\n",
    "p_j_alpha = context_counts_alpha / np.sum(context_counts_alpha)\n",
    "\n",
    "# Recalculación de la matriz PMI ajustada con alpha\n",
    "pmi_alpha_matrix = np.log2(np.maximum(p_ij / (p_i[:, None] * p_j_alpha[None, :]), epsilon))\n",
    "\n",
    "# Aplicación de PPMI con alpha\n",
    "ppmi_alpha_matrix = np.maximum(pmi_alpha_matrix, 0)\n",
    "\n",
    "# Definición del valor de k para el suavizado de Laplace\n",
    "k = 1.0\n",
    "\n",
    "# Aplicación del suavizado de Laplace a la matriz de co-ocurrencia\n",
    "cooccurrence_matrix_laplace = cooccurrence_matrix + k\n",
    "\n",
    "# Recalculación de las probabilidades con el suavizado de Laplace\n",
    "total_cooccurrences_laplace = np.sum(cooccurrence_matrix_laplace)\n",
    "p_ij_laplace = cooccurrence_matrix_laplace / total_cooccurrences_laplace\n",
    "p_i_laplace = np.sum(cooccurrence_matrix_laplace, axis=1) / total_cooccurrences_laplace\n",
    "p_j_laplace = np.sum(cooccurrence_matrix_laplace, axis=0) / total_cooccurrences_laplace\n",
    "\n",
    "# Recalculación de la matriz PMI con suavizado de Laplace\n",
    "pmi_laplace_matrix = np.log2(np.maximum(p_ij_laplace / (p_i_laplace[:, None] * p_j_laplace[None, :]), epsilon))\n",
    "\n",
    "# Aplicación de PPMI con suavizado de Laplace\n",
    "ppmi_laplace_matrix = np.maximum(pmi_laplace_matrix, 0)\n",
    "\n",
    "# Función para imprimir las matrices de forma legible\n",
    "def print_matrix(matrix, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"\".ljust(10), end=\"\")\n",
    "    for word in vocab:\n",
    "        print(f\"{word}\".ljust(10), end=\"\")\n",
    "    print()\n",
    "    for idx, row in enumerate(matrix):\n",
    "        print(f\"{id_to_word[idx]}\".ljust(10), end=\"\")\n",
    "        for value in row:\n",
    "            print(f\"{value:.2f}\".ljust(10), end=\"\")\n",
    "        print()\n",
    "\n",
    "# Impresión de la matriz de PPMI estándar\n",
    "print_matrix(ppmi_matrix, \"Matriz PPMI estándar\")\n",
    "\n",
    "# Impresión de la matriz de PPMI ajustada con alpha\n",
    "print_matrix(ppmi_alpha_matrix, f\"Matriz PPMI con alpha={alpha}\")\n",
    "\n",
    "# Impresión de la matriz de PPMI con suavizado de Laplace\n",
    "print_matrix(ppmi_laplace_matrix, f\"Matriz PPMI con suavizado de Laplace k={k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1578d-ba73-4598-8626-13c2457b9be7",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "##### **Ejercicio 1: Extensión del vocabulario**\n",
    "\n",
    "**a)** En el primer código, se construye una matriz término-documento con conteos simples. Añade un nuevo documento `'Doc5'` al diccionario `documentos` con el texto: `'Este es un documento nuevo con palabras adicionales y contenido único'`. Actualiza el código para incluir este nuevo documento y reconstruye la matriz término-documento.\n",
    "\n",
    "**b)** ¿Cómo afecta la inclusión de este nuevo documento al vocabulario y a la matriz término-documento?\n",
    "\n",
    "##### **Ejercicio 2: Implementación de TF-IDF**\n",
    "\n",
    "**a)** Utilizando la matriz término-documento del primer código, implementa el cálculo del TF-IDF para cada término en cada documento. Recuerda que:\n",
    "\n",
    "- **TF (Term Frequency)**: Es la frecuencia del término en el documento.\n",
    "- **IDF (Inverse Document Frequency)**: Se calcula como $\\text{IDF}(t) = \\log\\left(\\frac{N}{df_t}\\right)$, donde $N$ es el número total de documentos y $df_t$ es el número de documentos que contienen el término $t$.\n",
    "\n",
    "**b)** Construye una nueva matriz término-documento donde cada entrada es el valor TF-IDF correspondiente.\n",
    "\n",
    "##### **Ejercicio 3: Análisis de PMI y PPMI**\n",
    "\n",
    "**a)** En el segundo código, se calcula la matriz de PPMI estándar. Explica con tus propias palabras qué representa el valor en la posición $(i, j)$ de la matriz PPMI.\n",
    "\n",
    "**b)** Selecciona tres pares de palabras del vocabulario y calcula manualmente el valor PMI para esos pares, utilizando las fórmulas proporcionadas en el código. Compara tus resultados con los valores en la matriz PPMI impresa.\n",
    "\n",
    "##### **Ejercicio 4: Ajuste con alpha y suavizado de Laplace**\n",
    "\n",
    "**a)** Modifica el valor de $\\alpha$ en el segundo código a $1.0$ y vuelve a calcular la matriz PPMI ajustada. ¿Qué cambios observas en los valores de la matriz?\n",
    "\n",
    "**b)** Cambia el valor de $k$ en el suavizado de Laplace a $0.5$ y analiza cómo afecta esto a la matriz PPMI con suavizado. ¿Por qué es importante el suavizado en el cálculo de probabilidades?\n",
    "\n",
    "##### **Ejercicio 5: Interpretación de resultados**\n",
    "\n",
    "**a)** Observa las matrices PPMI obtenidas (estándar, con ajuste $\\alpha$ y con suavizado de Laplace). ¿Qué diferencias notas en los valores de co-ocurrencia entre las palabras? ¿Cómo influyen estas técnicas en la representación de las relaciones entre palabras?\n",
    "\n",
    "**b)** Elige una palabra del vocabulario y analiza con qué otras palabras tiene mayor asociación según las matrices PPMI. ¿Coinciden estos resultados con tu intuición basada en el corpus original?\n",
    "\n",
    "##### **Ejercicio 6: Optimización del código**\n",
    "\n",
    "**a)** En el primer código, actualmente se tokeniza el texto simplemente dividiendo por espacios y convirtiendo a minúsculas. Propón mejoras al preprocesamiento, como eliminar signos de puntuación o palabras vacías (stop words).\n",
    "\n",
    "**b)** Implementa estas mejoras y observa cómo cambian el vocabulario y la matriz término-documento.\n",
    "\n",
    "##### **Ejercicio 7: Aplicación de técnicas avanzadas**\n",
    "\n",
    "**a)** Investiga sobre el uso de SVD (Descomposición en Valores Singulares) para reducir la dimensionalidad de la matriz término-documento o de co-ocurrencias. Implementa una reducción de dimensionalidad en alguna de las matrices obtenidas.\n",
    "\n",
    "**b)** Analiza cómo la reducción de dimensionalidad afecta a la representación de las palabras y a la captura de las relaciones semánticas entre ellas.\n",
    "\n",
    "##### **Ejercicio 8: Comparación con modelos de embeddings**\n",
    "\n",
    "**a)** Compara la matriz PPMI obtenida con modelos de embeddings de palabras como Word2Vec o GloVe. ¿En qué se parecen y en qué se diferencian estas representaciones?\n",
    "\n",
    "**b)** Si dispones de las herramientas necesarias, entrena un modelo Word2Vec utilizando el mismo corpus y compara los vectores de palabras obtenidos con los de la matriz PPMI.\n",
    "\n",
    "##### **Ejercicio 9: Extensión del corpus**\n",
    "\n",
    "**a)** Amplía el corpus del segundo código añadiendo nuevas oraciones que introduzcan nuevas palabras y relaciones. Por ejemplo:\n",
    "\n",
    "```python\n",
    "corpus.extend([\n",
    "    \"el perro juega en el parque\",\n",
    "    \"el gato duerme en la casa\",\n",
    "    \"el perro y el gato comen comida\"\n",
    "])\n",
    "```\n",
    "\n",
    "Actualiza el código para incluir estas oraciones y vuelve a calcular las matrices.\n",
    "\n",
    "**b)** ¿Cómo afecta la ampliación del corpus a las matrices de co-ocurrencia y PPMI? Analiza los cambios en las asociaciones entre palabras.\n",
    "\n",
    "##### **Ejercicio 10: Reflexión sobre limitaciones**\n",
    "\n",
    "**a)** Discute las limitaciones de utilizar matrices de co-ocurrencia y PPMI en comparación con modelos más avanzados de procesamiento del lenguaje natural.\n",
    "\n",
    "**b)** ¿Qué problemas pueden surgir al aplicar estas técnicas a corpus muy grandes o con vocabularios extensos? ¿Cómo podríamos mitigarlos?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56036a1d-25a2-45af-bc9e-cb20a52ad54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
