{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a la representación de texto \n",
    "\n",
    "En el procesamiento del lenguaje natural, transformar el texto en una representación numérica es fundamental para que las computadoras puedan procesarlo y analizarlo. La idea central es convertir elementos textuales –como palabras, oraciones o documentos completos– en vectores en un espacio multidimensional. Estos **modelos de espacios vectoriales** permiten cuantificar similitudes y relaciones semánticas entre fragmentos de texto, de modo que entidades semánticamente similares se ubiquen cerca unas de otras en dicho espacio.\n",
    "\n",
    "La representación vectorial del texto se utiliza en múltiples tareas, como la clasificación de documentos, la búsqueda de información, el análisis de sentimientos, la traducción automática y muchas otras aplicaciones de NLP. En esta explicación se abordarán tanto métodos tradicionales (como la codificación one-hot, la bolsa de palabras y TF-IDF) como técnicas más avanzadas (como el uso de subpalabras y representaciones a nivel de oración o documento)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de espacios vectoriales en NLP\n",
    "\n",
    "Los **modelos de espacio vectorial** constituyen la base para representar cualquier tipo de dato textual. La idea clave es que cada unidad de texto (sea una palabra, una frase o un documento) se traduce en un vector numérico. Entre los aspectos más relevantes se encuentran:\n",
    "\n",
    "- **Conversión del texto en vectores:** Se asigna a cada palabra o token una dimensión en un espacio numérico. Por ejemplo, en el esquema de bolsa de palabras, cada dimensión del vector representa la presencia o frecuencia de una palabra en el documento.\n",
    "- **Medición de similitud:** Una vez que los textos están representados como vectores, se pueden utilizar métricas como la similitud coseno o distancias (Euclidiana, Manhattan, etc.) para comparar textos y extraer relaciones semánticas.\n",
    "- **Embeddings de palabras**: Métodos más avanzados como Word2Vec, GloVe y FastText aprenden representaciones vectoriales densas de palabras a partir de grandes corpus de texto. Estos embeddings capturan relaciones semánticas y sintácticas, de modo que palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "- **Modelos basados en transformers**: Aunque no son exclusivamente modelos de espacio vectorial en el sentido clásico, los modelos basados en transformers, como BERT y GPT, utilizan técnicas de representación vectorial para codificar información textual en vectores de características de alta dimensión. Estos vectores capturan contextos complejos y pueden ser utilizados para tareas avanzadas de NLP.\n",
    "\n",
    "- **Uso en tareas diversas:** Estas representaciones permiten, por ejemplo, identificar documentos similares, clasificar textos por temas o realizar búsquedas semánticas.\n",
    "\n",
    "Las representaciones pueden variar en complejidad desde las más simples, como las codificaciones basadas en frecuencias, hasta las más sofisticadas que involucran redes neuronales y transformers (por ejemplo, BERT o GPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoques básicos de vectorización\n",
    "\n",
    "En esta sección se abordan métodos básicos que permiten transformar el texto en vectores numéricos. Se parte de la idea elemental de asignar a cada palabra del vocabulario un identificador único, para posteriormente representar cada documento como un vector de dimensión igual al tamaño del vocabulario.\n",
    "\n",
    "**Preprocesamiento del corpus**\n",
    "\n",
    "Antes de vectorizar, se realiza un preprocesamiento que incluye convertir las cadenas a minúsculas y eliminar signos de puntuación para garantizar uniformidad en el análisis. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "docs_procesados = [doc.lower().replace(\".\", \"\") for doc in documentos]\n",
    "print(docs_procesados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se obtiene una lista de documentos en la que cada texto está en minúsculas y sin puntos. Esto facilita la construcción de un vocabulario sin duplicidades causadas por diferencias de mayúsculas/minúsculas o puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construcción del vocabulario y codificación One-Hot**\n",
    "\n",
    "El primer paso es construir el vocabulario, es decir, asignar a cada palabra única un identificador numérico. Se recorre cada documento y se separa en palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "conteo = 0\n",
    "for doc in docs_procesados:\n",
    "    for palabra in doc.split():\n",
    "        if palabra not in vocab:\n",
    "            conteo += 1\n",
    "            vocab[palabra] = conteo\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta asignación sirve de base para implementar la **codificación one-hot**. La idea es representar cada palabra mediante un vector binario de dimensión igual al tamaño del vocabulario, donde sólo la posición correspondiente a esa palabra es 1 y el resto son 0.\n",
    "\n",
    "Un esqueleto de función para obtener el vector one-hot es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtiene_vector_onehot(cadena):\n",
    "    onehot_codificado = []\n",
    "    for palabra in cadena.split():\n",
    "        # Se crea un vector de ceros con longitud igual al vocabulario.\n",
    "        temp = [0] * len(vocab)\n",
    "        # Aquí se debe completar: identificar la posición de la palabra en el vocabulario y marcarla como 1.\n",
    "        # Por ejemplo, si la palabra 'dog' tiene id 1, se asigna 1 en la primera posición.\n",
    "        # Completar la lógica para transformar cada palabra.\n",
    "    return onehot_codificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En proyectos reales, la implementación de la codificación one-hot se realiza de forma más optimizada usando librerías como scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación one-hot y label encoding con scikit-learn\n",
    "\n",
    "La librería **scikit-learn** ofrece herramientas optimizadas para realizar tanto la codificación one-hot como la codificación de etiquetas (label encoding). Estas técnicas se utilizan para transformar las palabras en representaciones numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label encoding**\n",
    "\n",
    "La **codificación de etiquetas** convierte cada palabra en un valor numérico único que va de `0` a `n-1`, donde `n` es el número de palabras únicas en el corpus. El siguiente fragmento de código muestra cómo hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "S1 = 'dog bites man'\n",
    "S2 = 'man bites dog'\n",
    "S3 = 'dog eats meat'\n",
    "S4 = 'man eats food'\n",
    "\n",
    "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
    "# Se concatenan todas las palabras de cada documento en una única lista\n",
    "valores = data[0] + data[1] + data[2] + data[3]\n",
    "print(\"Los datos: \", valores)\n",
    "\n",
    "# Instanciar y ajustar el LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(valores)\n",
    "# Transformar una lista de palabras usando el encoder\n",
    "etiquetas = le.transform(valores)\n",
    "print(\"Codificación de etiquetas: \", etiquetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot encoding con scikit-learn**\n",
    "\n",
    "Para realizar la codificación one-hot se utiliza el objeto `OneHotEncoder` de scikit-learn, el cual transforma los valores etiquetados en una matriz dispersa (sparse matrix) en la que cada columna representa una de las posibles categorías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "valores = ['rojo', 'verde', 'azul', 'verde', 'rojo']\n",
    "valores = np.array(valores).reshape(-1, 1)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False) \n",
    "ohe.fit(valores)\n",
    "\n",
    "# Transformar los datos\n",
    "datos_onehot = ohe.transform(valores)\n",
    "print(\"Codificación One-Hot:\\n\", datos_onehot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos ejemplos se comprende cómo se asigna a cada palabra una representación numérica, tanto en forma de etiqueta única como en un vector binario one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de palabras (bag of words, BoW)\n",
    "\n",
    "\n",
    "La **bolsa de palabras** (*bag of words*, BoW) es una técnica clásica de representación de texto. La idea clave detrás de esta técnica es representar el texto como una bolsa (o colección) de palabras, **ignorando tanto el orden como el contexto** en el que estas aparecen. \n",
    "La intuición básica es que se asume que un texto perteneciente a una clase determinada dentro de un conjunto de datos está caracterizado por un conjunto único de palabras. Si dos fragmentos de texto contienen casi las mismas palabras, es probable que pertenezcan al mismo grupo (o clase). Así, al analizar las palabras presentes en un texto, es posible identificar la clase (o categoría) a la que pertenece.\n",
    "\n",
    "De manera similar a la codificación **one-hot**, BoW asigna a cada palabra un identificador entero único entre `1` y `|V|` (el tamaño del vocabulario). Luego, cada documento del corpus se convierte en un vector de `|V|` dimensiones, donde el componente `i`, correspondiente a la palabra con ID `w_{id}`, representa simplemente el número de veces que dicha palabra `w` aparece en el documento. Es decir, se califica cada palabra en `V` según su conteo de apariciones en el documento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos \n",
    "\n",
    "##### **Paso 1: Definimos un pequeño corpus**\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"el gato duerme\",\n",
    "    \"el perro duerme\",\n",
    "    \"el gato y el perro juegan\"\n",
    "]\n",
    "```\n",
    "##### **Paso 2: Tokenizamos (convertimos cada frase en una lista de palabras)**\n",
    "\n",
    "```python\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "print(tokenized_corpus)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```python\n",
    "[['el', 'gato', 'duerme'], \n",
    " ['el', 'perro', 'duerme'], \n",
    " ['el', 'gato', 'y', 'el', 'perro', 'juegan']]\n",
    "```\n",
    "\n",
    "##### **Paso 3: Creamos el vocabulario (conjunto de palabras únicas)**\n",
    "\n",
    "```python\n",
    "from itertools import chain\n",
    "\n",
    "vocabulario = sorted(set(chain(*tokenized_corpus)))\n",
    "print(\"Vocabulario:\", vocabulario)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```python\n",
    "Vocabulario: ['duerme', 'el', 'gato', 'juegan', 'perro', 'y']\n",
    "```\n",
    "\n",
    "Asignamos ID a cada palabra:\n",
    "\n",
    "```python\n",
    "word2id = {word: idx for idx, word in enumerate(vocabulario)}\n",
    "print(\"ID de cada palabra:\", word2id)\n",
    "```\n",
    "\n",
    "##### **Paso 4: Representamos cada documento como un vector BoW**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def vector_bow(sentence, word2id):\n",
    "    vec = np.zeros(len(word2id), dtype=int)\n",
    "    for word in sentence.split():\n",
    "        idx = word2id[word]\n",
    "        vec[idx] += 1\n",
    "    return vec\n",
    "\n",
    "for i, sentence in enumerate(corpus):\n",
    "    vector = vector_bow(sentence, word2id)\n",
    "    print(f\"Vector BoW para documento {i+1}: {vector}\")\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```python\n",
    "Vector BoW para documento 1: [1 1 1 0 0 0]\n",
    "Vector BoW para documento 2: [1 1 0 0 1 0]\n",
    "Vector BoW para documento 3: [0 2 1 1 1 1]\n",
    "```\n",
    "\n",
    "\n",
    "##### **Interpretación del primer vector `[1 1 1 0 0 0]`**:\n",
    "Usando el vocabulario ordenado `['duerme', 'el', 'gato', 'juegan', 'perro', 'y']`:\n",
    "\n",
    "- `\"duerme\"`: aparece 1 vez\n",
    "- `\"el\"`: 1 vez\n",
    "- `\"gato\"`: 1 vez\n",
    "- `\"juegan\"`, `\"perro\"`, `\"y\"`: 0 veces\n",
    "\n",
    "\n",
    ">Puedes continuar verificando los resultados dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, realizaremos la tarea de encontrar la representación de una bolsa de palabras. Utilizaremos [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"el gato duerme\",\n",
    "    \"el perro duerme\",\n",
    "    \"el gato y el perro juegan\"\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulario:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matriz BoW:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Lista de documentos\n",
    "docs_procesados = [\"dog bites man\", \"man bites dog\", \"dog and dog are friends\"]\n",
    "print(\"El corpus: \", docs_procesados)\n",
    "\n",
    "# Inicializar el vectorizador\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Construcción de la representación BoW para el corpus\n",
    "bow_rep = count_vect.fit_transform(docs_procesados)  # Aquí se ajusta y transforma\n",
    "\n",
    "# Mapeo del vocabulario\n",
    "print(\"El vocabulario: \", count_vect.vocabulary_)\n",
    "\n",
    "# Ver la representación BoW para los dos primeros documentos\n",
    "print(\"Representación BoW para 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Representación BoW para 'man bites dog': \", bow_rep[1].toarray())\n",
    "\n",
    "# Representación usando este vocabulario para un nuevo texto\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representación BoW para 'dog and dog are friends':\", temp.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explicación**\n",
    "\n",
    "```python\n",
    "['dog bites man', 'man bites dog', 'dog and dog are friends']\n",
    "```\n",
    "\n",
    "Este es tu conjunto de 3 documentos (frases).\n",
    "\n",
    "**Vocabulario generado**\n",
    "\n",
    "```python\n",
    "{'dog': 3, 'bites': 2, 'man': 5, 'and': 0, 'are': 1, 'friends': 4}\n",
    "```\n",
    "\n",
    "El `CountVectorizer` asignó un **ID único** (índice de columna en el vector BoW) a cada palabra que aparece en el corpus completo.\n",
    "\n",
    "| Palabra   | Índice |\n",
    "|-----------|--------|\n",
    "| 'and'     | 0      |\n",
    "| 'are'     | 1      |\n",
    "| 'bites'   | 2      |\n",
    "| 'dog'     | 3      |\n",
    "| 'friends' | 4      |\n",
    "| 'man'     | 5      |\n",
    "\n",
    "Entonces cada documento se representa como un vector de 6 elementos (uno por cada palabra).\n",
    "\n",
    "**Representación BoW para cada documento**\n",
    "\n",
    "**Documento 1:** `'dog bites man'`\n",
    "Vector BoW: `[[0 0 1 1 0 1]]`\n",
    "\n",
    "| Palabra   | Conteo |\n",
    "|-----------|--------|\n",
    "| 'and'     | 0      |\n",
    "| 'are'     | 0      |\n",
    "| 'bites'   | 1      |\n",
    "| 'dog'     | 1      |\n",
    "| 'friends' | 0      |\n",
    "| 'man'     | 1      |\n",
    "\n",
    "El vector indica que las palabras `'dog'`, `'bites'` y `'man'` aparecen una vez en ese documento.\n",
    "\n",
    "**Documento 2:** `'man bites dog'`\n",
    "\n",
    "Vector BoW : `[[0 0 1 1 0 1]]`\n",
    "\n",
    "Es **idéntico al documento 1**, ya que tiene las mismas palabras, solo que en otro orden (que BoW **ignora**).\n",
    "\n",
    "**Documento 3:** `'dog and dog are friends'`\n",
    "\n",
    "Vector BoW: `[[1 1 0 2 1 0]]`\n",
    "\n",
    "| Palabra   | Conteo |\n",
    "|-----------|--------|\n",
    "| 'and'     | 1      |\n",
    "| 'are'     | 1      |\n",
    "| 'bites'   | 0      |\n",
    "| 'dog'     | 2      |\n",
    "| 'friends' | 1      |\n",
    "| 'man'     | 0      |\n",
    "\n",
    "Aquí se ve que `'dog'` aparece **dos veces**, y que `'and'`, `'are'`, y `'friends'` aparecen **una vez**.\n",
    "\n",
    "**Resultados**\n",
    "\n",
    "- Cada vector tiene 6 posiciones, una por palabra en el vocabulario.\n",
    "- La posición `i` representa el número de veces que la palabra con índice `i` aparece en ese documento.\n",
    "- El modelo **ignora el orden** de las palabras y se enfoca solo en **la frecuencia de aparición**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicación a nuevos textos y uso de vectores ninarios\n",
    "\n",
    "Una ventaja del método BoW es que, una vez construido el vocabulario, se pueden transformar nuevos textos utilizando el mismo esquema. Por ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar CountVectorizer para vectores binarios\n",
    "count_vect_bin = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect_bin.fit_transform(docs_procesados)\n",
    "\n",
    "temp_bin = count_vect_bin.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow (binaria) para 'dog and dog are friends':\", temp_bin.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto da como resultado una representación diferente para la misma oración. `CountVectorizer` admite n-gramas tanto de palabras como de caracteres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método BoW descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de n-gramas\n",
    "\n",
    "Los esquemas de representación que hemos visto hasta ahora tratan las palabras como unidades independientes, sin tener en cuenta las frases ni el orden en que aparecen. El enfoque de la **bolsa de n-gramas** (BoN) intenta remediar esta limitación dividiendo el texto en fragmentos de `n` palabras (o *tokens*) contiguas. Esto permite capturar algo de contexto, lo cual no era posible con los enfoques anteriores. Cada uno de estos fragmentos se denomina *n-grama*.\n",
    "\n",
    "El vocabulario del corpus, `V`, se define como la colección de todos los *n-gramas* únicos presentes en el texto. Luego, cada documento del corpus se representa mediante un vector de longitud `|V|`, donde cada componente del vector contiene el recuento de frecuencia de los *n-gramas* presentes en el documento, asignando un valor de cero a los *n-gramas* que no aparecen.\n",
    "\n",
    "En el ámbito del procesamiento del lenguaje natural (*NLP*), este esquema también se conoce como **selección de características basada en n-gramas**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus de ejemplo\n",
    "docs_procesados = ['dog bites man', 'man bites dog', 'dog and dog are friends']\n",
    "print(\"El corpus: \", docs_procesados)\n",
    "\n",
    "# Vectorización con bigramas\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Construcción de la representación BoW para el corpus\n",
    "bow_rep = count_vect.fit_transform(docs_procesados)\n",
    "\n",
    "# Mapeo del vocabulario\n",
    "print(\"El vocabulario: \", count_vect.vocabulary_)\n",
    "\n",
    "# Representación BoW para los dos primeros documentos\n",
    "print(\"Representacion BoW para 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Representacion BoW para 'man bites dog': \", bow_rep[1].toarray())\n",
    "\n",
    "# Representación usando el mismo vocabulario para un nuevo texto\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representación BoW para 'dog and dog are friends':\", temp.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método BoN descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "El método **TF-IDF** es una técnica que pondera la frecuencia de las palabras en un documento en relación a su frecuencia en el corpus completo. Este método ayuda a disminuir el peso de términos muy frecuentes (que no discriminan bien entre documentos) y aumentar el de aquellos que son raros y más informativos.\n",
    "\n",
    "**Cálculo de TF-IDF**\n",
    "\n",
    "- **TF (frecuencia de término):** Mide cuántas veces aparece una palabra en un documento. Se puede normalizar para evitar favorecer documentos más largos.\n",
    "- **IDF (frecuencia inversa de documento):** Calcula la importancia de una palabra a partir de la cantidad de documentos en los que aparece. Se formula tomando el logaritmo de la razón entre el número total de documentos y la cantidad de documentos que contienen la palabra.\n",
    "- **Producto TF-IDF:** Multiplicar TF por IDF da el valor final que indica la relevancia de una palabra en el contexto del documento y el corpus.\n",
    "\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "Donde `t` es el término, `d` es el documento, y el corpus es el conjunto total de documentos.\n",
    "\n",
    "Scikit-learn proporciona el objeto `TfidfVectorizer` para calcular esta representación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instanciar TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(docs_procesados)\n",
    "\n",
    "# Imprimir los valores IDF para cada palabra en el vocabulario\n",
    "print(\"IDF para todas las palabras en el vocabulario:\", tfidf.idf_)\n",
    "print(\"-\" * 10)\n",
    "print(\"Todas las palabras en el vocabulario:\", tfidf.get_feature_names_out())\n",
    "print(\"-\" * 10)\n",
    "\n",
    "# Mostrar la representación TF-IDF para todo el corpus\n",
    "print(\"Representacion TFIDF para todos los documentos en el corpus:\\n\", bow_rep_tfidf.toarray())\n",
    "print(\"-\" * 10)\n",
    "\n",
    "# Transformar un nuevo texto usando el mismo modelo TF-IDF\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Representacion Tfidf para 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explicación de resultados**\n",
    "\n",
    "**Corpus original**\n",
    "\n",
    "```python\n",
    "docs_procesados = [\n",
    "    'dog bites man', \n",
    "    'man bites dog', \n",
    "    'dog and dog are friends'\n",
    "]\n",
    "```\n",
    "\n",
    "**Paso 1: Palabras en el vocabulario**\n",
    "\n",
    "```python\n",
    "['and', 'are', 'bites', 'dog', 'friends', 'man']\n",
    "```\n",
    "\n",
    "Esto es el vocabulario ordenado que el `TfidfVectorizer` extrajo. Cada posición del vector representa una de estas palabras.\n",
    "\n",
    "**Paso 2: Valores IDF**\n",
    "\n",
    "```python\n",
    "[1.6931, 1.6931, 1.2877, 1.0000, 1.6931, 1.2877]\n",
    "```\n",
    "\n",
    "Esto nos dice qué tan **rara** es cada palabra en el corpus:\n",
    "\n",
    "| Palabra   | IDF     | Significado                                      |\n",
    "|-----------|---------|--------------------------------------------------|\n",
    "| 'dog'     | 1.000   | Aparece en **todos** los documentos → menos útil |\n",
    "| 'bites'   | 1.2877  | Aparece en 2/3 documentos                        |\n",
    "| 'man'     | 1.2877  | Aparece en 2/3 documentos                        |\n",
    "| 'and'     | 1.6931  | Solo aparece en 1 documento → más informativa   |\n",
    "| 'are'     | 1.6931  | Solo aparece en 1 documento → más informativa   |\n",
    "| 'friends' | 1.6931  | Solo aparece en 1 documento → más informativa   |\n",
    "\n",
    "Cuanto **mayor el IDF**, más valiosa es la palabra como discriminante.\n",
    "\n",
    "**Paso 3: Representación TF-IDF del corpus**\n",
    "\n",
    "```python\n",
    "[[0.         0.         0.6198  0.4813  0.         0.6198 ]\n",
    " [0.         0.         0.6198  0.4813  0.         0.6198 ]\n",
    " [0.4770     0.4770     0.       0.5634  0.4770     0.      ]]\n",
    "```\n",
    "\n",
    "Cada fila es un documento, y cada columna es una palabra del vocabulario:\n",
    "\n",
    "Documento 1:`'dog bites man'`\n",
    "\n",
    "| Palabra   | TF-IDF |\n",
    "|-----------|--------|\n",
    "| 'dog'     | 0.4813 |\n",
    "| 'bites'   | 0.6198 |\n",
    "| 'man'     | 0.6198 |\n",
    "\n",
    "Las otras palabras no aparecen, por eso su valor es 0.\n",
    "\n",
    "##### Documento 3: `'dog and dog are friends'`\n",
    "\n",
    "| Palabra   | TF-IDF                             |\n",
    "|-----------|-------------------------------------|\n",
    "| `dog`     | 0.5634 (ocurre dos veces, pero es común) |\n",
    "| `and`     | 0.4770                              |\n",
    "| `are`     | 0.4770                              |\n",
    "| `friends` | 0.4770                              |\n",
    "\n",
    "\n",
    "##### Paso 4: TF-IDF para nuevo texto:  \n",
    "`'dog and man are friends'`\n",
    "\n",
    "```python\n",
    "[[0.5046  0.5046  0.      0.2980  0.5046  0.3838]]\n",
    "```\n",
    "\n",
    "| Palabra   | TF-IDF                                 |\n",
    "|-----------|-----------------------------------------|\n",
    "| `dog`     | 0.2980 → común en el corpus             |\n",
    "| `and`     | 0.5046 → rara en el corpus              |\n",
    "| `are`     | 0.5046 → rara en el corpus              |\n",
    "| `friends` | 0.5046 → rara en el corpus              |\n",
    "| `man`     | 0.3838 → aparece moderadamente          |\n",
    "\n",
    "\n",
    "Esto refleja que `'dog'` tiene **menos peso** que `'friends'`, `'are'`, `'and'`, etc., ya que aparece **en casi todos los documentos**, mientras que las otras solo aparecen en uno → son más informativas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización en Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Corpus original\n",
    "docs_procesados = ['dog bites man', 'man bites dog', 'dog and dog are friends']\n",
    "\n",
    "# Instanciar y ajustar el vectorizador\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(docs_procesados)\n",
    "\n",
    "# Obtener las palabras del vocabulario\n",
    "palabras = tfidf.get_feature_names_out()\n",
    "\n",
    "# Crear DataFrame para visualizar el corpus representado como TF-IDF\n",
    "df_tfidf = pd.DataFrame(bow_rep_tfidf.toarray(), columns=palabras, index=[f'Doc_{i+1}' for i in range(len(docs_procesados))])\n",
    "print(\"Representación TF-IDF del corpus:\")\n",
    "print(df_tfidf)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Transformar un nuevo texto\n",
    "nuevo_texto = \"dog and man are friends\"\n",
    "temp = tfidf.transform([nuevo_texto])\n",
    "\n",
    "# Crear DataFrame para el nuevo texto\n",
    "df_nuevo = pd.DataFrame(temp.toarray(), columns=palabras, index=['Nuevo texto'])\n",
    "print(\"Representación TF-IDF para el nuevo texto:\")\n",
    "print(df_nuevo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método TF-IDF descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "1. Dado un pequeño conjunto de documentos (frases), implementa una función en Python que convierta cada palabra única en un vector one-hot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['el gato come pescado', 'el perro come carne', 'el gato juega con el perro']\n",
    "\n",
    "# Implementa la función de one-hot encoding aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Escribe una función en Python que tome como entrada el mismo conjunto de documentos del ejercicio anterior y devuelva una representación de bolsa de palabras de cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Modifica la función de Bolsa de Palabras del ejercicio 2 para que ahora soporte n-gramas. Por simplicidad, considera bigramas (n=2) para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Implementa una función en Python que calcule la matriz TF-IDF para el mismo conjunto de documentos. Puedes usar `TfidfVectorizer` de sklearn para simplificar la implementación, pero intenta entender qué está haciendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bolsa de subpalabras**\n",
    "\n",
    "El concepto de **bolsa de subpalabras** es una extensión del enfoque **bolsa de palabras (BoW)**. En lugar de considerar cada palabra como una unidad independiente, se dividen las palabras en subpalabras o fragmentos más pequeños, como prefijos, sufijos o secuencias intermedias de caracteres. Esto es especialmente útil en lenguas con alta carga morfológica, donde las variaciones morfológicas pueden cambiar el significado o el contexto de una palabra, así como en idiomas con vocabularios extensos o con muchas palabras poco frecuentes.\n",
    "\n",
    "En una **bolsa de subpalabras**, un documento se representa como un vector de frecuencias de subpalabras, en lugar de palabras completas. Este enfoque permite a los modelos manejar mejor las palabras desconocidas (*OOV*, *out-of-vocabulary*) o las variaciones morfológicas, y capturar relaciones entre palabras morfológicamente relacionadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Supongamos que estamos procesando un corpus en inglés con las palabras `\"running\"`, `\"runner\"` y `\"runs\"`. Si utilizamos una **bolsa de subpalabras**, podríamos descomponer estas palabras en fragmentos como:\n",
    "\n",
    "- `\"run\"`, `\"ning\"`, `\"er\"`, `\"s\"`\n",
    "\n",
    "Así, el modelo puede reconocer que estas palabras están relacionadas, aunque no aparezcan de forma idéntica en el texto. En lugar de trabajar únicamente con palabras completas, se tiene en cuenta la estructura interna de las palabras. Por ejemplo, la palabra `\"running\"` podría descomponerse en los subwords `[\"run\", \"ning\"]`.\n",
    "\n",
    "Esto resulta especialmente útil en lenguas como el alemán o el finlandés, que presentan palabras compuestas largas y complejas, o en aplicaciones con vocabularios dinámicos como la generación de lenguaje o la traducción automática.\n",
    "\n",
    "Un caso práctico es el uso de técnicas como **Byte-Pair Encoding (BPE)** o el **Unigram Language Model**, implementadas en herramientas como **SentencePiece**, que realizan esta segmentación en subpalabras. En modelos como **GPT** y **BERT**, las palabras se descomponen en subpalabras utilizando BPE para reducir el tamaño del vocabulario y manejar palabras desconocidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Ejemplo de corpus (normalmente, esto sería un conjunto de textos más grande)\n",
    "corpus = [\"running\", \"runner\", \"runs\", \"jumping\", \"jumper\", \"jumps\"]\n",
    "\n",
    "# Guardamos el corpus en un archivo temporal\n",
    "with open('corpus.txt', 'w') as f:\n",
    "    for word in corpus:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "# Entrenar un modelo BPE con SentencePiece\n",
    "spm.SentencePieceTrainer.train('--input=corpus.txt --model_prefix=mymodel --vocab_size=30 --model_type=bpe')\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "sp = spm.SentencePieceProcessor(model_file='mymodel.model')\n",
    "\n",
    "# Probar la tokenización de subpalabras en el corpus\n",
    "test_words = [\"running\", \"runner\", \"runs\"]\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"Word: {word}\")\n",
    "    subwords = sp.encode(word, out_type=str)\n",
    "    print(f\"Subwords: {subwords}\")\n",
    "    print()\n",
    "\n",
    "# Ejemplo con una palabra nueva\n",
    "new_word = \"jumped\"\n",
    "print(f\"Word: {new_word}\")\n",
    "subwords_new = sp.encode(new_word, out_type=str)\n",
    "print(f\"Subpalabras: {subwords_new}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explicación de los resultados**\n",
    "\n",
    "**¿Qué hace SentencePiece con BPE?**\n",
    "\n",
    "- **BPE (Byte Pair Encoding)** aprende un vocabulario de subpalabras (fragmentos frecuentes de texto) a partir de un corpus.\n",
    "- Cada palabra se divide en **subpalabras** (tokens) según esas unidades aprendidas.\n",
    "- SentencePiece **no depende de espacios**: trata el texto como una secuencia de caracteres, lo que le permite ser más flexible.\n",
    "\n",
    "**Corpus de entrenamiento**\n",
    "\n",
    "```python\n",
    "corpus = [\"running\", \"runner\", \"runs\", \"jumping\", \"jumper\", \"jumps\"]\n",
    "```\n",
    "\n",
    "Este conjunto contiene verbos con raíces similares: `\"run\"`, `\"jump\"`, y sus variaciones.\n",
    "\n",
    "**Resultado del modelo entrenado**\n",
    "\n",
    "- Palabra: **\"running\"**\n",
    "```python\n",
    "Subwords: ['▁runn', 'ing']\n",
    "```\n",
    "\n",
    "¿Qué significa?\n",
    "\n",
    "- `'▁'` indica el **inicio de una nueva palabra** (parecido a un separador).\n",
    "- SentencePiece ha aprendido que `\"runn\"` y `\"ing\"` son fragmentos frecuentes, así que los combina.\n",
    "- Esto sugiere que `\"runn\"` es una raíz útil.\n",
    "\n",
    "- Palabra: **\"runner\"**\n",
    "```python\n",
    "Subwords: ['▁runner']\n",
    "```\n",
    "\n",
    "¿Qué significa?*\n",
    "- `\"runner\"` es probablemente una **unidad frecuente en el corpus**, así que no se divide.\n",
    "- El modelo aprendió que esta palabra se puede manejar como un solo token.\n",
    "\n",
    "- Palabra: **\"runs\"**\n",
    "```python\n",
    "Subwords: ['▁runs']\n",
    "```\n",
    "\n",
    "¿Qué significa?\n",
    "\n",
    "- Similar a `\"runner\"`, `\"runs\"` es una palabra vista frecuentemente y no requiere segmentación.\n",
    "\n",
    "\n",
    "- Palabra nueva: **\"jumped\"**\n",
    "```python\n",
    "Subpalabras: ['▁jump', 'e', 'd']\n",
    "```\n",
    "\n",
    "¿Qué significa?\n",
    "\n",
    "- `\"jumped\"` **no estaba** en el corpus, así que el modelo intenta dividirla usando subpalabras conocidas.\n",
    "- `\"jump\"` fue aprendida como raíz (vista en `\"jumping\"`, `\"jumper\"`, `\"jumps\"`).\n",
    "- `\"e\"` y `\"d\"` son fragmentos pequeños que el modelo usa como piezas para completar la palabra.\n",
    "\n",
    "Esto demuestra que BPE permite manejar palabras fuera del vocabulario (OOV) al dividirlas en fragmentos conocidos.\n",
    "\n",
    "\n",
    "##### **En general**\n",
    "\n",
    "- **\"running\" → ['▁runn', 'ing']**: aprendió a dividir en raíz + sufijo.\n",
    "- **\"runner\", \"runs\" → ['▁runner'], ['▁runs']**: no divididas porque son frecuentes.\n",
    "- **\"jumped\" → ['▁jump', 'e', 'd']**: manejada correctamente aunque no estaba en el corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representaciones a nivel de oración o documento\n",
    "\n",
    "Mientras que los métodos anteriores se centran en representar palabras o secuencias cortas, las **representaciones a nivel de oración o documento** buscan capturar el significado global y la estructura completa del texto. Estas técnicas son cruciales para tareas en las que se necesita comprender el contexto extendido, como la clasificación de documentos, la detección de sentimientos o la búsqueda semántica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Doc2Vec: Representación de documentos**\n",
    "\n",
    "**Doc2Vec** es una extensión del modelo Word2Vec que genera vectores representativos no solo para palabras, sino para documentos completos (incluyendo oraciones, párrafos, etc.). Doc2Vec aprende a representar un documento en un espacio vectorial donde los documentos con contenido similar están más cerca entre sí. Este enfoque es útil para tareas como la clasificación de documentos y la detección de similitud entre textos.\n",
    "\n",
    "Para entrenar el modelo, Doc2Vec utiliza dos enfoques:\n",
    "   - **Distributed memory (DM)**: Aprender representaciones vectoriales de documentos, manteniendo el contexto de las palabras presentes.\n",
    "   - **Distributed bag of words (DBOW)**: Similar a Skip-Gram en Word2Vec, aprende representaciones de documentos prediciendo palabras aleatorias del documento.\n",
    "\n",
    "Mediante este enfoque, documentos con contenido similar se ubican en regiones cercanas del espacio vectorial, lo cual es muy útil para la clasificación y la agrupación de textos.\n",
    "\n",
    "**Ejemplo**: En un corpus de noticias, si tienes artículos sobre deportes y política, **Doc2Vec** generará representaciones vectoriales donde los documentos sobre deportes estarán cercanos entre sí en el espacio vectorial, y los artículos sobre política estarán en otro grupo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Universal Sentence Encoder (USE)**:\n",
    "\n",
    "El **Universal Sentence Encoder**, desarrollado por Google, es un modelo basado en redes neuronales profundas (y transformers en su versión más avanzada) que convierte oraciones o documentos en vectores de alta dimensión. Estos vectores pueden ser utilizados para una variedad de tareas como análisis semántico, búsqueda de similitud de oraciones o detección de temas.\n",
    "\n",
    "USE tiene la capacidad para manejar frases o documentos completos, representándolos de una manera que captura las relaciones de largo alcance en un texto. Esto es crucial para tareas como la clasificación de documentos, la traducción automática, y la búsqueda de información, donde la estructura completa del documento es relevante para comprender su significado\n",
    "\n",
    "**Ejemplo**: Supongamos que tenemos dos oraciones: *\"The cat is on the mat\"* y *\"A feline is resting on a rug\"*. Aunque estas oraciones utilizan palabras diferentes, el **universal sentence encoder** generará representaciones vectoriales que estarán cercanas en el espacio vectorial porque capturan el significado semántico similar entre ambas oraciones. Esto hace que USE sea adecuado para tareas como la búsqueda semántica o la detección de parafraseo, donde oraciones con significados similares deben ser reconocidas, aunque usen diferentes palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Cargar el modelo preentrenado de Universal Sentence Encoder desde TensorFlow Hub\n",
    "modelo = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Definir oraciones para generar representaciones\n",
    "sentences = [\n",
    "    \"The cat is on the mat\",\n",
    "    \"A feline is resting on a rug\",\n",
    "    \"The dog barked at the stranger\",\n",
    "    \"The mouse ran away from the cat\"\n",
    "]\n",
    "\n",
    "# Generar las representaciones vectoriales de las oraciones\n",
    "sentence_vectors = modelo(sentences)\n",
    "\n",
    "# Calcular la similitud entre la primera oración y las demás\n",
    "similarities = cosine_similarity([sentence_vectors[0]], sentence_vectors)\n",
    "\n",
    "# Mostrar los resultados de similitud\n",
    "for idx, similarity in enumerate(similarities[0]):\n",
    "    print(f\"Similaridad con la oracion {idx}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explicación de los resultados**\n",
    "\n",
    "**Oraciones comparadas:**\n",
    "\n",
    "```python\n",
    "sentences = [\n",
    "    \"The cat is on the mat\",             # oracion 0\n",
    "    \"A feline is resting on a rug\",      # oracion 1\n",
    "    \"The dog barked at the stranger\",    # oracion 2\n",
    "    \"The mouse ran away from the cat\"    # oracion 3\n",
    "]\n",
    "```\n",
    "\n",
    "**Resultado de similitud (coseno):**\n",
    "\n",
    "```\n",
    "Similaridad con la oracion 0: 1.00\n",
    "Similaridad con la oracion 1: 0.63\n",
    "Similaridad con la oracion 2: 0.27\n",
    "Similaridad con la oracion 3: 0.51\n",
    "```\n",
    "\n",
    "**Interpretación:**\n",
    "\n",
    "| Comparación                         | Similaridad | Explicación |\n",
    "|------------------------------------|-------------|-------------|\n",
    "| Oración 0 vs oración 0             | 1.00        | Igual a sí misma, similitud perfecta. |\n",
    "| Oración 0 vs oración 1             | 0.63        | \"cat\" ≈ \"feline\", \"mat\" ≈ \"rug\" → palabras relacionadas semánticamente. |\n",
    "| Oración 0 vs oración 2             | 0.27        | Poca relación entre \"cat on mat\" y \"dog barked at stranger\". |\n",
    "| Oración 0 vs oración 3             | 0.51        | Aparece \"cat\", y ambos involucran animales y acciones → relación parcial. |\n",
    "\n",
    "El modelo USE captura **relaciones semánticas**, no solo palabras iguales. Por eso:\n",
    "- `\"cat\"` y `\"feline\"` se entienden como similares.\n",
    "- `\"mat\"` y `\"rug\"` también son conceptualmente parecidos.\n",
    "- `\"cat\"` aparece en la oración 3, lo cual explica la similitud moderada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "**Ejercicio 1: Entrenamiento y análisis con Doc2Vec**\n",
    "1. **Entrena un modelo Doc2Vec** con un corpus de documentos de noticias que cubra al menos tres áreas temáticas diferentes (por ejemplo, deportes, política, tecnología). \n",
    "2. **Evalúa el modelo** generando representaciones vectoriales para los documentos y realiza las siguientes tareas:\n",
    "   - Agrupa los documentos en función de su similitud, utilizando medidas de similitud como la **similitud coseno**.\n",
    "   - Visualiza los documentos en un espacio bidimensional usando técnicas de reducción de dimensionalidad como **t-SNE** o **PCA** para observar cómo se agrupan los documentos.\n",
    "3. **Pregunta reflexiva**: ¿Cómo afecta el tamaño del corpus y el número de dimensiones del vector al rendimiento del modelo y la calidad de las agrupaciones?\n",
    "\n",
    "**Ejercicio 2: Comparación semántica con universal sentence encoder (USE)**\n",
    "1. Carga un conjunto de oraciones que describan eventos similares con diferentes palabras (por ejemplo, *\"The cat sat on the mat\"* y *\"A feline rested on a rug\"*).\n",
    "2. Genera las representaciones vectoriales de estas oraciones utilizando el **universal sentence encoder (USE)**.\n",
    "3. **Mide la similitud semántica** entre las oraciones utilizando la similitud coseno y analiza los resultados.\n",
    "   - ¿Qué patrones observas en las similitudes de oraciones con diferentes estructuras pero significados similares?\n",
    "   - ¿Cómo responde el modelo USE a sinónimos y diferentes expresiones gramaticales?\n",
    "\n",
    "**Ejercicio 3: Detección de parafraseo**\n",
    "1. Recopila un conjunto de oraciones que sean parafraseos entre sí y un conjunto de oraciones no relacionadas.\n",
    "2. Utiliza **Universal sentence encoder (USE)** para generar vectores de representación para todas las oraciones.\n",
    "3. Calcula la similitud entre cada par de oraciones y clasifica si son parafraseos o no en función de un umbral de similitud.\n",
    "   - **Pregunta reflexiva**: ¿Qué umbral de similitud es el más adecuado para detectar parafraseos? ¿Cómo cambiarías este umbral según el dominio (por ejemplo, noticias versus redes sociales)?\n",
    "\n",
    "**Ejercicio 4: Clasificación de documentos con Doc2Vec**\n",
    "1. Entrena un modelo **Doc2Vec** con un corpus de reseñas de productos de diferentes categorías (por ejemplo, tecnología, ropa, libros).\n",
    "2. Genera las representaciones vectoriales de las reseñas y usa estos vectores como entradas para un modelo de **clasificación supervisada** (como un clasificador SVM o un perceptrón multicapa) para predecir la categoría de cada reseña.\n",
    "3. **Pregunta reflexiva**: ¿Qué características del modelo Doc2Vec (como el tamaño de ventana o la cantidad de dimensiones) impactan más en el rendimiento del clasificador? ¿Qué observaciones puedes hacer al respecto?\n",
    "\n",
    "**Ejercicio 5: Detección de tópicos con Doc2Vec**\n",
    "1. Usando un corpus extenso (por ejemplo, artículos científicos o publicaciones de blogs), entrena un modelo **Doc2Vec**.\n",
    "2. Agrupa los documentos utilizando técnicas no supervisadas como **k-means** o **DBSCAN** basadas en las representaciones vectoriales de los documentos.\n",
    "3. **Pregunta reflexiva**: ¿Qué tópicos emergen de los documentos agrupados? ¿Los grupos formados por los documentos reflejan de manera precisa las categorías esperadas o aparecen relaciones temáticas nuevas y sorprendentes?\n",
    "\n",
    "**Ejercicio 6: Búsqueda de documentos semánticamente similares**\n",
    "1. Recopila un corpus de documentos cortos (por ejemplo, entradas de blog, descripciones de productos, artículos cortos).\n",
    "2. Utiliza **universal sentence encoder (USE)** para generar embeddings vectoriales de estos documentos.\n",
    "3. Implementa un sistema de búsqueda semántica: dado un documento de consulta, encuentra los documentos más similares en el corpus usando la similitud coseno.\n",
    "   - **Pregunta reflexiva**: ¿Cómo influye la longitud del documento de consulta en la calidad de los documentos recuperados? ¿El modelo es capaz de capturar adecuadamente la semántica de consultas largas versus cortas?\n",
    "\n",
    "**Ejercicio 7: Evaluación de modelos Doc2Vec vs USE**\n",
    "1. Usando un conjunto de datos con documentos y oraciones, genera representaciones utilizando tanto **Doc2Vec** como **USE**.\n",
    "2. Evalúa la similitud entre documentos o la clasificación de textos con ambos modelos y compara el rendimiento en términos de precisión, tiempo de ejecución, y similitud semántica capturada.\n",
    "   - **Pregunta reflexiva**: ¿En qué tareas sobresale cada modelo? ¿Qué ventajas tiene el modelo USE frente a Doc2Vec y viceversa? ¿Cuál es más adecuado para conjuntos de datos pequeños versus grandes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
