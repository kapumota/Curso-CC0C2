{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc5fb96",
   "metadata": {},
   "source": [
    "### Enfoques de tokenización: top-down vs. bottom-up\n",
    "\n",
    "La tokenización puede implementarse mediante diferentes estrategias. Dos de los enfoques principales son:\n",
    "\n",
    "#### Tokenización top-down\n",
    "\n",
    "El enfoque **top-down** consiste en comenzar con una estructura global del texto y descomponerla en partes cada vez más pequeñas hasta obtener los tokens deseados. Este método es muy utilizado en contextos donde se conocen de antemano las reglas o estándares de segmentación. Por ejemplo, en el análisis sintáctico de expresiones matemáticas o en la preparación de datos para algoritmos que se apoyan en estructuras lingüísticas predefinidas.\n",
    "\n",
    "#### Características clave\n",
    "\n",
    "- **Reglas definidas**: Se establecen estándares y patrones (por ejemplo, mediante expresiones regulares) para identificar los tokens.  \n",
    "- **Preservación de elementos**: Se mantiene la puntuación, números, y caracteres especiales (como en \"m.p.h.\", \"$45.55\" o URLs), tratándolos como tokens separados cuando es necesario.\n",
    "- **Expansión de contracciones**: Se pueden expandir contracciones clíticas (por ejemplo, convertir \"doesn’t\" en \"does\" y \"n’t\"), algo común en la tokenización basada en estándares como el Penn Treebank.\n",
    "- **Aplicación en lenguajes con delimitadores claros**: Es especialmente útil en idiomas como el inglés, donde existen reglas ortográficas y gramaticales que facilitan la segmentación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a11098",
   "metadata": {},
   "source": [
    "#### Ejemplo de algoritmo de tokenización top-down\n",
    "\n",
    "**Escenario**: Imagina que estás tokenizando una expresión matemática simple.\n",
    "\n",
    "**Expresión**: `\"3 + 5 * (10 - 2)\"`\n",
    "\n",
    "**Proceso de tokenización (top-down)**:\n",
    "1. **Inicio con la expresión completa**: Se reconoce que toda la cadena es una expresión matemática.\n",
    "2. **Descomposición en partes mayores: números, operadores y paréntesis.**:\n",
    "   - `\"3\"` → Número.\n",
    "   - `\"+\"` → Operador.\n",
    "   - `\"5\"` → Número.\n",
    "   - `\"*\"` → Operador.\n",
    "   - `\"(\"` → Paréntesis de apertura.\n",
    "   - `\"10\"` → Número.\n",
    "   - `\"-\"` → Operador.\n",
    "   - `\"2\"` → Número.\n",
    "   - `\")\"` → Paréntesis de cierre.\n",
    "3. **Descomposición de cada parte en tokens**:\n",
    "   - `\"3\"` → Token de número.\n",
    "   - `\"+\"` → Token de operador.\n",
    "   - `\"5\"` → Token de número.\n",
    "   - `\"*\"` → Token de operador.\n",
    "   - `\"(\"` → Token de paréntesis de apertura.\n",
    "   - `\"10\"` → Token de número.\n",
    "   - `\"-\"` → Token de operador.\n",
    "   - `\"2\"` → Token de número.\n",
    "   - `\")\"` → Token de paréntesis de cierre.\n",
    "\n",
    "- El resultado es la lista: `[3, +, 5, *, (, 10, -, 2, )]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fc1d0-a796-41bd-a271-14dcf1b9fe33",
   "metadata": {},
   "source": [
    "#### Ejemplo práctico en texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708cc84",
   "metadata": {},
   "source": [
    "Supongamos que tenemos la siguiente oración en inglés:\n",
    "\n",
    "**Texto:** `\"The quick brown fox jumps over the lazy dog.\"`\n",
    "\n",
    "\n",
    "**Paso 1: Identificar frases o cláusulas**\n",
    "\n",
    "En un enfoque *top-down*, podríamos primero descomponer el texto en frases o cláusulas más grandes. Sin embargo, dado que esta oración no tiene múltiples cláusulas separadas por comas o conjunciones, consideraremos la oración completa como una unidad.\n",
    "\n",
    "**Resultado del paso 1:**\n",
    "\n",
    "```plaintext\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "**Paso 2: Descomponer en palabras**\n",
    "\n",
    "El siguiente paso es descomponer la oración en palabras. En un enfoque *top-down*, nos movemos desde la estructura más grande (la oración) hacia componentes más pequeños (las palabras).\n",
    "\n",
    "**Resultado del paso 2:**\n",
    "\n",
    "```plaintext\n",
    "[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "```\n",
    "\n",
    "**Paso 3: Descomponer palabras en prefijos, raíces y sufijos**\n",
    "\n",
    "Podríamos seguir descomponiendo cada palabra en prefijos, raíces y sufijos, si es necesario. Esto es especialmente útil en lenguajes con morfología compleja o en tareas de análisis morfológico.\n",
    "\n",
    "##### Ejemplo de descomposición de palabras:\n",
    "\n",
    "- `\"jumps\"` → `[\"jump\", \"s\"]` (raíz y sufijo)\n",
    "- `\"quick\"` → `[\"quick\"]` (palabra base)\n",
    "- `\"lazy\"` → `[\"lazy\"]` (palabra base)\n",
    "\n",
    "**Resultado del paso 3:**\n",
    "\n",
    "```plaintext\n",
    "[\"The\", \"quick\", \"brown\", \"fox\", \"jump\", \"s\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "```\n",
    "\n",
    "**Descomponer en caracteres (opcional)**\n",
    "\n",
    "Si quisiéramos ir aún más lejos, podríamos descomponer cada palabra en caracteres individuales.\n",
    "\n",
    "##### Ejemplo de descomposición en caracteres:\n",
    "\n",
    "- `\"jump\"` → `[\"j\", \"u\", \"m\", \"p\"]`\n",
    "- `\"s\"` → `[\"s\"]`\n",
    "- `\"quick\"` → `[\"q\", \"u\", \"i\", \"c\", \"k\"]`\n",
    "\n",
    "**Resultado del paso 4:**\n",
    "\n",
    "```plaintext\n",
    "[\"T\", \"h\", \"e\", \"q\", \"u\", \"i\", \"c\", \"k\", \"b\", \"r\", \"o\", \"w\", \"n\", \"f\", \"o\", \"x\", \"j\", \"u\", \"m\", \"p\", \"s\", \"o\", \"v\", \"e\", \"r\", \"t\", \"h\", \"e\", \"l\", \"a\", \"z\", \"y\", \"d\", \"o\", \"g\"]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728911b3-f492-42f9-871a-2151bb559e6c",
   "metadata": {},
   "source": [
    "#### Código de tokenización Top-Down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_top_down(text):\n",
    "    # Paso 1: Tokenización en palabras (top-level)\n",
    "    words = tokenize_sentence(text)\n",
    "    \n",
    "    # Paso 2: Descomposición de palabras en sub-palabras o caracteres\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        tokens.extend(tokenize_word(word))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Utilizamos una expresión regular simple para dividir la oración en palabras y puntuación\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', sentence, re.UNICODE)\n",
    "\n",
    "def tokenize_word(word):\n",
    "    # Aquí podríamos implementar un tokenizador más sofisticado, pero por simplicidad\n",
    "    # lo dividimos en caracteres individuales\n",
    "    return list(word)\n",
    "\n",
    "# Ejemplo de uso\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenize_top_down(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4c96a-9612-43ca-b84d-0df264d4029d",
   "metadata": {},
   "source": [
    "#### Tokenización Bottom-Up\n",
    "\n",
    "El enfoque **bottom-up** es, en cierto sentido, el inverso del top‐down. En lugar de partir de una estructura global, este método utiliza estadísticas y análisis de secuencias para construir tokens a partir de elementos básicos. Es particularmente útil para crear vocabularios de subpalabras en modelos de lenguaje modernos, como aquellos que utilizan técnicas de aprendizaje profundo.\n",
    "\n",
    "#### Características clave\n",
    "\n",
    "- **Análisis estadístico**: Se analiza la frecuencia y la coocurrencia de secuencias de letras o caracteres para determinar cuáles deben combinarse y formar tokens.\n",
    "- **Generación de subpalabras**: Se crean tokens que pueden representar palabras completas, partes de palabras o incluso letras individuales, lo que permite manejar vocabularios muy grandes y palabras poco frecuentes.\n",
    "- **Adaptabilidad a idiomas complejos**: En lenguajes con morfología rica o en aquellos en los que la división en palabras es ambigua, la tokenización bottom‐up permite construir una representación más flexible del vocabulario.\n",
    "- **Uso en algoritmos modernos**: Este enfoque se utiliza en técnicas como Byte-Pair Encoding (BPE) o WordPiece, que se han convertido en estándares para el preprocesamiento de texto en modelos de lenguaje avanzados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ad185-93f1-494a-94f7-86fdea39a4ed",
   "metadata": {},
   "source": [
    "#### Ejemplo: Creación de subpalabras\n",
    "\n",
    "Imagina que se quiere tokenizar la palabra \"unhappiness\". El proceso bottom‐up puede identificar que la secuencia \"un\", \"happi\" y \"ness\" se presentan con alta frecuencia en un corpus y, por lo tanto, se pueden considerar subunidades significativas. De esta forma, la palabra se descompone en:\n",
    "\n",
    "```plaintext\n",
    "[\"un\", \"happi\", \"ness\"]\n",
    "```\n",
    "\n",
    "Esta segmentación permite que el modelo reconozca patrones y relaciones morfológicas, facilitando el manejo de palabras que de otra forma serían muy raras o inexistentes en el vocabulario base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8702206-07e5-4153-93ec-3940bb4443d3",
   "metadata": {},
   "source": [
    "#### Ejemplo con expresiones regulares\n",
    "\n",
    "El siguiente código en Python muestra cómo se puede tokenizar un texto en inglés utilizando expresiones regulares y la biblioteca NLTK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Texto de entrada\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "# Patrón para la tokenización\n",
    "pattern = r'''(?x)                   # activar verbose mode\n",
    "              (?:[A-Z]\\.)+           # abreviaturas, por ejemplo, U.S.A.\n",
    "            | \\w+(?:-\\w+)*           # palabras con guiones internos opcionales\n",
    "            | \\$?\\d+(?:\\.\\d+)?%?     # monedas, porcentajes, ej. $12.40, 82%\n",
    "            | \\.\\.\\.                 # elipsis\n",
    "            | [][.,;\"'?():-_`]       # tokens separados; incluye ], [\n",
    "            '''\n",
    "\n",
    "# Aplicando la tokenización usando el patrón\n",
    "tokens = nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "# Mostrando los tokens resultantes\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4beb4b5",
   "metadata": {},
   "source": [
    "Este código ilustra varios puntos importantes:\n",
    "- Se utiliza el modo _verbose_ en la expresión regular para mejorar la legibilidad.\n",
    "- Se reconocen abreviaturas, palabras con guiones, números con o sin símbolos de moneda, elipsis y ciertos signos de puntuación.\n",
    "- El proceso es determinista y se ejecuta de forma rápida, lo cual es esencial en sistemas de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6faab",
   "metadata": {},
   "source": [
    "#### Tokenización en idiomas sin delimitadores explícitos\n",
    "\n",
    "El desafío se vuelve mucho mayor en idiomas como el chino, japonés o tailandés, donde los espacios no indican de forma explícita los límites de las palabras.  \n",
    "En el caso del chino, por ejemplo, los caracteres (llamados *hanzi*) representan generalmente un morfema y cada uno suele pronunciarse como una sola sílaba. Un hecho interesante es que, en promedio, una palabra en chino tiene alrededor de 2 a 4 caracteres. Sin embargo, la definición de \"palabra\" en chino puede variar según el estándar de segmentación adoptado.\n",
    "\n",
    "##### Ejemplos de segmentación en Chino\n",
    "\n",
    "1. **Segmentación según el Chinese Treebank**:  \n",
    "   Se podría segmentar la oración  \n",
    "   ```plaintext\n",
    "   姚明进入总决赛\n",
    "   ```  \n",
    "   en tres palabras, donde “姚明” se considera una unidad y “进入总决赛” se trata como otra unidad compuesta.\n",
    "\n",
    "2. **Segmentación según la Peking University**:  \n",
    "   Otra aproximación puede segmentar la misma oración en cinco palabras, separando cada componente de manera más granular:  \n",
    "   ```plaintext\n",
    "   姚 明 进入 总 决赛\n",
    "   ```\n",
    "\n",
    "3. **Tokenización a nivel de caracteres**:  \n",
    "   En muchos casos, especialmente en aplicaciones de aprendizaje profundo, se opta por ignorar la noción de \"palabra\" y se tokeniza a nivel de caracteres. Así, la oración se convierte en una secuencia de 7 caracteres individuales:  \n",
    "   ```plaintext\n",
    "   姚 明 进 入 总 决 赛\n",
    "   ```\n",
    "\n",
    "El uso de caracteres como tokens en chino suele resultar más eficiente para muchas tareas de NLP, ya que evita la creación de un vocabulario excesivamente grande y maneja de forma natural las variaciones en la segmentación. En contraste, en idiomas como el japonés y el tailandés, el carácter individual es demasiado pequeño para capturar el significado completo, por lo que se deben aplicar algoritmos de segmentación de palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10373daa",
   "metadata": {},
   "source": [
    "### **Byte-Pair Encoding (BPE)**: Un algoritmo de tokenización bottom-up\n",
    "\n",
    "El procesamiento del lenguaje natural (NLP) requiere transformar el texto en unidades manejables que puedan ser interpretadas y procesadas por algoritmos de aprendizaje automático. Tradicionalmente, la tokenización se realizaba a nivel de palabras o caracteres; sin embargo, estos enfoques presentan desafíos. La tokenización a nivel de palabra, por ejemplo, se ve afectada por el problema de las **palabras desconocidas**: cuando un modelo se enfrenta a una palabra no presente en su vocabulario de entrenamiento, este puede fallar al procesarla adecuadamente.\n",
    "\n",
    "Para solucionar este inconveniente, se han desarrollado técnicas que dividen las palabras en unidades más pequeñas conocidas como **subpalabras**. Estas subunidades pueden ser fragmentos arbitrarios o componentes lingüísticos significativos (como morfemas, que son la unidad mínima portadora de significado). De esta forma, palabras que no fueron vistas durante el entrenamiento pueden representarse mediante la combinación de subpalabras conocidas. Este método no solo mejora la cobertura del vocabulario, sino que también ayuda a reducir la cantidad de tokens necesarios para representar una palabra, manteniendo una buena eficiencia en el modelo.\n",
    "\n",
    "Originalmente, el algoritmo BPE fue desarrollado como una técnica de compresión de datos. Su capacidad para identificar y fusionar los pares de caracteres más frecuentes permitía reducir el tamaño del texto sin perder información esencial. Con el tiempo, se adaptó al ámbito del NLP, aprovechando la idea de que la información lingüística también se puede compactar fusionando unidades frecuentes de caracteres. En este sentido, **BPE** actúa como un método bottom-up: inicia con un vocabulario elemental de caracteres y, mediante iteraciones, fusiona pares adyacentes hasta formar tokens que pueden abarcar palabras completas o partes significativas de ellas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92142202-a8d4-4732-8388-696f37e35100",
   "metadata": {},
   "source": [
    "#### Descripción detallada del algoritmo BPE\n",
    "\n",
    "##### Proceso general de BPE\n",
    "\n",
    "El algoritmo BPE se compone de dos fases principales:\n",
    "1. **Token learner (aprendiz de tokens):** Se toma un corpus de entrenamiento y se induce un vocabulario inicial basado únicamente en caracteres. A partir de allí, se identifican y fusionan de manera iterativa los pares de caracteres adyacentes que aparecen con mayor frecuencia en el corpus. Cada fusión agrega un nuevo token al vocabulario.\n",
    "2. **Token segmenter (segmentador de tokens):** Una vez aprendido el vocabulario con las fusiones, el token segmenter aplica estas fusiones de forma greedy (codiciosa) en el mismo orden en el que fueron aprendidas para tokenizar nuevos textos.\n",
    "\n",
    "Este procedimiento permite construir un vocabulario que abarca desde los caracteres individuales hasta combinaciones que representan subpalabras o palabras completas.\n",
    "\n",
    "##### Pasos del algoritmo\n",
    "\n",
    "A continuación se describe el proceso paso a paso:\n",
    "\n",
    "- **Inicialización:**\n",
    "  - Se parte de un corpus de entrenamiento y se separa el texto en palabras (generalmente utilizando espacios en blanco).\n",
    "  - Cada palabra se descompone en caracteres individuales. Por ejemplo, la palabra `\"low\"` se transforma en `[\"l\", \"o\", \"w\"]`.\n",
    "  - Se construye el vocabulario inicial utilizando todos los caracteres únicos presentes en el corpus.\n",
    "\n",
    "- **Identificación del par más frecuente:**\n",
    "  - Se analiza el corpus tokenizado para contar la frecuencia de cada par de tokens adyacentes.\n",
    "  - Se identifica el par con la frecuencia más alta. Por ejemplo, si en las palabras **\"newer\"** y **\"wider\"** el par **(\"e\", \"r\")** aparece de manera recurrente, éste se selecciona para la fusión.\n",
    "\n",
    "- **Fusión de tokens:**\n",
    "  - El par identificado se fusiona en un nuevo token, que se añade al vocabulario.\n",
    "  - En el corpus, cada aparición de dicho par es reemplazada por el nuevo token. Este proceso se repite un número predefinido de veces, denominado **num_merges**.\n",
    "\n",
    "- **Actualización del corpus y del vocabulario:**\n",
    "  - Con cada fusión, el corpus se actualiza para reflejar la nueva estructura de tokens.\n",
    "  - El vocabulario crece incorporando los nuevos tokens generados a partir de las fusiones.\n",
    "\n",
    "- **Tokenización de nuevos textos:**\n",
    "  - Una vez completado el proceso de entrenamiento, se dispone de una lista ordenada de fusiones. Para tokenizar una nueva palabra, se comienza con la división en caracteres y se aplican las fusiones en el orden aprendido.\n",
    "\n",
    "Esta metodología permite que, al final del proceso, la mayoría de las palabras se representen por tokens completos. Solo aquellas palabras que son poco frecuentes o que no aparecieron durante el entrenamiento se segmentarán en subpalabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293bb5d4-2a02-478d-9997-01c2015e63ad",
   "metadata": {},
   "source": [
    "#### Ejemplos ilustrativos de BPE\n",
    "\n",
    "##### **Ejemplo con palabras cortas**\n",
    "\n",
    "Imaginemos un corpus muy pequeño formado por las palabras:  \n",
    "\n",
    "- `\"low\"` (5 apariciones)  \n",
    "- `\"lowest\"` (2 apariciones)  \n",
    "- `\"newer\"` (6 apariciones)  \n",
    "- `\"wider\"` (3 apariciones)  \n",
    "- `\"new\"` (2 apariciones)\n",
    "\n",
    "El vocabulario inicial consiste en los caracteres únicos:  \n",
    "`{_, d, e, i, l, n, o, r, s, t, w}`  \n",
    "\n",
    "Aquí, el símbolo especial de fin de palabra (`_`) se podría usar para marcar el final de cada palabra.\n",
    "\n",
    "El algoritmo procede fusionando los pares de caracteres más frecuentes. Por ejemplo, en la primera iteración se cuenta la frecuencia de todos los pares y se encuentra que **(\"e\", \"r\")** es el par más frecuente. Se fusiona para obtener el token `\"er\"`, y el corpus se actualiza. En las siguientes iteraciones se podrían fusionar otros pares como **(\"n\", \"e\")** para formar `\"ne\"`, luego **(\"ne\", \"w\")** para formar `\"new\"`, y así sucesivamente. Este proceso permite que palabras conocidas se representen con tokens completos, mientras que palabras raras se descomponen en subpalabras.\n",
    "\n",
    "##### **Ejemplo de descomposición en subpalabras**\n",
    "\n",
    "Consideremos un corpus con las palabras: `\"hello\"`, `\"hell\"` y `\"help\"`. El proceso es el siguiente:\n",
    "- Se inicia con el vocabulario: `{\"h\", \"e\", \"l\", \"o\", \"p\"}`.\n",
    "- Las palabras se tokenizan inicialmente como:\n",
    "  - `\"hello\"` → `[\"h\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "  - `\"hell\"` → `[\"h\", \"e\", \"l\", \"l\"]`\n",
    "  - `\"help\"` → `[\"h\", \"e\", \"l\", \"p\"]`\n",
    "- Se identifican y fusionan los pares de tokens más frecuentes. Por ejemplo, si **(\"l\", \"l\")** es el par más común, se fusiona para formar `\"ll\"`, obteniéndose:\n",
    "  - `\"hello\"` → `[\"h\", \"e\", \"ll\", \"o\"]`\n",
    "  - `\"hell\"` → `[\"h\", \"e\", \"ll\"]`\n",
    "  - `\"help\"` → `[\"h\", \"e\", \"l\", \"p\"]`\n",
    "- Posteriormente, se podría fusionar otro par, como **(\"h\", \"e\")**, dando lugar a:\n",
    "  - `\"hello\"` → `[\"he\", \"ll\", \"o\"]`\n",
    "  - `\"hell\"` → `[\"he\", \"ll\"]`\n",
    "  - `\"help\"` → `[\"he\", \"l\", \"p\"]`\n",
    "\n",
    "De esta manera, las palabras se segmentan en subpalabras que se pueden recombinar para representar tanto palabras conocidas como desconocidas.\n",
    "\n",
    "##### **Ejemplo con una oración completa**\n",
    "\n",
    "Supongamos que queremos tokenizar la oración:  \n",
    "`\"I am learning BPE\"`  \n",
    "El proceso sería:\n",
    "\n",
    "1. **Inicialización:** Se dividen los caracteres de cada palabra, generando tokens individuales.\n",
    "2. **Identificación y fusión:** Se identifican los pares más frecuentes, por ejemplo, **(\"i\", \"n\")** en la palabra `\"learning\"`.\n",
    "3. **Aplicación de fusiones:** Se fusionan los tokens correspondientes en el orden aprendido hasta obtener tokens que pueden representar palabras enteras o fragmentos significativos.\n",
    "\n",
    "El resultado final podría ser que palabras como `\"learning\"` se representen por un token único o por una combinación de subpalabras, dependiendo de la granularidad deseada.\n",
    "\n",
    "##### **Manejo de palabras desconocidas**\n",
    "\n",
    "Un caso interesante ocurre al tokenizar una palabra no vista durante el entrenamiento. Por ejemplo, si el vocabulario aprendido incluye tokens como `\"cat\"`, `\"er\"`, `\"p\"`, `\"ill\"` y `\"ar\"`, la palabra `\"caterpillar\"` podría descomponerse en:  \n",
    "`[\"cat\", \"er\", \"p\", \"ill\", \"ar\"]`  \n",
    "Incluso sin haber aparecido la palabra completa en el corpus, se logra una representación mediante la combinación de subpalabras conocidas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744258d8",
   "metadata": {},
   "source": [
    "### Implementación básica del algoritmo BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0774f-fa1d-45ee-bd86-9af704d0d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoding:\n",
    "    def __init__(self, num_merges):\n",
    "        self.num_merges = num_merges  # num_merges: define el número de fusiones de pares de tokens\n",
    "        self.vocab = {}  # vocab: almacena el vocabulario actual \n",
    "        self.merges = []  # merges: lista donde se almacenan las fusiones que se van realizando\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Inicializa el corpus tokenizado y el vocabulario con todos los caracteres únicos en el corpus\n",
    "        tokenized_corpus = []\n",
    "        for word in corpus:\n",
    "            tokens = list(word)  # Convierte cada palabra en una lista de caracteres (tokens iniciales)\n",
    "            tokenized_corpus.append(tokens)\n",
    "            # Agrega todos los caracteres únicos al vocabulario\n",
    "            for char in tokens:\n",
    "                self.vocab[char] = char\n",
    "\n",
    "        print(f\"Vocabulario inicial: {self.vocab}\")\n",
    "        \n",
    "        # En cada iteración, el modelo cuenta las frecuencias de todos los pares de tokens\n",
    "        for merge_step in range(1, self.num_merges + 1):\n",
    "            pairs = self.get_pair_frequencies(tokenized_corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Encuentra el par con la frecuencia más alta\n",
    "            most_frequent_pair = max(pairs, key=pairs.get)\n",
    "\n",
    "            # Concatenar el par más frecuente para formar un nuevo token\n",
    "            new_token = ''.join(most_frequent_pair)\n",
    "\n",
    "            # Actualiza el vocabulario y guarda la fusión\n",
    "            self.vocab[new_token] = new_token\n",
    "            self.merges.append(most_frequent_pair)\n",
    "\n",
    "            # Muestra el estado actual del vocabulario y el corpus\n",
    "            print(f\"\\nPaso {merge_step}:\")\n",
    "            print(f\"Par más frecuente: {most_frequent_pair}\")\n",
    "            print(f\"Nuevo token: {new_token}\")\n",
    "\n",
    "            # Reemplaza cada aparición del par en las listas de tokens\n",
    "            tokenized_corpus = self.replace_pairs_in_corpus(tokenized_corpus, most_frequent_pair, new_token)\n",
    "\n",
    "            print(f\"Corpus actualizado: {tokenized_corpus}\")\n",
    "            print(f\"Vocabulario actualizado: {self.vocab}\")\n",
    "\n",
    "    # Calcula las frecuencias de todos los pares consecutivos de tokens en el corpus.\n",
    "    def get_pair_frequencies(self, tokenized_corpus):\n",
    "        pairs = {}\n",
    "        for tokens in tokenized_corpus:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                if pair in pairs:\n",
    "                    pairs[pair] += 1\n",
    "                else:\n",
    "                    pairs[pair] = 1\n",
    "        return pairs\n",
    "\n",
    "    # Toma un par de tokens (pair) y lo reemplaza por un nuevo token\n",
    "    def replace_pairs_in_corpus(self, tokenized_corpus, pair, new_token):\n",
    "        new_corpus = []\n",
    "        for tokens in tokenized_corpus:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            # Verifica cada posición para detectar el par\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                    new_word.append(new_token)  # Fusiona el par en un nuevo token\n",
    "                    i += 2  # Salta ambos tokens fusionados\n",
    "                else:\n",
    "                    new_word.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "\n",
    "    # Tokeniza una palabra nueva utilizando las fusiones aprendidas durante el entrenamiento\n",
    "    def tokenize(self, word):\n",
    "        tokens = list(word)  # Inicialmente cada letra es un token separado\n",
    "        for merge in self.merges:\n",
    "            new_token = ''.join(merge)\n",
    "            merged_word = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == merge:\n",
    "                    merged_word.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    merged_word.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = merged_word\n",
    "        return tokens\n",
    "\n",
    "    def get_vocabulary(self):  # Devuelve el vocabulario actual de trabajo\n",
    "        return self.vocab\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"low\", \"lowest\", \"new\", \"wider\", \"lowering\"]\n",
    "num_merges = 4  # Número de combinaciones deseadas\n",
    "\n",
    "bpe = BytePairEncoding(num_merges)\n",
    "bpe.train(corpus)\n",
    "vocab = bpe.get_vocabulary()\n",
    "\n",
    "print(\"\\nVocabulario final generado:\")\n",
    "print(vocab)\n",
    "\n",
    "# Tokenización de una palabra nueva\n",
    "word_to_tokenize = \"lower\"\n",
    "tokens = bpe.tokenize(word_to_tokenize)\n",
    "print(f\"\\nTokenización de '{word_to_tokenize}': {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230f19a",
   "metadata": {},
   "source": [
    "### Variantes del BPE\n",
    "\n",
    "1. **[Subword regularization](https://arxiv.org/abs/1804.10959)**\n",
    "   - **Descripción**: Subword Regularization es una técnica que introduce aleatoriedad en la tokenización para mejorar la robustez del modelo. En lugar de generar una única secuencia de sub-palabras para una palabra, esta técnica genera múltiples posibles segmentaciones de sub-palabras durante el entrenamiento. Esto mejora la capacidad del modelo para manejar variaciones y reduce la dependencia en una única tokenización.\n",
    "   - **Aplicación**: Este método es útil en situaciones donde las palabras pueden tener múltiples formas o en idiomas con morfología compleja.\n",
    "   - **Implementación**: En lugar de siempre elegir la segmentación con la mayor frecuencia, se elige de forma aleatoria entre las opciones disponibles, ponderando según la probabilidad de cada segmentación.\n",
    "   \n",
    "2. **[BPE-Dropout](https://arxiv.org/abs/1910.13267)**\n",
    "   - **Descripción**: BPE-Dropout es una técnica que introduce aleatoriedad durante el entrenamiento de BPE. En lugar de siempre aplicar la regla de combinación más frecuente, se omite aleatoriamente ciertas reglas durante la construcción del vocabulario.\n",
    "   - **Ventaja**: Esta técnica promueve la diversidad en la tokenización y reduce el sobreajuste del modelo a una tokenización específica, lo que puede mejorar el rendimiento en situaciones de prueba con palabras no vistas.\n",
    "   - **Aplicación**: Es útil para mejorar la generalización de modelos de lenguaje, especialmente en casos con corpus de entrenamiento limitado.\n",
    "\n",
    "3. **[Unigram language model](https://huggingface.co/learn/nlp-course/en/chapter6/7)**\n",
    "   - **Descripción**: En lugar de usar BPE, donde los pares de tokens más frecuentes se combinan de manera iterativa, el modelo de lenguaje Unigram es un enfoque basado en la probabilidad. En este enfoque, se construye un modelo de lenguaje unigram (donde cada token es independiente de los demás), y se realiza una selección basada en la probabilidad para determinar qué tokens mantener y cuáles eliminar durante la tokenización.\n",
    "   - **Ventaja**: Este método es más flexible en cuanto a la tokenización, ya que permite mantener múltiples segmentaciones posibles, y se ajusta bien a diferentes idiomas con complejidades morfológicas.\n",
    "   - **Aplicación**: Es utilizado en modelos como SentencePiece, que implementa tanto BPE como Unigram para la tokenización.\n",
    "\n",
    "4. **BBPE (Bilateral Byte Pair Encoding)**\n",
    "   - **Descripción**: BBPE es una variante del BPE que considera tanto la frecuencia de los tokens como la regularidad de la estructura de sub-palabras. Esta técnica busca evitar la creación de sub-palabras que no sean lingüísticamente significativas o que sean demasiado raras.\n",
    "   - **Proceso**: BBPE introduce un criterio de regularización que penaliza la fusión de pares de tokens si estos no contribuyen a una representación más coherente del lenguaje.\n",
    "   - **Aplicación**: Es especialmente útil para reducir la fragmentación en palabras largas o compuestas.\n",
    "\n",
    "5. **[Scaffold-BPE](https://arxiv.org/html/2404.17808v1)**\n",
    "    - **Descripción**: Incorpora un mecanismo dinámico de eliminación de tokens de andamiaje mediante modificaciones sin parámetros, de bajo costo computacional y fáciles de implementar al BPE original.\n",
    "   - **Proceso**: Usa un mecanismo de exclusión de Scaffold Tokens de baja frecuencia de las representaciones de tokens para los textos dados, mitigando así el problema del desequilibrio de frecuencia y facilitando el entrenamiento del modelo.\n",
    "   - **Aplicación**: Es especialmente útil para tareas de modelado de lenguaje y de traducción automática.\n",
    "\n",
    "6. **[Multi-lingual BPE](https://aclanthology.org/P19-1341/)**\n",
    "   - **Descripción**: Multi-lingual BPE es una extensión del BPE que se aplica a múltiples idiomas simultáneamente. El vocabulario se construye a partir de corpus multilingües y las reglas de combinación se aplican de manera que maximicen la reutilización de sub-palabras comunes entre diferentes idiomas.\n",
    "   - **Ventaja**: Promueve la transferencia de conocimiento entre idiomas y es útil en modelos multilingües.\n",
    "   - **Aplicación**: Utilizado en sistemas de traducción automática multilingüe o en modelos de lenguaje que abarcan varios idiomas.\n",
    "\n",
    "7. **Adaptive BPE**\n",
    "   - **Descripción**: En Adaptive BPE, la segmentación de palabras se adapta dinámicamente durante el entrenamiento de un modelo, en lugar de utilizar un vocabulario estático. Esto permite al modelo ajustar la granularidad de la tokenización según la complejidad del contexto.\n",
    "   - **Ventaja**: Mejora la capacidad del modelo para manejar palabras raras o complejas y permite una segmentación más fina cuando es necesario.\n",
    "   - **Aplicación**: Este enfoque es ideal para modelos que requieren un alto nivel de precisión en la tokenización, como los utilizados en tareas de traducción automática.\n",
    "\n",
    "8. **BPE con vocabulary restriction**\n",
    "   - **Descripción**: En esta variante, se aplica una restricción de vocabulario donde solo se permite que ciertas combinaciones de tokens se realicen si no exceden un tamaño de vocabulario predeterminado.\n",
    "   - **Ventaja**: Controla el crecimiento del vocabulario y previene la generación de demasiados tokens, lo que puede ser útil en situaciones donde el tamaño del vocabulario debe mantenerse limitado.\n",
    "   - **Aplicación**: Utilizado en sistemas donde los recursos de memoria o procesamiento son limitados.\n",
    "\n",
    "9. **BPE con hierarchical merging**\n",
    "   - **Descripción**: En lugar de combinar los pares de tokens más frecuentes en una sola fase, la combinación se realiza en múltiples fases, siguiendo una estructura jerárquica. Este enfoque permite una tokenización que respeta la estructura morfológica o gramatical del idioma.\n",
    "   - **Ventaja**: Mejor preservación de la semántica y la morfología del lenguaje durante la tokenización.\n",
    "   - **Aplicación**: Especialmente útil en idiomas con estructuras morfológicas complejas, como el turco o el finés.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028beea5-87bd-4e3b-a0eb-f90f802cb045",
   "metadata": {},
   "source": [
    "#### **Uso en modelos de lenguaje modernos**\n",
    "\n",
    "La tokenización basada en BPE ha sido adoptada ampliamente en modelos de lenguaje modernos, como GPT, BERT y sus variantes. Su capacidad para reducir el número de tokens y gestionar de forma flexible la diversidad léxica ha contribuido a mejorar el rendimiento en tareas de generación y comprensión del lenguaje. Además, su implementación relativamente sencilla permite integrarlo en pipelines de procesamiento de datos sin requerir grandes recursos computacionales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980df15f-31d8-4818-9fb1-b775837cec58",
   "metadata": {},
   "source": [
    "#### Consideraciones técnicas y limitaciones del enfoque BPE\n",
    "\n",
    "##### **El enfoque greedy y sus implicaciones**\n",
    "\n",
    "Una característica importante del algoritmo BPE es su naturaleza **greedy** al aplicar las fusiones: se realizan las combinaciones en el orden en que fueron aprendidas sin reevaluar el contexto en cada paso de tokenización. Este enfoque tiene implicaciones en la calidad de la segmentación, ya que las fusiones tempranas pueden limitar las opciones disponibles para subdividir palabras complejas. Sin embargo, en la práctica, la simplicidad y eficiencia del método han hecho que este compromiso sea aceptable para la mayoría de las aplicaciones.\n",
    "\n",
    "##### **Elección del número de fusiones**\n",
    "\n",
    "El parámetro `num_merges` define cuántas veces se fusionarán pares de tokens y, por ende, el tamaño final del vocabulario. La elección de este número es crucial:\n",
    "- Un valor muy bajo puede resultar en un vocabulario fragmentado en el que muchas palabras se dividen en demasiados tokens, perdiendo información semántica.\n",
    "- Un valor muy alto puede llevar a que se incluyan tokens demasiado específicos, lo que podría dificultar la representación de palabras desconocidas.\n",
    "\n",
    "En aplicaciones reales, la elección de `num_merges` se basa en un balance entre la cobertura del vocabulario y la eficiencia computacional.\n",
    "\n",
    "##### **Relación entre el corpus de entrenamiento y el vocabulario generado**\n",
    "\n",
    "El rendimiento de BPE depende en gran medida del corpus de entrenamiento:\n",
    "- **Diversidad lingüística:**  \n",
    "  Un corpus amplio y representativo permite que BPE aprenda subpalabras que capturan la variación morfológica y léxica del idioma.\n",
    "- **Frecuencia de tokens:**  \n",
    "  El algoritmo se basa en la frecuencia de aparición de pares de tokens. Por ello, en dominios especializados, es importante disponer de un corpus que refleje el uso real del lenguaje en ese contexto.\n",
    "\n",
    "\n",
    "\n",
    "##### **Comparación con implementaciones existentes**\n",
    "\n",
    "Bibliotecas como **SentencePiece** han popularizado el uso de BPE y otros métodos (como el modelado de lenguaje unigram) en la industria. Estas implementaciones ofrecen:\n",
    "- Herramientas robustas para el preprocesamiento y la tokenización.\n",
    "- Optimización para trabajar en entornos de producción.\n",
    "- Flexibilidad para ajustar parámetros y adaptar el vocabulario a necesidades específicas del dominio.\n",
    "\n",
    "\n",
    "##### **Consideraciones sobre la granularidad y el contexto**\n",
    "\n",
    "Un aspecto importante en el diseño de tokenizadores es la **granularidad** con la que se segmenta el texto. BPE ofrece la posibilidad de controlar esta granularidad mediante el parámetro de fusiones. Una granularidad fina (más fusiones) permite capturar subpalabras muy específicas, lo que puede ser ventajoso en idiomas con alta morfología o en dominios técnicos. Sin embargo, una granularidad excesivamente fina podría conducir a una mayor longitud de secuencias y, en consecuencia, a un aumento del costo computacional durante el entrenamiento y la inferencia del modelo.\n",
    "\n",
    "Asimismo, el contexto en el que se aplica la tokenización es relevante. Por ejemplo, en tareas de traducción automática o resumen de textos, mantener ciertas unidades semánticas intactas puede ser crucial para preservar el significado original. BPE, al aprender de la frecuencia de pares en el corpus, tiende a agrupar aquellas combinaciones que son estadísticamente significativas, contribuyendo a una representación coherente del lenguaje.\n",
    "\n",
    "\n",
    "##### **Integración en modelos de lenguaje**\n",
    "\n",
    "El uso de BPE no se limita a la tokenización de palabras aisladas; es una parte integral en el preprocesamiento de datos para modelos de lenguaje. Al incorporar un tokenizador BPE en el pipeline de entrenamiento, se consigue:\n",
    "- Una reducción en el tamaño del vocabulario, lo que simplifica la representación del texto.\n",
    "- Una mayor robustez frente a la variabilidad lingüística, ya que se pueden representar palabras nuevas mediante la combinación de subpalabras.\n",
    "- Una mayor eficiencia en el procesamiento, ya que la longitud de las secuencias puede disminuir notablemente en comparación con una tokenización estrictamente a nivel de caracteres.\n",
    "\n",
    "La implementación presentada puede ser adaptada y extendida para formar parte de sistemas más complejos, integrándose con bibliotecas y frameworks especializados en NLP.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
