### Tokenizaci√≥n para FM y LLM, BPE y variantes, normalizaci√≥n, lematizaci√≥n, segmentaci√≥n, distancias de edici√≥n y Viterbi


#### 1) Por qu√© la tokenizaci√≥n es el "puente" entre texto y tensores

En modelos fundacionales (FM) y modelos de lenguaje grandes (LLM), la tokenizaci√≥n transforma texto crudo en unidades discretas que el modelo puede mapear  a identificadores e incrustaciones. 
Esta capa es m√°s que un tr√°mite: condiciona el largo efectivo de secuencia, el costo de entrenamiento e inferencia, la robustez ante dominios variados y la cobertura frente a idiomas, emojis y s√≠mbolos t√©cnicos. 

Una buena pol√≠tica de tokenizaci√≥n equilibra tres tensiones pr√°cticas: cobertura abierta para evitar fuera de vocabulario, compacidad para no disparar el c√≥mputo por secuencias largu√≠simas y estabilidad para mantener consistencia entre entrenamiento e inferencia.

La tokenizaci√≥n moderna suele combinar dos etapas. Primero, una pre-tokenizaci√≥n *top-down* que protege patrones enteros con reglas, por ejemplo, direcciones de correo, URLs, cantidades monetarias o abreviaturas. 
Despu√©s, una subtokenizaci√≥n *bottom-up* que descompone cada fragmento en subunidades √≥ptimas, casi siempre con variantes de BPE u otros m√©todos  de subpalabras. 

Esta combinaci√≥n captura regularidades ling√º√≠sticas sin renunciar a una cobertura universal.

#### 2) Top-down vs bottom-up: c√≥mo se complementan

Top-down significa partir desde la estructura del texto para preservar "trozos" que deben permanecer √≠ntegros. 
En un enunciado como "That U.S.A. poster-print costs $12.40... Email me@example.com!", una estrategia *top-down* extrae tokens como U.S.A., poster-print, $12.40 y me@example.com para evitar que la etapa posterior rompa entidades √∫tiles. 

Esto reduce errores comunes como separar el signo de d√≥lar de la cantidad o descomponer una abreviatura en letras sueltas. 

En LLM, esta capa ayuda a estabilizar entradas con signos de puntuaci√≥n, n√∫meros y formatos t√©cnicos, lo que mejora la coherencia sem√°ntica que m√°s tarde el modelo ver√° como secuencias de IDs.

*Bottom-up*, en cambio, parte de unidades peque√±as y aprende a fusionarlas si son frecuentes.
Aqu√≠ entra **BPE (Byte-pair encoding)**: un algoritmo de fusiones que induce un vocabulario de subpalabras a partir de un corpus. En producci√≥n, el ranking de fusiones se aplica de forma codiciosa a cadenas nuevas. 
Esta idea captura morfemas comunes y repeticiones ortogr√°ficas, disminuye la tasa de tokens por palabra habitual y mantiene una v√≠a de escape para palabras raras, nombres propios o neologismos que se descomponen en piezas m√°s cortas sin quedar fuera de vocabulario.

La clave pr√°ctica es que *top-down* y *bottom-up* no compiten, sino que se encadenan: primero se preservan unidades cr√≠ticas con reglas, luego se subtokeniza  con BPE o un m√©todo af√≠n. 

Esta arquitectura produce entradas m√°s estables, con menos sorpresas para el modelo y m√©tricas de eficiencia mejores.

#### 3) BPE cl√°sico: qu√© es y c√≥mo funciona

**Byte-Pair Encoding (BPE)** es un algoritmo bottom-up que aprende fusiones de pares adyacentes. El flujo cl√°sico es sencillo y efectivo:

- Representar cada palabra como una secuencia de s√≠mbolos iniciales, t√≠picamente caracteres.
- Contar frecuencias de pares adyacentes en todo el corpus.
- Fusionar el par m√°s frecuente creando un nuevo s√≠mbolo de subpalabra.
- Reemplazar y repetir hasta alcanzar un n√∫mero objetivo de fusiones, que define el tama√±o del vocabulario.
- En inferencia, aplicar el ranking de fusiones de forma codiciosa sobre texto nuevo.

Este ciclo compacta secuencias repetidas y morfemas. En un ejemplo can√≥nico con el corpus "low, lowest, new, wider, lowering", las primeras fusiones 
tienden a crear unidades como "lo" y "low", y m√°s adelante "er" o "est", que permiten tokenizar "lower" en "low" + "er" y "lowest" en algo  como "low" + "we" + "st", seg√∫n frecuencias y trayectoria de fusiones. 

El resultado es un vocabulario de subpalabras que reduce el n√∫mero promedio de tokens por palabra sin perder la posibilidad de descomponer t√©rminos raros.

En espa√±ol, BPE puede capturar piezas como "ci√≥n" o "mente" si el corpus las respalda. Cuando una secuencia con acentos no aparece de forma consistente, el algoritmo cae a unidades m√°s peque√±as y sigue cubriendo el texto de manera determinista.
Marcadores como un prefijo especial para espacio o un delimitador de fin de palabra ayudan a mantener l√≠mites coherentes entre palabras.

**Ventajas pr√°cticas del BPE cl√°sico:**

- Eficiencia: menos tokens por palabra t√≠pica, lo que permite m√°s contexto con el mismo presupuesto.
- Robustez razonable: palabras desconocidas se descomponen en trozos vistos, evitando el s√≠mbolo desconocido.
- Simplicidad y velocidad: el ranking de fusiones es est√°tico y su aplicaci√≥n codiciosa es r√°pida.

**Limitaciones:**

- Si la base son caracteres Unicode, ciertos s√≠mbolos poco frecuentes o combinaciones de bytes para emojis pueden quedar mal segmentados o escapar a la cobertura esperada.
- Idiomas con escritura compleja o cadenas multilenguaje pueden sufrir degradaci√≥n, sobre todo si el corpus de entrenamiento no los represent√≥ bien.

Estas limitaciones motivan variantes m√°s robustas.

#### 4) Byte-level BPE: cobertura universal desde los bytes

Byte-level BPE parte de los bytes crudos del texto en lugar de caracteres Unicode. Dado que cualquier cadena puede codificarse en bytes, esta variante ofrece cobertura universal: no existen fuera de vocabulario, y emojis o secuencias de distintos alfabetos entran de manera natural. 

Con un ejemplo como "ma√±ana üòä", los bytes UTF-8 de la *√±* y del emoji se tratan como s√≠mbolos iniciales. 
Sin fusiones, la tokenizaci√≥n emite cada byte. Con fusiones aprendidas, puede colapsar pares o grupos frecuentes, por ejemplo, los dos bytes quecodifican *√±* o los cuatro del emoji, y mantener un prefijo especial para el espacio que estabiliza los l√≠mites de palabra. 

Esta estrategia garantiza que toda entrada sea tokenizable y que los trozos m√°s comunes de bytes se compacten, logrando un buen balance entre cobertura y longitud de secuencia.

**Limitaciones y su impacto pr√°ctico**

Aunque byte-level BPE garantiza cobertura universal, su menor alineaci√≥n con morfemas complica tareas de an√°lisis humano y depuraci√≥n. 
En auditor√≠as de salida, rastrear por qu√© un modelo gener√≥ una forma concreta es m√°s dif√≠cil cuando los tokens son bytes o grupos de bytes que no se corresponden con unidades ling√º√≠sticas interpretables.

Esto afecta las inspecciones de *saliency* (t√©cnicas que muestran qu√© partes de la entrada influyen m√°s en la salida), la atribuci√≥n de errores y 
las *safety reviews* (revisiones de seguridad y cumplimiento, por ejemplo para detectar toxicidad o informaci√≥n personal identificable), porque 
los filtros y las reglas suelen operar a nivel de palabra o de subpalabra con sentido ling√º√≠stico, en lugar de tokens de bytes.
 
Tambi√©n puede enmascarar sesgos morfol√≥gicos (prefijos/sufijos) y dificultar diagn√≥sticos de inyecci√≥n de prompt o jailbreaks que se esconden en secuencias de s√≠mbolos. 

Recomendaci√≥n: instrumentar auditor√≠as peri√≥dicas de subpalabras por dominio (listas de fusiones m√°s frecuentes, proporci√≥n de tokens de un solo byte,...etc), definir listas blancas/negras de grafemas cr√≠ticos y mantener corpora de prueba por dominio para validar que las fusiones preservan patrones 
relevantes sin degradar la interpretabilidad.

**Implementaci√≥n de auditor√≠as para byte-level BPE**

Para mitigar las limitaciones de byte-level BPE, las auditor√≠as deben ser sistem√°ticas y enfocadas en m√©tricas pr√°cticas:

- An√°lisis de fusiones frecuentes: Generar listas de las subpalabras m√°s comunes por dominio ( redes sociales, textos legales,... etc) y evaluar si reflejan patrones ling√º√≠sticos relevantes.
- Proporci√≥n de tokens de un solo byte: Medir la fracci√≥n de tokens que caen a bytes individuales (indicador de baja compresi√≥n).
- Drift por s√≠mbolo: Monitorear la aparici√≥n de nuevos s√≠mbolos (emojis, caracteres t√©cnicos, ...etc) en inferencia frente al corpus de entrenamiento.
- Listas blancas/negras: Definir grafemas cr√≠ticos que deban preservarse o evitarse en fusiones; protegerlos con reglas de pre-tokenizaci√≥n.
- Corpora de prueba: Mantener conjuntos de datos por dominio para evaluar la calidad de las fusiones.
- Herramientas: Usar bibliotecas como Hugging Face Tokenizers para generar reportes de tokenizaci√≥n y analizar m√©tricas clave.

En la pr√°ctica, byte-level BPE brilla en dominios heterog√©neos: logs, redes sociales, mezclas de idiomas, cadenas con s√≠mbolos de moneda o unidades t√©cnicas.
A cambio, los tokens intermedios pueden no alinearse con morfemas ling√º√≠sticos, lo que dificulta la interpretabilidad en tareas de an√°lisis humano o debugging.
Adem√°s, la calidad de las fusiones depende de un corpus representativo; si ciertos dominios o alfabetos est√°n subrepresentados, las subpalabras pueden ser menos √≥ptimas, aumentando la longitud de secuencia.

#### 5) BPE con byte-fallback: un h√≠brido pragm√°tico

BPE con byte-fallback combina lo mejor de dos mundos. El vocabulario principal se entrena como en BPE cl√°sico sobre unidades Unicode 
o subpalabras m√°s "ling√º√≠sticas". Solo cuando aparece un tramo que no existe en el vocabulario, el tokenizador cae a bytes para ese fragmento espec√≠fico. 

As√≠, "pago ‚Çø100" puede tokenizar como subpalabras para "pago" y "100", y emitir bytes √∫nicamente para el s√≠mbolo de Bitcoin si ese car√°cter particular no existe en el vocabulario. 

Esto preserva la compacidad y la interpretabilidad de subpalabras comunes, a la vez que asegura cobertura universal para raros extremos.

En FM y LLM de producci√≥n, byte-fallback resulta especialmente √∫til: se entrena y perfila como un BPE convencional, no se dispara el tama√±o del vocabulario, pero se garantizan "salvavidas" ante novedades del mundo real sin necesidad de volver a entrenar el tokenizador.

#### 6) Ejemplos comparativos integrados

Consideremos el texto: "That U.S.A. poster-print costs $12.40... ma√±ana ‚Çø y üòä".

- Con pre-tokenizaci√≥n top-down, se preservan unidades como U.S.A., poster-print y $12.40.
- Con BPE cl√°sico, cada fragmento se subtokeniza seg√∫n sus fusiones aprendidas. S√≠mbolos raros como "‚Çø" o el emoji pueden no tener entrada directa y descomponerse en piezas peque√±as o quedar como desconocidos si el esquema no es byte-aware.
- Con byte-level BPE, todo es tokenizable desde el inicio porque parte de bytes y fusiona lo frecuente.
- Con byte-fallback, se utilizan subpalabras "ling√º√≠sticas (alineadas con la escritura natural)" y se emiten bytes solo para el s√≠mbolo poco com√∫n o el emoji, manteniendo el resto en piezas m√°s interpretables.


#### 7) Normalizaci√≥n: coherencia antes de segmentar

La normalizaci√≥n estabiliza el texto antes de cualquier segmentaci√≥n. Suele incluir *case folding* para reducir variaciones por may√∫sculas, normalizaci√≥n *Unicode* (por ejemplo, *NFKC*) para unificar representaciones can√≥nicas, compactaci√≥n de espacios y sustituciones de signos equivalentes. 

En dominios t√©cnicos conviene estandarizar formatos num√©ricos y unidades; en redes sociales, definir pol√≠ticas para emojis, elongaciones o repeticiones de caracteres.

Un punto cr√≠tico es no destruir informaci√≥n sem√°ntica que el modelo necesita. Por ejemplo, pasar todo a min√∫sculas puede ser deseable en un  clasificador peque√±o, pero en LLM modernos a menudo se conserva el caso porque influye en significado y estilo. 
Con *Unicode*, elegir una forma de normalizaci√≥n estable evita duplicaciones invisibles de tokens. 

La normalizaci√≥n debe ser consistente entre entrenamiento e inferencia y debe documentarse como parte del contrato del tokenizador.El dominio manda. En legal o cl√≠nico es com√∫n respetar tildes, s√≠mbolos de cita y separadores de secciones. 
En logs y monitoreo quiz√° se prefiera preservar hashes y direcciones, pero colapsar repeticiones triviales de espacios. 

En general, conviene hacer lo m√≠nimo necesario para estabilizar, posponer transformaciones destructivas y medir su impacto en longitud de secuencia y calidad.

#### 8) Lematizaci√≥n: cu√°ndo aporta valor

La lematizaci√≥n mapea formas flexionadas a su lema can√≥nico, por ejemplo, "cant√©", "cantaba", "cantar√©" -> "cantar".  A diferencia del **stemming**, evita recortes agresivos y mantiene palabras v√°lidas. 

En pipelines cl√°sicos de NLP mejora la generalizaci√≥n de modelos n-grama o lineales, y en recuperaci√≥n de informaci√≥n unifica variantes. 

En LLM grandes no es requisito de entrenamiento, porque el modelo aprende regularidades morfol√≥gicas sobre subpalabras. 
Sin embargo, la lematizaci√≥n puede ser valiosa para limpieza y an√°lisis de datasets, b√∫squeda sem√°ntica, indexaci√≥n o para construir se√±ales de  evaluaci√≥n que comparan predicciones a nivel de lema. 

Tambi√©n ayuda a reducir **sparsity** cuando se entrenan cabezales especializados (clasificador lineal, CRF, MLP) con pocos datos: al colapsar 
variantes morfol√≥gicas en un solo lema, concentras ejemplos en menos tipos, aumentas la densidad por caracter√≠stica, estabilizas las estimaciones de pesos y mejoras calibraci√≥n y eficiencia muestral. 
Incluso usando embeddings, agrupar por lema reduce la varianza entre formas casi equivalentes y hace m√°s estables las representaciones agregadas.

#### 9) Segmentaci√≥n de palabras y subpalabras

La segmentaci√≥n de palabras es trivial en idiomas con espacios, pero crucial en lenguas como chino o tailand√©s. 
En el enfoque *top-down*, se usan diccionarios y reglas, en el probabil√≠stico, se recurre a **Viterbi** con un modelo de lenguaje de caracteres o palabras para encontrar la segmentaci√≥n m√°s probable. 

La segmentaci√≥n de subpalabras con BPE es distinta: no usa **Viterbi**, sino un algoritmo determinista de fusiones. Aun as√≠, pensar en t√©rminos probabil√≠sticos 
es √∫til al fijar pol√≠ticas de pre-tokenizaci√≥n o al comparar dise√±os de vocabulario, porque el objetivo final sigue siendo maximizar la probabilidad del texto con el menor costo de tokens. 

En LLM, la combinaci√≥n de un marcador especial para el espacio y fusiones frecuentes permite que los l√≠mites de palabras sean estables. En dominios con mezcla de alfabetos y emojis, variantes byte-aware evitan que el tokenizador "rompa" caracteres compuestos y reducen sorpresas.

#### 10) Distancias de edici√≥n: Levenshtein y familia

Las distancias de edici√≥n miden cu√°nta transformaci√≥n necesita una cadena para convertirse en otra. **Levenshtein** contabiliza inserciones, eliminaciones y sustituciones con costo uno. 

**Damerau-Levenshtein** a√±ade transposiciones adyacentes, y **Jaro-Winkler** pondera prefijos compartidos, √∫til para nombres propios.

En FM y LLM estas distancias son √∫tiles en varias capas:

- Calidad de datos: detectar duplicados casi id√©nticos, consolidar variantes ortogr√°ficas, auditar drift en capturas de texto.
- Normalizaci√≥n y limpieza: sugerir correcciones de ruido en OCR o tecleo, con umbrales prudentes.
- M√©tricas de evaluaci√≥n: a nivel de caracteres para WER en ASR u OCR, o como se√±al auxiliar cuando el objetivo no es estrictamente sem√°ntico.
- Alineaci√≥n y deduplicaci√≥n: filtros de similitud reducen repeticiones que sesgan el entrenamiento.

Recomendaci√≥n: no usarlas de forma ciega en producci√≥n, sino como filtros asistidos en pipelines de ingenier√≠a de datos.

En evaluaci√≥n, complementan m√©tricas sem√°nticas y ayudan a diagnosticar si un descenso de calidad se debe a errores ortogr√°ficos o a desalineaci√≥n conceptual.

#### 11) Viterbi: decodificaci√≥n √≥ptima con modelos de estados

El algoritmo de **Viterbi** encuentra la secuencia de estados m√°s probable en un modelo oculto dado un conjunto de observaciones. 
En NLP cl√°sico se usa para etiquetado gramatical, segmentaci√≥n en idiomas sin espacios y decodificaci√≥n de CRF para tareas como NER. 

En contextos de FM y LLM aparece como herramienta para:

- Generar etiquetas d√©biles a gran escala (por ejemplo, un etiquetador de entidades basado en CRF con Viterbi).
- Segmentaci√≥n previa en idiomas sin espacios, donde un tokenizador por subpalabras se beneficia de l√≠mites de palabra razonables.
- Post-proceso de predicciones token-level, imponiendo restricciones globales (esquemas BIO v√°lidos o validaciones de formato,... etc).

Con Viterbi se garantiza consistencia global en secuencias. En la pr√°ctica, conviene perfilarlo respecto al tama√±o de secuencia y al inventario de estados, yaque la complejidad crece con ambos.

#### 12) M√©tricas para evaluar la tokenizaci√≥n en FM y LLM

Una pol√≠tica de tokenizaci√≥n se valora por sus m√©tricas operativas:

- Radio de compresi√≥n: n√∫mero promedio de tokens por car√°cter o por palabra.
- Distribuci√≥n de longitudes: colas pesadas pueden indicar reglas deficientes o vocabularios poco ajustados al dominio.
- Consistencia entrenamiento-inferencia: divergencias en normalizaci√≥n o reglas previas se multiplican en errores.
- Cobertura de s√≠mbolos: cu√°ntos caracteres/bytes comunes quedan resueltos como subpalabras √∫tiles y cu√°ntas veces se cae a rutas de escape.
- Estabilidad entre dominios: validar en redes sociales, legal, cl√≠nico, logs.

En la pr√°ctica, tama√±os de vocabulario entre 32k y 100k suelen equilibrar compacidad y costo de embeddings/softmax. 
Conviene incorporar marcadores de palabra y serializar el ranking de fusiones en estructuras veloces (tries, aut√≥matas) para acelerar la inferencia.

#### 13) Recomendaciones de dise√±o por variante de BPE

- BPE cl√°sico: adecuado si el corpus es ling√º√≠sticamente homog√©neo y se desea alineaci√≥n con morfemas; reforzar con pre-tokenizaci√≥n s√≥lida.
- Byte-level BPE: cuando se necesita cobertura universal y se enfrentan emojis, signos t√©cnicos o mezcla de alfabetos, auditar que no emerjan subpalabras in√∫tiles por dominio.
- BPE con byte-fallback: soluci√≥n h√≠brida generalista; mantiene interpretabilidad y cobertura sin reentrenar el tokenizador.

En los tres casos, perfilar el trade-off entre tama√±o de vocabulario y longitud de secuencia, y observar el impacto en throughput y latencia. 
La sensibilidad a la normalizaci√≥n es real: peque√±as decisiones sobre Unicode/espacios pueden mover varios puntos porcentuales en tokens por mil caracteres.

#### 14) Integraci√≥n con normalizaci√≥n, lematizaci√≥n y distancias

Una pol√≠tica integral para FM y LLM orquesta estas piezas:

1. Normalizaci√≥n m√≠nima pero estable.
2. Pre-tokenizaci√≥n top-down para proteger formatos especiales.
3. Subtokenizaci√≥n con BPE o af√≠n (cl√°sico, byte-level o byte-fallback).
4. Medici√≥n de compresi√≥n y estabilidad.
5. Lematizaci√≥n selectiva si aporta a tareas aguas arriba.
6. Distancias de edici√≥n para QA y deduplicaci√≥n.
7. Viterbi u otra decodificaci√≥n global para coherencia secuencial.

Esto reduce sorpresas en inferencia, estabiliza costos y mejora la calidad percibida en dominios con ruido.

#### 15) Ejemplo narrativo de extremo a extremo

Texto de prueba: "That U.S.A. poster-print costs $12.40... ma√±ana ‚Çø y üòä".

1. Normalizaci√≥n: forma Unicode estable (p. ej., NFKC).
2. Pre-tokenizaci√≥n top-down: detectar U.S.A., poster-print, $12.40; tratar el emoji como car√°cter indivisible.
3. Subtokenizaci√≥n:
   -  BPE cl√°sico: "poster-print" en subpalabras morfol√≥gicas; "ma√±ana" puede partirse si fue infrecuente; "‚Çø" puede quedar fuera si el esquema no es byte-aware.
   -  Byte-level BPE: cada s√≠mbolo representable por bytes; fusiones frecuentes compactan "√±" y el emoji; secuencia robusta.
   -  Byte-fallback: palabras comunes en subpalabras interpretables; "‚Çø" y el emoji se emiten como bytes.


| M√©todo | Salida de tokens | N¬∫ de tokens | Comentarios sobre interpretabilidad |
|---|---|---|---|
| BPE cl√°sico | That, U.S.A., poster, -, print, costs, $12.40, ..., ma, √±a, na, ‚Çø, y, üòä | 14 | Alta para texto com√∫n; baja para s√≠mbolos raros (‚Çø, üòä). |
| Byte-level BPE | T, h, a, t,  , U, ., S, ., A, .,  , p, o, s, t, e, r, -, p, r, i, n, t,  , c, o, s, t, s,  , $, 1, 2, ., 4, 0,  , ...,  , m, a, √±, a, n, a,  , ‚Çø,  , y,  , üòä | 50 | Cobertura total; interpretabilidad baja a nivel morfol√≥gico. |
| Byte-fallback | That, U.S.A., poster-print, costs, $12.40, ..., ma√±ana, byte:‚Çø, y, byte:üòä | 10 | Alta; bytes solo para s√≠mbolos fuera del vocabulario (‚Çø, üòä). |

#### 16) Consideraciones operativas para producci√≥n

- Serializar el ranking de fusiones y usar estructuras eficientes (tries, aut√≥matas) para acelerar la aplicaci√≥n codiciosa.
- Documentar normalizaci√≥n y pre-tokenizaci√≥n como parte del contrato de entrada del modelo.
- Versionar vocabulario y fusiones; peque√±os cambios pueden afectar m√©tricas.
- Monitorear la distribuci√≥n de s√≠mbolos y la proporci√≥n de ca√≠das a bytes en byte-fallback.
- Auditar con distancias de edici√≥n para evitar duplicados o near-duplicates que sesguen la distribuci√≥n.
- Dimensionar vocabulario al presupuesto de memoria y throughput; 32k‚Äì100k suele ser un rango √∫til.

#### 17) Conexi√≥n con FM y LLM actuales y consideraciones multiling√ºes

Los FM y LLM recientes operan sobre datos ruidosos y multiformato; por ello se popularizan variantes byte-aware y ca√≠das a bytes. La pre-tokenizaci√≥n top-down sigue siendo √∫til para proteger entidades y formatos. 

Aunque la lematizaci√≥n no es requisito de entrenamiento, mejora ciertas tareas de b√∫squeda y an√°lisis.

En modelos multiling√ºes, un vocabulario sesgado hacia un idioma dominante puede generar subpalabras ineficientes para lenguas minoritarias, elevando longitud de secuencia y costo. 

Mitigaciones:

- Corpos balanceados al entrenar el tokenizador.
- Pre-tokenizaci√≥n espec√≠fica por idioma donde aplique (por ejemplo: segmentaci√≥n para chino).
- M√©tricas de compresi√≥n por idioma para detectar desequilibrios.
- Considerar tokenizadores h√≠bridos (BPE cl√°sico + byte-level para coberturas espec√≠ficas).

#### 18) Herramientas y bibliotecas para implementaci√≥n

- [SentencePiece](https://github.com/google/sentencepiece): BPE y otras t√©cnicas de subtokenizaci√≥n con opciones byte-aware.
- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/index):  BPE, byte-level BPE y byte-fallback; personalizaci√≥n de normalizaci√≥n y pre-tokenizaci√≥n.
- [Tiktoken](https://github.com/openai/tiktoken): orientado a grandes modelos e inferencia eficiente con texto heterog√©neo.

