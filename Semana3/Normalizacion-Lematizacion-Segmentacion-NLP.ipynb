{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b7ca14-d87b-4b16-8ada-608833a5c2e2",
   "metadata": {},
   "source": [
    "### **Normalizaci贸n de palabras**\n",
    "\n",
    "La **normalizaci贸n de palabras** es el proceso de estandarizar palabras o tokens para que sigan un formato uniforme. Un ejemplo com煤n es el **case folding**, que consiste en convertir todas las letras a min煤sculas. Este enfoque permite tratar t茅rminos como \"Woodchuck\" y \"woodchuck\" como id茅nticos, lo cual es particularmente 煤til en tareas como la recuperaci贸n de informaci贸n o el reconocimiento de voz.\n",
    "\n",
    "Sin embargo, en tareas como el an谩lisis de sentimientos, la clasificaci贸n de textos, la extracci贸n de informaci贸n y la traducci贸n autom谩tica, la distinci贸n entre may煤sculas y min煤sculas puede ser crucial. Por ejemplo, diferenciar entre \"US\" (Estados Unidos) y \"us\" (nosotros) puede ser m谩s importante que la simplificaci贸n proporcionada por el **case folding**. Por ello, en estos contextos, suele preferirse conservar la distinci贸n entre may煤sculas y min煤sculas. En algunos casos, se generan tanto versiones diferenciadas como versiones completamente en min煤sculas para modelos de lenguaje, dependiendo de la tarea espec铆fica.\n",
    "\n",
    "Los sistemas que emplean **BPE** u otros m茅todos de tokenizaci贸n bottom-up pueden prescindir de una normalizaci贸n adicional de las palabras. Sin embargo, en otros sistemas de **NLP**, puede ser beneficioso aplicar normalizaciones m谩s avanzadas, como unificar diferentes formas de una palabra, por ejemplo, \"USA\" y \"US\" o \"uh-huh\" y \"uhhuh\". Aunque esta estandarizaci贸n puede implicar la p茅rdida de cierta informaci贸n ortogr谩fica, puede resultar valiosa en algunos casos.\n",
    "\n",
    "Por ejemplo, en la recuperaci贸n o extracci贸n de informaci贸n relacionada con \"US.\", podr铆amos querer identificar datos relevantes tanto si el documento menciona \"US\" como \"USA\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008113a6-9bb2-41a6-bcf9-fb3dde3f33c7",
   "metadata": {},
   "source": [
    "### **Lematizaci贸n**\n",
    "\n",
    "En procesamiento de lenguaje natural, a menudo es deseable que dos formas morfol贸gicamente diferentes de una palabra se traten como equivalentes. Por ejemplo, en una b煤squeda web, un usuario podr铆a escribir \"woodchucks\", pero un sistema eficiente deber铆a ser capaz de devolver p谩ginas que mencionen \"woodchuck\" sin la \"s\". Esto es especialmente relevante en idiomas morfol贸gicamente complejos como el polaco, donde una palabra puede cambiar de forma seg煤n su funci贸n en la oraci贸n. Por ejemplo, \"Warsaw\" toma diferentes terminaciones seg煤n el caso gramatical: **\"Warszawa\"** como sujeto, **\"w Warszawie\"** para \"en Varsovia\" o **\"do Warszawy\"** para \"hacia Varsovia\", entre otras variantes.\n",
    "\n",
    "La **lematizaci贸n** es el proceso de reducir las palabras a su forma base o **lema**, independientemente de sus variaciones morfol贸gicas. Por ejemplo, \"am\", \"are\" e \"is\" tienen como lema com煤n \"be\", mientras que \"dinner\" y \"dinners\" comparten el lema \"dinner\". Lematizar correctamente ayuda a agrupar menciones de palabras en diferentes formas, como en el caso de \"Warsaw\" en polaco. La forma lematizada de una oraci贸n como **\"He is reading detective stories\"** ser铆a **\"He be read detective story\"**, eliminando la flexi贸n verbal y el plural.\n",
    "\n",
    "Los m茅todos m谩s sofisticados para la lematizaci贸n implican un an谩lisis morfol贸gico detallado. La **morfolog铆a** es el estudio de la estructura de las palabras y c贸mo estas se forman a partir de unidades m谩s peque帽as con significado, llamadas **morfemas**. Existen dos tipos principales de morfemas: \n",
    "- **Stems**: el n煤cleo de la palabra, que aporta el significado principal.\n",
    "- **Affixes**: elementos adicionales que modifican el significado o la funci贸n de la palabra.\n",
    "\n",
    "Por ejemplo, la palabra **\"fox\"** consta de un solo morfema (\"fox\"), mientras que **\"cats\"** se compone de dos: el morfema ra铆z **\"cat\"** y el sufijo **\"-s\"** que indica plural. Un **analizador morfol贸gico** (morphological parser) descompone palabras como \"cats\" en estos morfemas, facilitando tareas como la lematizaci贸n y la comprensi贸n del significado en NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafd45d-e3c3-4658-90f2-dc7fe7df9575",
   "metadata": {},
   "source": [
    "#### Stemming: The Porter Stemmer\n",
    "\n",
    "Los algoritmos de **lematizaci贸n** pueden ser complejos y computacionalmente costosos. Por esta raz贸n, en algunos casos se emplea un m茅todo m谩s simple basado en la eliminaci贸n de afijos finales de las palabras, conocido como **stemming**. A diferencia de la lematizaci贸n, el stemming no busca identificar la forma base de la palabra con precisi贸n gramatical, sino simplemente recortar sufijos comunes. \n",
    "\n",
    "Un ejemplo cl谩sico es el **Porter Stemmer**, que cuando se aplica al siguiente p谩rrafo:\n",
    "\n",
    "```plaintext\n",
    "This was not the map we found in Billy Boness chest, but\n",
    "an accurate copy, complete in all thingsnames and heights\n",
    "and soundingswith the single exception of the red crosses\n",
    "and the written notes.\n",
    "```\n",
    "\n",
    "produce la siguiente salida:\n",
    "\n",
    "```plaintext\n",
    "Thi wa not the map we found in Billi Bone s chest but an\n",
    "accur copi complet in all thing name and height and sound\n",
    "with the singl except of the red cross and the written note\n",
    "```\n",
    "\n",
    "El algoritmo se basa en una serie de reglas de reescritura que se aplican en secuencia, donde la salida de cada paso se convierte en la entrada del siguiente.\n",
    "\n",
    "Algunas reglas de muestra (m谩s en [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)):\n",
    "\n",
    "| **Regla**        | **Ejemplo**                        |\n",
    "|------------------|------------------------------------|\n",
    "| **ATIONAL -> ATE**| (relational -> relate)              |\n",
    "| **SSES -> SS**    | (grasses -> grass)                  |\n",
    "\n",
    "Los **stemmers** simples pueden ser 煤tiles en situaciones donde es necesario agrupar diferentes variantes de una palabra bajo una forma com煤n. Sin embargo, su uso en sistemas modernos ha disminuido, ya que pueden cometer errores tanto de **sobregeneralizaci贸n** (por ejemplo, reduciendo *policy* a *police*) como de **subgeneralizaci贸n** (no reduciendo *European* a *Europe*). Debido a estas limitaciones, en muchas aplicaciones de NLP se prefiere la lematizaci贸n, que proporciona resultados m谩s precisos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61906511-9d65-42cc-9ffb-097b01334704",
   "metadata": {},
   "source": [
    "#### **Segmentaci贸n de oraciones**\n",
    "\n",
    "La **segmentaci贸n de oraciones** es un paso fundamental en el procesamiento de texto. Las se帽ales m谩s 煤tiles para dividir un texto en oraciones son los signos de puntuaci贸n, como los puntos, signos de interrogaci贸n y signos de exclamaci贸n. Los signos de interrogaci贸n y exclamaci贸n suelen ser indicadores claros de los l铆mites de una oraci贸n. Sin embargo, los puntos (**\".\"**) pueden ser m谩s ambiguos, ya que pueden marcar el final de una oraci贸n o formar parte de abreviaciones como *\"Mr.\"* o *\"Inc.\"*. Debido a esta ambig眉edad, la segmentaci贸n de oraciones y la tokenizaci贸n de palabras a menudo se abordan conjuntamente.\n",
    "\n",
    "En general, los m茅todos de segmentaci贸n de oraciones funcionan determinando, ya sea mediante reglas o aprendizaje autom谩tico, si un punto act煤a como un marcador de fin de oraci贸n o si forma parte de una palabra. Un diccionario de abreviaciones puede ser 煤til para identificar si un punto pertenece a una abreviatura de uso com煤n. Estos diccionarios pueden construirse manualmente o aprenderse mediante t茅cnicas de machine learning, al igual que los modelos que realizan la segmentaci贸n de oraciones.\n",
    "\n",
    "Por ejemplo, en el **[Stanford CoreNLP toolkit](https://stanfordnlp.github.io/CoreNLP/)**, la segmentaci贸n de oraciones se basa en reglas y es una consecuencia determinista de la tokenizaci贸n. Una oraci贸n termina cuando una puntuaci贸n de final de oraci贸n (*\".\", \"!\", \"?\"*) no est谩 agrupada con otros caracteres dentro de un solo token (como en el caso de abreviaturas o n煤meros), y puede estar seguida opcionalmente por comillas finales o corchetes adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854804be",
   "metadata": {},
   "source": [
    "**Normalizaci贸n de palabras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c338e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Folding\n",
    "def normalize_case(text):\n",
    "    \"\"\"Convierte todo el texto a min煤sculas para la normalizaci贸n.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"Woodchuck\"\n",
    "normalized_text = normalize_case(texto)\n",
    "print(normalized_text)  # Salida: \"woodchuck\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6204d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unify_terms(texto):\n",
    "    \"\"\"Reemplaza diferentes variantes de un t茅rmino por una versi贸n est谩ndar.\"\"\"\n",
    "    term_mapping = {\n",
    "        \"USA\": \"US\",\n",
    "        \"uh-huh\": \"uhhuh\"\n",
    "    }\n",
    "    words = texto.split()\n",
    "    unified_words = [term_mapping.get(word, word) for word in words]\n",
    "    return ' '.join(unified_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"The USA has many dialects. Uh-huh, that's true.\"\n",
    "unified_text = unify_terms(texto)\n",
    "print(unified_text)  # Salida: \"The US has many dialects. uhhuh, that's true.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427734cf",
   "metadata": {},
   "source": [
    "**Lematizaci贸n usando nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_text(texto):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = texto.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"He is reading detective stories\"\n",
    "lemmatized_text = lemmatize_text(texto)\n",
    "print(lemmatized_text)  # Salida: \"He be read detective story\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3c847",
   "metadata": {},
   "source": [
    "**Stemming usando el Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "\n",
    "def stem_text(texto):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = nltk.word_tokenize(texto)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"\"\"\n",
    "This was not the map we found in Billy Boness chest, but\n",
    "an accurate copy, complete in all things-names and heights\n",
    "and soundings-with the single exception of the red crosses\n",
    "and the written notes.\n",
    "\"\"\"\n",
    "stemmed_text = stem_text(texto)\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88018131",
   "metadata": {},
   "source": [
    "**Segmentaci贸n de oraciones usando nltk**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def sentence_tokenize(texto):\n",
    "    return nltk.sent_tokenize(texto)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"This is a sentence. This is another sentence! Is this the third one?\"\n",
    "sentences = sentence_tokenize(texto)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899eed-ed22-4a9b-9101-c7e3aa8adeac",
   "metadata": {},
   "source": [
    "### **Normalizaci贸n en modelos de lenguaje (LLM)**\n",
    "\n",
    "Los modelos de lenguaje a gran escala (LLM) requieren, como parte de su preprocesamiento, una tokenizaci贸n y normalizaci贸n que asegure la coherencia y el aprovechamiento de patrones ling眉铆sticos. Aunque los sistemas basados en **BPE (Byte-Pair Encoding)** o en otras t茅cnicas de tokenizaci贸n bottom-up pueden prescindir de una normalizaci贸n adicional, en algunos contextos se opta por aplicar transformaciones adicionales que unifiquen la representaci贸n de los datos.\n",
    "\n",
    "##### **Tokenizaci贸n y normalizaci贸n en LLM**\n",
    "\n",
    "En modelos de lenguaje, la **tokenizaci贸n** es el primer paso para transformar el texto en una secuencia de tokens que pueda ser procesada por la red neuronal. La tokenizaci贸n puede implicar desde la simple divisi贸n en palabras hasta la descomposici贸n en subpalabras o caracteres. Durante este proceso, la normalizaci贸n puede involucrar pasos como el case folding, la eliminaci贸n de caracteres especiales o la unificaci贸n de ciertas variantes.\n",
    "\n",
    "El uso de algoritmos de tokenizaci贸n avanzados, como BPE, permite a los modelos manejar vocabularios extensos sin necesidad de tener una lista fija de palabras. Estos m茅todos identifican las subunidades m谩s frecuentes y las utilizan para representar palabras completas. Sin embargo, cuando se emplean modelos preentrenados, es habitual que el texto de entrada ya haya pasado por un proceso de normalizaci贸n est谩ndar que asegure la consistencia en la representaci贸n de las palabras.\n",
    "\n",
    "##### **Otros m茅todos de normalizaci贸n en LLM**\n",
    "\n",
    "Aunque la normalizaci贸n por case folding y la unificaci贸n de t茅rminos son comunes, en entornos LLM tambi茅n se aplican otros procesos como:\n",
    "\n",
    "- **Eliminaci贸n o transformaci贸n de caracteres especiales:**  \n",
    "  Algunos sistemas eliminan o convierten caracteres no alfab茅ticos para evitar que elementos como emoticonos, s铆mbolos o puntuaciones excesivas alteren el an谩lisis.\n",
    "  \n",
    "- **Normalizaci贸n Unicode:**  \n",
    "  En textos multiling眉es, es crucial normalizar las representaciones Unicode para que las variantes gr谩ficas de un mismo car谩cter sean interpretadas de forma id茅ntica.\n",
    "\n",
    "- **Segmentaci贸n en subpalabras:**  \n",
    "  La tokenizaci贸n en subpalabras permite que incluso palabras desconocidas se representen a partir de combinaciones de unidades m谩s peque帽as, lo que mejora la capacidad del modelo para generalizar.\n",
    "\n",
    "Estos procesos no solo optimizan la representaci贸n del lenguaje, sino que adem谩s permiten a los modelos de lenguaje captar mejor patrones y relaciones sem谩nticas, lo cual es fundamental para tareas complejas como la traducci贸n autom谩tica, la generaci贸n de texto o la respuesta a preguntas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb3b5-355f-4dd7-ba0a-4508e818219b",
   "metadata": {},
   "source": [
    "#### **Otras formas de normalizaci贸n y an谩lisis morfol贸gico**\n",
    "\n",
    "Adem谩s de los m茅todos ya descritos, existen otras estrategias de normalizaci贸n que se utilizan en el procesamiento de texto. Entre ellas destaca la unificaci贸n de variantes ortogr谩ficas o la transformaci贸n de diferentes formas de una misma palabra a una forma can贸nica. Este proceso, aunque puede implicar la p茅rdida de informaci贸n espec铆fica (como la distinci贸n entre \"US\" y \"USA\"), es 煤til para tareas en las que se desea agrupar informaci贸n similar.\n",
    "\n",
    "El an谩lisis morfol贸gico estudia la forma en la que se construyen las palabras a partir de **morfemas**, que son las unidades m铆nimas con significado. Se distinguen dos tipos principales:\n",
    "\n",
    "- **Stems (ra铆ces):** La parte central de la palabra que contiene el significado principal.  \n",
    "- **Affixes (afijos):** Elementos que se a帽aden a la ra铆z para modificar o matizar su significado (como sufijos o prefijos).\n",
    "\n",
    "Un ejemplo simple es la palabra \"cats\", que se compone del stem \"cat\" y el sufijo \"-s\". Un **morphological parser** descompone la palabra en estas partes, permitiendo identificar la ra铆z y aplicar procesos de normalizaci贸n o lematizaci贸n de manera m谩s precisa.\n",
    "\n",
    "El an谩lisis morfol贸gico no se limita 煤nicamente a la lematizaci贸n y el stemming, sino que tambi茅n puede incluir procesos que separan expl铆citamente los componentes de una palabra. Esto es especialmente 煤til en lenguajes con alta morfolog铆a. \n",
    "\n",
    "Por ejemplo, en el procesamiento de la palabra \"Jakonmax\", un sistema avanzado podr铆a identificar dos componentes: la ra铆z \"Jakon\" y el afijo \"max\". De forma similar, en \"Raiokon2, se distinguir铆an la ra铆z \"Raio\" y el afijo \"kon\". La representaci贸n de estos componentes podr铆a organizarse en un diccionario o estructura de datos que relacione cada palabra con su ra铆z y sus afijos, como se muestra a continuaci贸n:\n",
    "\n",
    "```plaintext\n",
    "{'Jakonmax': {'ra铆z': 'Jakon', 'afijo': 'max'}, 'Raiokon': {'ra铆z': 'Raio', 'afijo': 'kon'}}\n",
    "```\n",
    "Este ejemplo ilustra la direcci贸n en la que se pueden extender los procesos de normalizaci贸n para manejar de manera m谩s detallada la estructura interna de las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f79f0-b6fe-4541-b08e-5f6cb08324de",
   "metadata": {},
   "source": [
    "#### **Integraci贸n de conceptos**\n",
    "\n",
    "En la pr谩ctica, la combinaci贸n de normalizaci贸n, lematizaci贸n, stemming y segmentaci贸n de oraciones conforma la base del preprocesamiento de datos para una gran variedad de aplicaciones de NLP. Este conjunto de t茅cnicas permite:\n",
    "\n",
    "- **Reducir la dimensionalidad del vocabulario:**  \n",
    "  Al normalizar y unificar palabras, se evita que variaciones menores influyan en el desempe帽o de los modelos de b煤squeda o clasificaci贸n. Por ejemplo, convertir \"Woodchuck\" a \"woodchuck\" o unificar \"USA\" a \"US\" reduce la cantidad de tokens diferentes a procesar.\n",
    "\n",
    "- **Mejorar la precisi贸n de la b煤squeda y recuperaci贸n de informaci贸n:**  \n",
    "  Cuando un sistema de recuperaci贸n de informaci贸n enfrenta consultas, la normalizaci贸n asegura que se recuperen documentos relevantes aunque existan variaciones en la escritura de los t茅rminos. La lematizaci贸n permite agrupar formas verbales y nominales, incrementando la tasa de acierto en las respuestas.\n",
    "\n",
    "- **Optimizaci贸n en modelos de lenguaje a gran escala (LLM):**  \n",
    "  En los LLM, la calidad del preprocesamiento es determinante para el aprendizaje de patrones. La tokenizaci贸n, combinada con t茅cnicas de normalizaci贸n, garantiza que el modelo reciba informaci贸n homog茅nea, lo que mejora la capacidad de generalizaci贸n y reduce la complejidad en el entrenamiento.\n",
    "\n",
    "- **Manejo de idiomas morfol贸gicamente complejos:**  \n",
    "  Idiomas como el polaco o el ruso presentan una gran cantidad de variaciones morfol贸gicas. La lematizaci贸n y el an谩lisis morfol贸gico permiten tratar estas variantes de forma coherente, facilitando la construcci贸n de sistemas multiling眉es y el an谩lisis sem谩ntico.\n",
    "\n",
    "- **Facilitaci贸n de an谩lisis sint谩cticos y sem谩nticos:**  \n",
    "  La segmentaci贸n de oraciones es un paso esencial para descomponer un texto en unidades manejables, lo que permite aplicar modelos de an谩lisis sint谩ctico y sem谩ntico de manera m谩s efectiva. Cada oraci贸n segmentada se puede analizar individualmente, lo que resulta en una mejor identificaci贸n de patrones y relaciones dentro del texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01359a-784c-4f24-a177-2c85110b7a50",
   "metadata": {},
   "source": [
    "#### **Aplicaciones avanzadas**\n",
    "\n",
    "Si bien las t茅cnicas descritas previamente son ampliamente utilizadas, en entornos m谩s avanzados se han desarrollado m茅todos que combinan estas estrategias con t茅cnicas de aprendizaje autom谩tico. Por ejemplo, algunos sistemas modernos de NLP utilizan redes neuronales para aprender directamente la mejor forma de representar palabras a partir de grandes corpus de datos. Estas representaciones, conocidas como *embeddings*, pueden incorporar informaci贸n sobre la sintaxis, sem谩ntica y contexto en el que aparece cada palabra, lo que a su vez mejora la calidad de tareas como la traducci贸n autom谩tica o la generaci贸n de texto.\n",
    "\n",
    "##### **Normalizaci贸n y embeddings**\n",
    "\n",
    "En la generaci贸n de *word embeddings* o *subword embeddings*, la normalizaci贸n previa es fundamental para reducir el ruido y garantizar que el modelo no distinga entre variantes irrelevantes. La eliminaci贸n de diferencias de may煤sculas y min煤sculas, la unificaci贸n de t茅rminos y la segmentaci贸n en subpalabras contribuyen a obtener representaciones vectoriales m谩s robustas y generalizables.\n",
    "\n",
    "##### **Normalizaci贸n en el contexto de LLM**\n",
    "\n",
    "En los modelos de lenguaje de gran escala, la normalizaci贸n es parte integral del proceso de tokenizaci贸n. Los algoritmos modernos de tokenizaci贸n, como el BPE, realizan un an谩lisis estad铆stico de las frecuencias de subpalabras y aprenden una representaci贸n que minimiza la cantidad de tokens necesarios para representar un texto. Aunque estos m茅todos pueden reducir la necesidad de normalizaciones \"manuales\", en muchos casos se aplican transformaciones adicionales (por ejemplo, convertir todo el texto a min煤sculas) para asegurar la coherencia en el entrenamiento del modelo.\n",
    "\n",
    "##### **Consideraciones sobre errores en el preprocesamiento**\n",
    "\n",
    "A pesar de los beneficios, es importante tener en cuenta que las t茅cnicas de normalizaci贸n pueden introducir errores. Por ejemplo, el stemming puede llevar a sobregeneralizaciones, mientras que la lematizaci贸n, aunque m谩s precisa, puede requerir mayor poder computacional y depender de recursos externos que deben actualizarse peri贸dicamente. La elecci贸n entre lematizaci贸n y stemming depender谩 del equilibrio deseado entre precisi贸n y velocidad, as铆 como del tipo de tarea que se est茅 abordando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675815a1-e0db-4cf0-a12f-a885b7d1095c",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "**1) Normalizaci贸n Unicode y espacios**\n",
    "\n",
    "**Objetivo:** unificar representaciones de texto y limpiar espacios.\n",
    "\n",
    "* **Entrada:** una lista corta de oraciones que incluya tildes, emojis, \"quotes\" curvas, y caracteres raros (por ejemplo, `\\u00A0`, `\\t`).\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Convierte todo a min煤sculas.\n",
    "  2. Aplica normalizaci贸n Unicode (forma can贸nica que unifique ligaduras y s铆mbolos equivalentes).\n",
    "  3. Elimina caracteres de control y espacios no imprimibles.\n",
    "  4. Compacta espacios m煤ltiples a un solo espacio.\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * Cantidad de caracteres antes y despu茅s.\n",
    "  * Lista breve de transformaciones detectadas (ej.: \"comillas curvas -> comillas rectas\", \"NBSP -> espacio\").\n",
    "  * 3 ejemplos \"antes -> despu茅s\".\n",
    "\n",
    "\n",
    "**2) Limpieza b谩sica: URLs, correos y n煤meros**\n",
    "\n",
    "**Objetivo:** sanear entidades comunes del texto.\n",
    "\n",
    "* **Entrada:** oraciones con URLs, correos, n煤meros, hashtags y menciones.\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Reemplaza las **URLs** por un marcador claro (por ejemplo, `<URL>`).\n",
    "  2. Reemplaza los **correos** por `<EMAIL>`.\n",
    "  3. Reemplaza n煤meros por `<NUM>` manteniendo signos decimales y separadores simples.\n",
    "  4. Conserva tildes y e帽es (no borres diacr铆ticos).\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * Recuento de reemplazos por tipo (cu谩ntas URLs, correos, n煤meros).\n",
    "  * Tabla corta con 5 ejemplos \"antes -> despu茅s\".\n",
    "  * Una frase explicando por qu茅 es 煤til usar marcadores en vez de borrar.\n",
    "\n",
    "**3) Acentos: conservar vs. quitar (impacto en vocabulario)**\n",
    "\n",
    "**Objetivo:** comparar efectos de remover tildes sobre el vocabulario.\n",
    "\n",
    "* **Entrada:** p谩rrafos en espa帽ol con palabras acentuadas (canci贸n, acci贸n, 煤til) y sin acentuar (accion, util) mezcladas.\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Genera **Versi贸n A** (con tildes) y **Versi贸n B** (sin tildes).\n",
    "  2. Tokeniza en palabras con una regla simple (separar por espacios y signos).\n",
    "  3. Calcula tama帽o del vocabulario y top-10 de tokens m谩s frecuentes en A y B.\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * Tama帽o de vocabulario en A vs B.\n",
    "  * Diferencias notables (ej.: \"canci贸n\" y \"cancion\" se fusionan).\n",
    "  * Una recomendaci贸n breve de cu谩ndo conviene conservar vs. quitar tildes.\n",
    "\n",
    "\n",
    "**4) Lematizaci贸n vs. stemming (espa帽ol)**\n",
    "\n",
    "**Objetivo:** ver diferencias pr谩cticas entre lematizar y \"recortar\" palabras (stemming).\n",
    "\n",
    "* **Entrada:** 10-15 oraciones con verbos conjugados, plurales y g茅neros (ej.: \"corriendo\", \"habl谩bamos\", \"casas\", \"ni帽as\", \"r谩pidamente\").\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Aplica un **lematizador** (que devuelva formas can贸nicas).\n",
    "  2. Aplica un **stemmer** (que recorte sufijos).\n",
    "  3. Crea dos columnas de salida: Lema vs. Ra铆z (stem) por token.\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * 8-10 pares token -> (lema, stem) que ilustren diferencias.\n",
    "  * Errores t铆picos del stemmer (palabras \"rotas\").\n",
    "  * Una frase sobre qu茅 t茅cnica prefieres para an谩lisis sem谩ntico.\n",
    "\n",
    "\n",
    "**5) Diccionario de sustituciones (estandarizaci贸n ligera)**\n",
    "\n",
    "**Objetivo:** unificar variantes y abreviaturas frecuentes.\n",
    "\n",
    "* **Entrada:** oraciones con comillas curvas/rectas, unidades (km, kms, kil贸metros), abreviaturas (aprox., aprox, aprox), meses (ene., enero), y variantes de comillas (芦 禄, \" \").\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Define un **peque帽o diccionario** de sustituciones (5-10 entradas) para estandarizar: comillas, unidades, abreviaturas, meses.\n",
    "  2. Aplica el diccionario **antes** de tokenizar.\n",
    "  3. Verifica que se reduzcan variantes (ej.: \"kms\" y \"km\").\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * Tabla con 5 sustituciones clave (original -> est谩ndar).\n",
    "  * Recuento de ocurrencias unificadas (cu谩ntas veces aplic贸 cada regla).\n",
    "  * Un ejemplo donde la estandarizaci贸n evita ambig眉edad en el an谩lisis.\n",
    "\n",
    "\n",
    "**6) Tokenizaci贸n con reglas simples (emojis y palabras compuestas)**\n",
    "\n",
    "**Objetivo:** crear una tokenizaci贸n sensible a emojis y t茅rminos compuestos.\n",
    "\n",
    "* **Entrada:** oraciones que incluyan emojis , palabras con guion (bien-estar), tecnicismos (DevSecOps), contracciones o ap贸strofes (d'Artagnan), y fechas/horas.\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Define reglas sencillas para:\n",
    "\n",
    "     * Mantener **emojis** como tokens individuales.\n",
    "     * Conservar **palabras compuestas** y tecnicismos (no partir \"DevSecOps\").\n",
    "     * Separar puntuaci贸n com煤n sin perder contexto b谩sico.\n",
    "  2. Aplica las reglas y lista tokens para 5 oraciones.\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * 5 l铆neas con su lista de tokens.\n",
    "  * 2 casos l铆mite y c贸mo los resolviste (ej.: \"bien-estar\", \"d'Artagnan\").\n",
    "  * Una frase justificando por qu茅 mantener emojis puede ser 煤til.\n",
    "\n",
    "\n",
    "**7) Mini-pipeline reproducible y m茅tricas simples**\n",
    "\n",
    "**Objetivo:** encadenar pasos b谩sicos y medir su efecto.\n",
    "\n",
    "* **Entrada:** un peque帽o conjunto (10-20 oraciones) variadas (con URLs, tildes, n煤meros, emojis).\n",
    "* **Tareas (en este orden):**\n",
    "\n",
    "  1. Min煤sculas -> Normalizaci贸n Unicode -> Limpieza b谩sica (URLs, correos, n煤meros) -> Diccionario de sustituciones -> Tokenizaci贸n.\n",
    "  2. Calcula por oraci贸n:\n",
    "\n",
    "     * Longitud original y longitud normalizada (en caracteres).\n",
    "     * N煤mero de tokens y n煤mero de tokens 煤nicos.\n",
    "  3. Exporta un peque帽o resumen en formato tabular (por ejemplo, CSV con columnas: `id, len_original, len_norm, n_tokens, n_unicos`).\n",
    "* **Qu茅 reportar (texto):**\n",
    "\n",
    "  * 3 filas de ejemplo con n煤meros (escritas como texto).\n",
    "  * Un p谩rrafo corto explicando qu茅 paso del pipeline tuvo mayor impacto y por qu茅.\n",
    "  * Una nota sobre c贸mo repetir exactamente el mismo procesamiento (orden de pasos y par谩metros fijos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680efe0-346a-46c0-bec6-ade8f8077d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
