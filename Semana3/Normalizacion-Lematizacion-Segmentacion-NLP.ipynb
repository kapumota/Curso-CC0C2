{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b7ca14-d87b-4b16-8ada-608833a5c2e2",
   "metadata": {},
   "source": [
    "### **Normalización de palabras**\n",
    "\n",
    "La **normalización de palabras** es el proceso de estandarizar palabras o tokens para que sigan un formato uniforme. Un ejemplo común es el **case folding**, que consiste en convertir todas las letras a minúsculas. Este enfoque permite tratar términos como \"Woodchuck\" y \"woodchuck\" como idénticos, lo cual es particularmente útil en tareas como la recuperación de información o el reconocimiento de voz.\n",
    "\n",
    "Sin embargo, en tareas como el análisis de sentimientos, la clasificación de textos, la extracción de información y la traducción automática, la distinción entre mayúsculas y minúsculas puede ser crucial. Por ejemplo, diferenciar entre \"US\" (Estados Unidos) y \"us\" (nosotros) puede ser más importante que la simplificación proporcionada por el **case folding**. Por ello, en estos contextos, suele preferirse conservar la distinción entre mayúsculas y minúsculas. En algunos casos, se generan tanto versiones diferenciadas como versiones completamente en minúsculas para modelos de lenguaje, dependiendo de la tarea específica.\n",
    "\n",
    "Los sistemas que emplean **BPE** u otros métodos de tokenización bottom-up pueden prescindir de una normalización adicional de las palabras. Sin embargo, en otros sistemas de **NLP**, puede ser beneficioso aplicar normalizaciones más avanzadas, como unificar diferentes formas de una palabra, por ejemplo, \"USA\" y \"US\" o \"uh-huh\" y \"uhhuh\". Aunque esta estandarización puede implicar la pérdida de cierta información ortográfica, puede resultar valiosa en algunos casos.\n",
    "\n",
    "Por ejemplo, en la recuperación o extracción de información relacionada con \"US.\", podríamos querer identificar datos relevantes tanto si el documento menciona \"US\" como \"USA\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008113a6-9bb2-41a6-bcf9-fb3dde3f33c7",
   "metadata": {},
   "source": [
    "### **Lematización**\n",
    "\n",
    "En procesamiento de lenguaje natural, a menudo es deseable que dos formas morfológicamente diferentes de una palabra se traten como equivalentes. Por ejemplo, en una búsqueda web, un usuario podría escribir \"woodchucks\", pero un sistema eficiente debería ser capaz de devolver páginas que mencionen \"woodchuck\" sin la \"s\". Esto es especialmente relevante en idiomas morfológicamente complejos como el polaco, donde una palabra puede cambiar de forma según su función en la oración. Por ejemplo, \"Warsaw\" toma diferentes terminaciones según el caso gramatical: **\"Warszawa\"** como sujeto, **\"w Warszawie\"** para \"en Varsovia\" o **\"do Warszawy\"** para \"hacia Varsovia\", entre otras variantes.\n",
    "\n",
    "La **lematización** es el proceso de reducir las palabras a su forma base o **lema**, independientemente de sus variaciones morfológicas. Por ejemplo, \"am\", \"are\" e \"is\" tienen como lema común \"be\", mientras que \"dinner\" y \"dinners\" comparten el lema \"dinner\". Lematizar correctamente ayuda a agrupar menciones de palabras en diferentes formas, como en el caso de \"Warsaw\" en polaco. La forma lematizada de una oración como **\"He is reading detective stories\"** sería **\"He be read detective story\"**, eliminando la flexión verbal y el plural.\n",
    "\n",
    "Los métodos más sofisticados para la lematización implican un análisis morfológico detallado. La **morfología** es el estudio de la estructura de las palabras y cómo estas se forman a partir de unidades más pequeñas con significado, llamadas **morfemas**. Existen dos tipos principales de morfemas: \n",
    "- **Stems**: el núcleo de la palabra, que aporta el significado principal.\n",
    "- **Affixes**: elementos adicionales que modifican el significado o la función de la palabra.\n",
    "\n",
    "Por ejemplo, la palabra **\"fox\"** consta de un solo morfema (\"fox\"), mientras que **\"cats\"** se compone de dos: el morfema raíz **\"cat\"** y el sufijo **\"-s\"** que indica plural. Un **analizador morfológico** (morphological parser) descompone palabras como \"cats\" en estos morfemas, facilitando tareas como la lematización y la comprensión del significado en NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafd45d-e3c3-4658-90f2-dc7fe7df9575",
   "metadata": {},
   "source": [
    "#### Stemming: The Porter Stemmer\n",
    "\n",
    "Los algoritmos de **lematización** pueden ser complejos y computacionalmente costosos. Por esta razón, en algunos casos se emplea un método más simple basado en la eliminación de afijos finales de las palabras, conocido como **stemming**. A diferencia de la lematización, el stemming no busca identificar la forma base de la palabra con precisión gramatical, sino simplemente recortar sufijos comunes. \n",
    "\n",
    "Un ejemplo clásico es el **Porter Stemmer**, que cuando se aplica al siguiente párrafo:\n",
    "\n",
    "```plaintext\n",
    "This was not the map we found in Billy Bones’s chest, but\n",
    "an accurate copy, complete in all things—names and heights\n",
    "and soundings—with the single exception of the red crosses\n",
    "and the written notes.\n",
    "```\n",
    "\n",
    "produce la siguiente salida:\n",
    "\n",
    "```plaintext\n",
    "Thi wa not the map we found in Billi Bone s chest but an\n",
    "accur copi complet in all thing name and height and sound\n",
    "with the singl except of the red cross and the written note\n",
    "```\n",
    "\n",
    "El algoritmo se basa en una serie de reglas de reescritura que se aplican en secuencia, donde la salida de cada paso se convierte en la entrada del siguiente.\n",
    "\n",
    "Algunas reglas de muestra (más en [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)):\n",
    "\n",
    "| **Regla**        | **Ejemplo**                        |\n",
    "|------------------|------------------------------------|\n",
    "| **ATIONAL -> ATE**| (relational -> relate)              |\n",
    "| **SSES -> SS**    | (grasses -> grass)                  |\n",
    "\n",
    "Los **stemmers** simples pueden ser útiles en situaciones donde es necesario agrupar diferentes variantes de una palabra bajo una forma común. Sin embargo, su uso en sistemas modernos ha disminuido, ya que pueden cometer errores tanto de **sobregeneralización** (por ejemplo, reduciendo *policy* a *police*) como de **subgeneralización** (no reduciendo *European* a *Europe*). Debido a estas limitaciones, en muchas aplicaciones de NLP se prefiere la lematización, que proporciona resultados más precisos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61906511-9d65-42cc-9ffb-097b01334704",
   "metadata": {},
   "source": [
    "#### **Segmentación de oraciones**\n",
    "\n",
    "La **segmentación de oraciones** es un paso fundamental en el procesamiento de texto. Las señales más útiles para dividir un texto en oraciones son los signos de puntuación, como los puntos, signos de interrogación y signos de exclamación. Los signos de interrogación y exclamación suelen ser indicadores claros de los límites de una oración. Sin embargo, los puntos (**\".\"**) pueden ser más ambiguos, ya que pueden marcar el final de una oración o formar parte de abreviaciones como *\"Mr.\"* o *\"Inc.\"*. Debido a esta ambigüedad, la segmentación de oraciones y la tokenización de palabras a menudo se abordan conjuntamente.\n",
    "\n",
    "En general, los métodos de segmentación de oraciones funcionan determinando, ya sea mediante reglas o aprendizaje automático, si un punto actúa como un marcador de fin de oración o si forma parte de una palabra. Un diccionario de abreviaciones puede ser útil para identificar si un punto pertenece a una abreviatura de uso común. Estos diccionarios pueden construirse manualmente o aprenderse mediante técnicas de machine learning, al igual que los modelos que realizan la segmentación de oraciones.\n",
    "\n",
    "Por ejemplo, en el **[Stanford CoreNLP toolkit](https://stanfordnlp.github.io/CoreNLP/)**, la segmentación de oraciones se basa en reglas y es una consecuencia determinista de la tokenización. Una oración termina cuando una puntuación de final de oración (*\".\", \"!\", \"?\"*) no está agrupada con otros caracteres dentro de un solo token (como en el caso de abreviaturas o números), y puede estar seguida opcionalmente por comillas finales o corchetes adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854804be",
   "metadata": {},
   "source": [
    "**Normalización de palabras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c338e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Folding\n",
    "def normalize_case(text):\n",
    "    \"\"\"Convierte todo el texto a minúsculas para la normalización.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"Woodchuck\"\n",
    "normalized_text = normalize_case(texto)\n",
    "print(normalized_text)  # Salida: \"woodchuck\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6204d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unify_terms(texto):\n",
    "    \"\"\"Reemplaza diferentes variantes de un término por una versión estándar.\"\"\"\n",
    "    term_mapping = {\n",
    "        \"USA\": \"US\",\n",
    "        \"uh-huh\": \"uhhuh\"\n",
    "    }\n",
    "    words = texto.split()\n",
    "    unified_words = [term_mapping.get(word, word) for word in words]\n",
    "    return ' '.join(unified_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"The USA has many dialects. Uh-huh, that's true.\"\n",
    "unified_text = unify_terms(texto)\n",
    "print(unified_text)  # Salida: \"The US has many dialects. uhhuh, that's true.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427734cf",
   "metadata": {},
   "source": [
    "**Lematización usando nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_text(texto):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = texto.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"He is reading detective stories\"\n",
    "lemmatized_text = lemmatize_text(texto)\n",
    "print(lemmatized_text)  # Salida: \"He be read detective story\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3c847",
   "metadata": {},
   "source": [
    "**Stemming usando el Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "\n",
    "def stem_text(texto):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = nltk.word_tokenize(texto)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"\"\"\n",
    "This was not the map we found in Billy Bones’s chest, but\n",
    "an accurate copy, complete in all things-names and heights\n",
    "and soundings-with the single exception of the red crosses\n",
    "and the written notes.\n",
    "\"\"\"\n",
    "stemmed_text = stem_text(texto)\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88018131",
   "metadata": {},
   "source": [
    "**Segmentación de oraciones usando nltk**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def sentence_tokenize(texto):\n",
    "    return nltk.sent_tokenize(texto)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"This is a sentence. This is another sentence! Is this the third one?\"\n",
    "sentences = sentence_tokenize(texto)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899eed-ed22-4a9b-9101-c7e3aa8adeac",
   "metadata": {},
   "source": [
    "### **Normalización en modelos de lenguaje (LLM)**\n",
    "\n",
    "Los modelos de lenguaje a gran escala (LLM) requieren, como parte de su preprocesamiento, una tokenización y normalización que asegure la coherencia y el aprovechamiento de patrones lingüísticos. Aunque los sistemas basados en **BPE (Byte-Pair Encoding)** o en otras técnicas de tokenización bottom-up pueden prescindir de una normalización adicional, en algunos contextos se opta por aplicar transformaciones adicionales que unifiquen la representación de los datos.\n",
    "\n",
    "##### **Tokenización y normalización en LLM**\n",
    "\n",
    "En modelos de lenguaje, la **tokenización** es el primer paso para transformar el texto en una secuencia de tokens que pueda ser procesada por la red neuronal. La tokenización puede implicar desde la simple división en palabras hasta la descomposición en subpalabras o caracteres. Durante este proceso, la normalización puede involucrar pasos como el case folding, la eliminación de caracteres especiales o la unificación de ciertas variantes.\n",
    "\n",
    "El uso de algoritmos de tokenización avanzados, como BPE, permite a los modelos manejar vocabularios extensos sin necesidad de tener una lista fija de palabras. Estos métodos identifican las subunidades más frecuentes y las utilizan para representar palabras completas. Sin embargo, cuando se emplean modelos preentrenados, es habitual que el texto de entrada ya haya pasado por un proceso de normalización estándar que asegure la consistencia en la representación de las palabras.\n",
    "\n",
    "##### **Otros métodos de normalización en LLM**\n",
    "\n",
    "Aunque la normalización por case folding y la unificación de términos son comunes, en entornos LLM también se aplican otros procesos como:\n",
    "\n",
    "- **Eliminación o transformación de caracteres especiales:**  \n",
    "  Algunos sistemas eliminan o convierten caracteres no alfabéticos para evitar que elementos como emoticonos, símbolos o puntuaciones excesivas alteren el análisis.\n",
    "  \n",
    "- **Normalización Unicode:**  \n",
    "  En textos multilingües, es crucial normalizar las representaciones Unicode para que las variantes gráficas de un mismo carácter sean interpretadas de forma idéntica.\n",
    "\n",
    "- **Segmentación en subpalabras:**  \n",
    "  La tokenización en subpalabras permite que incluso palabras desconocidas se representen a partir de combinaciones de unidades más pequeñas, lo que mejora la capacidad del modelo para generalizar.\n",
    "\n",
    "Estos procesos no solo optimizan la representación del lenguaje, sino que además permiten a los modelos de lenguaje captar mejor patrones y relaciones semánticas, lo cual es fundamental para tareas complejas como la traducción automática, la generación de texto o la respuesta a preguntas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb3b5-355f-4dd7-ba0a-4508e818219b",
   "metadata": {},
   "source": [
    "#### **Otras formas de normalización y análisis morfológico**\n",
    "\n",
    "Además de los métodos ya descritos, existen otras estrategias de normalización que se utilizan en el procesamiento de texto. Entre ellas destaca la unificación de variantes ortográficas o la transformación de diferentes formas de una misma palabra a una forma canónica. Este proceso, aunque puede implicar la pérdida de información específica (como la distinción entre \"US\" y \"USA\"), es útil para tareas en las que se desea agrupar información similar.\n",
    "\n",
    "El análisis morfológico estudia la forma en la que se construyen las palabras a partir de **morfemas**, que son las unidades mínimas con significado. Se distinguen dos tipos principales:\n",
    "\n",
    "- **Stems (raíces):** La parte central de la palabra que contiene el significado principal.  \n",
    "- **Affixes (afijos):** Elementos que se añaden a la raíz para modificar o matizar su significado (como sufijos o prefijos).\n",
    "\n",
    "Un ejemplo simple es la palabra \"cats\", que se compone del stem \"cat\" y el sufijo \"-s\". Un **morphological parser** descompone la palabra en estas partes, permitiendo identificar la raíz y aplicar procesos de normalización o lematización de manera más precisa.\n",
    "\n",
    "El análisis morfológico no se limita únicamente a la lematización y el stemming, sino que también puede incluir procesos que separan explícitamente los componentes de una palabra. Esto es especialmente útil en lenguajes con alta morfología. \n",
    "\n",
    "Por ejemplo, en el procesamiento de la palabra \"Jakonmax\", un sistema avanzado podría identificar dos componentes: la raíz \"Jakon\" y el afijo \"max\". De forma similar, en \"Raiokon2, se distinguirían la raíz \"Raio\" y el afijo \"kon\". La representación de estos componentes podría organizarse en un diccionario o estructura de datos que relacione cada palabra con su raíz y sus afijos, como se muestra a continuación:\n",
    "\n",
    "```plaintext\n",
    "{'Jakonmax': {'raíz': 'Jakon', 'afijo': 'max'}, 'Raiokon': {'raíz': 'Raio', 'afijo': 'kon'}}\n",
    "```\n",
    "Este ejemplo ilustra la dirección en la que se pueden extender los procesos de normalización para manejar de manera más detallada la estructura interna de las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f79f0-b6fe-4541-b08e-5f6cb08324de",
   "metadata": {},
   "source": [
    "#### **Integración de conceptos**\n",
    "\n",
    "En la práctica, la combinación de normalización, lematización, stemming y segmentación de oraciones conforma la base del preprocesamiento de datos para una gran variedad de aplicaciones de NLP. Este conjunto de técnicas permite:\n",
    "\n",
    "- **Reducir la dimensionalidad del vocabulario:**  \n",
    "  Al normalizar y unificar palabras, se evita que variaciones menores influyan en el desempeño de los modelos de búsqueda o clasificación. Por ejemplo, convertir \"Woodchuck\" a \"woodchuck\" o unificar \"USA\" a \"US\" reduce la cantidad de tokens diferentes a procesar.\n",
    "\n",
    "- **Mejorar la precisión de la búsqueda y recuperación de información:**  \n",
    "  Cuando un sistema de recuperación de información enfrenta consultas, la normalización asegura que se recuperen documentos relevantes aunque existan variaciones en la escritura de los términos. La lematización permite agrupar formas verbales y nominales, incrementando la tasa de acierto en las respuestas.\n",
    "\n",
    "- **Optimización en modelos de lenguaje a gran escala (LLM):**  \n",
    "  En los LLM, la calidad del preprocesamiento es determinante para el aprendizaje de patrones. La tokenización, combinada con técnicas de normalización, garantiza que el modelo reciba información homogénea, lo que mejora la capacidad de generalización y reduce la complejidad en el entrenamiento.\n",
    "\n",
    "- **Manejo de idiomas morfológicamente complejos:**  \n",
    "  Idiomas como el polaco o el ruso presentan una gran cantidad de variaciones morfológicas. La lematización y el análisis morfológico permiten tratar estas variantes de forma coherente, facilitando la construcción de sistemas multilingües y el análisis semántico.\n",
    "\n",
    "- **Facilitación de análisis sintácticos y semánticos:**  \n",
    "  La segmentación de oraciones es un paso esencial para descomponer un texto en unidades manejables, lo que permite aplicar modelos de análisis sintáctico y semántico de manera más efectiva. Cada oración segmentada se puede analizar individualmente, lo que resulta en una mejor identificación de patrones y relaciones dentro del texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01359a-784c-4f24-a177-2c85110b7a50",
   "metadata": {},
   "source": [
    "#### **Aplicaciones avanzadas**\n",
    "\n",
    "Si bien las técnicas descritas previamente son ampliamente utilizadas, en entornos más avanzados se han desarrollado métodos que combinan estas estrategias con técnicas de aprendizaje automático. Por ejemplo, algunos sistemas modernos de NLP utilizan redes neuronales para aprender directamente la mejor forma de representar palabras a partir de grandes corpus de datos. Estas representaciones, conocidas como *embeddings*, pueden incorporar información sobre la sintaxis, semántica y contexto en el que aparece cada palabra, lo que a su vez mejora la calidad de tareas como la traducción automática o la generación de texto.\n",
    "\n",
    "##### **Normalización y embeddings**\n",
    "\n",
    "En la generación de *word embeddings* o *subword embeddings*, la normalización previa es fundamental para reducir el ruido y garantizar que el modelo no distinga entre variantes irrelevantes. La eliminación de diferencias de mayúsculas y minúsculas, la unificación de términos y la segmentación en subpalabras contribuyen a obtener representaciones vectoriales más robustas y generalizables.\n",
    "\n",
    "##### **Normalización en el contexto de LLM**\n",
    "\n",
    "En los modelos de lenguaje de gran escala, la normalización es parte integral del proceso de tokenización. Los algoritmos modernos de tokenización, como el BPE, realizan un análisis estadístico de las frecuencias de subpalabras y aprenden una representación que minimiza la cantidad de tokens necesarios para representar un texto. Aunque estos métodos pueden reducir la necesidad de normalizaciones \"manuales\", en muchos casos se aplican transformaciones adicionales (por ejemplo, convertir todo el texto a minúsculas) para asegurar la coherencia en el entrenamiento del modelo.\n",
    "\n",
    "##### **Consideraciones sobre errores en el preprocesamiento**\n",
    "\n",
    "A pesar de los beneficios, es importante tener en cuenta que las técnicas de normalización pueden introducir errores. Por ejemplo, el stemming puede llevar a sobregeneralizaciones, mientras que la lematización, aunque más precisa, puede requerir mayor poder computacional y depender de recursos externos que deben actualizarse periódicamente. La elección entre lematización y stemming dependerá del equilibrio deseado entre precisión y velocidad, así como del tipo de tarea que se esté abordando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675815a1-e0db-4cf0-a12f-a885b7d1095c",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "**1) Normalización Unicode y espacios**\n",
    "\n",
    "**Objetivo:** unificar representaciones de texto y limpiar espacios.\n",
    "\n",
    "* **Entrada:** una lista corta de oraciones que incluya tildes, emojis, \"quotes\" curvas, y caracteres raros (por ejemplo, `\\u00A0`, `\\t`).\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Convierte todo a minúsculas.\n",
    "  2. Aplica normalización Unicode (forma canónica que unifique ligaduras y símbolos equivalentes).\n",
    "  3. Elimina caracteres de control y espacios no imprimibles.\n",
    "  4. Compacta espacios múltiples a un solo espacio.\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * Cantidad de caracteres antes y después.\n",
    "  * Lista breve de transformaciones detectadas (ej.: \"comillas curvas -> comillas rectas\", \"NBSP -> espacio\").\n",
    "  * 3 ejemplos \"antes -> después\".\n",
    "\n",
    "\n",
    "**2) Limpieza básica: URLs, correos y números**\n",
    "\n",
    "**Objetivo:** sanear entidades comunes del texto.\n",
    "\n",
    "* **Entrada:** oraciones con URLs, correos, números, hashtags y menciones.\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Reemplaza las **URLs** por un marcador claro (por ejemplo, `<URL>`).\n",
    "  2. Reemplaza los **correos** por `<EMAIL>`.\n",
    "  3. Reemplaza números por `<NUM>` manteniendo signos decimales y separadores simples.\n",
    "  4. Conserva tildes y eñes (no borres diacríticos).\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * Recuento de reemplazos por tipo (cuántas URLs, correos, números).\n",
    "  * Tabla corta con 5 ejemplos \"antes -> después\".\n",
    "  * Una frase explicando por qué es útil usar marcadores en vez de borrar.\n",
    "\n",
    "**3) Acentos: conservar vs. quitar (impacto en vocabulario)**\n",
    "\n",
    "**Objetivo:** comparar efectos de remover tildes sobre el vocabulario.\n",
    "\n",
    "* **Entrada:** párrafos en español con palabras acentuadas (canción, acción, útil) y sin acentuar (accion, util) mezcladas.\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Genera **Versión A** (con tildes) y **Versión B** (sin tildes).\n",
    "  2. Tokeniza en palabras con una regla simple (separar por espacios y signos).\n",
    "  3. Calcula tamaño del vocabulario y top-10 de tokens más frecuentes en A y B.\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * Tamaño de vocabulario en A vs B.\n",
    "  * Diferencias notables (ej.: \"canción\" y \"cancion\" se fusionan).\n",
    "  * Una recomendación breve de cuándo conviene conservar vs. quitar tildes.\n",
    "\n",
    "\n",
    "**4) Lematización vs. stemming (español)**\n",
    "\n",
    "**Objetivo:** ver diferencias prácticas entre lematizar y \"recortar\" palabras (stemming).\n",
    "\n",
    "* **Entrada:** 10-15 oraciones con verbos conjugados, plurales y géneros (ej.: \"corriendo\", \"hablábamos\", \"casas\", \"niñas\", \"rápidamente\").\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Aplica un **lematizador** (que devuelva formas canónicas).\n",
    "  2. Aplica un **stemmer** (que recorte sufijos).\n",
    "  3. Crea dos columnas de salida: Lema vs. Raíz (stem) por token.\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * 8-10 pares token -> (lema, stem) que ilustren diferencias.\n",
    "  * Errores típicos del stemmer (palabras \"rotas\").\n",
    "  * Una frase sobre qué técnica prefieres para análisis semántico.\n",
    "\n",
    "\n",
    "**5) Diccionario de sustituciones (estandarización ligera)**\n",
    "\n",
    "**Objetivo:** unificar variantes y abreviaturas frecuentes.\n",
    "\n",
    "* **Entrada:** oraciones con comillas curvas/rectas, unidades (km, kms, kilómetros), abreviaturas (aprox., aprox, aprox), meses (ene., enero), y variantes de comillas (« », \" \").\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Define un **pequeño diccionario** de sustituciones (5-10 entradas) para estandarizar: comillas, unidades, abreviaturas, meses.\n",
    "  2. Aplica el diccionario **antes** de tokenizar.\n",
    "  3. Verifica que se reduzcan variantes (ej.: \"kms\" y \"km\").\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * Tabla con 5 sustituciones clave (original -> estándar).\n",
    "  * Recuento de ocurrencias unificadas (cuántas veces aplicó cada regla).\n",
    "  * Un ejemplo donde la estandarización evita ambigüedad en el análisis.\n",
    "\n",
    "\n",
    "**6) Tokenización con reglas simples (emojis y palabras compuestas)**\n",
    "\n",
    "**Objetivo:** crear una tokenización sensible a emojis y términos compuestos.\n",
    "\n",
    "* **Entrada:** oraciones que incluyan emojis 🙂, palabras con guion (bien-estar), tecnicismos (DevSecOps), contracciones o apóstrofes (d'Artagnan), y fechas/horas.\n",
    "* **Tareas:**\n",
    "\n",
    "  1. Define reglas sencillas para:\n",
    "\n",
    "     * Mantener **emojis** como tokens individuales.\n",
    "     * Conservar **palabras compuestas** y tecnicismos (no partir \"DevSecOps\").\n",
    "     * Separar puntuación común sin perder contexto básico.\n",
    "  2. Aplica las reglas y lista tokens para 5 oraciones.\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * 5 líneas con su lista de tokens.\n",
    "  * 2 casos límite y cómo los resolviste (ej.: \"bien-estar\", \"d'Artagnan\").\n",
    "  * Una frase justificando por qué mantener emojis puede ser útil.\n",
    "\n",
    "\n",
    "**7) Mini-pipeline reproducible y métricas simples**\n",
    "\n",
    "**Objetivo:** encadenar pasos básicos y medir su efecto.\n",
    "\n",
    "* **Entrada:** un pequeño conjunto (10-20 oraciones) variadas (con URLs, tildes, números, emojis).\n",
    "* **Tareas (en este orden):**\n",
    "\n",
    "  1. Minúsculas -> Normalización Unicode -> Limpieza básica (URLs, correos, números) -> Diccionario de sustituciones -> Tokenización.\n",
    "  2. Calcula por oración:\n",
    "\n",
    "     * Longitud original y longitud normalizada (en caracteres).\n",
    "     * Número de tokens y número de tokens únicos.\n",
    "  3. Exporta un pequeño resumen en formato tabular (por ejemplo, CSV con columnas: `id, len_original, len_norm, n_tokens, n_unicos`).\n",
    "* **Qué reportar (texto):**\n",
    "\n",
    "  * 3 filas de ejemplo con números (escritas como texto).\n",
    "  * Un párrafo corto explicando qué paso del pipeline tuvo mayor impacto y por qué.\n",
    "  * Una nota sobre cómo repetir exactamente el mismo procesamiento (orden de pasos y parámetros fijos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680efe0-346a-46c0-bec6-ade8f8077d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
