{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10253d40",
   "metadata": {},
   "source": [
    "### **Optimización de inferencia en LLM**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d44847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Usa un modelo pequeño para las demostraciones\n",
    "MODEL_NAME = \"gpt2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6d0b7",
   "metadata": {},
   "source": [
    "### **Desafíos de la inferencia**\n",
    "\n",
    "En producción (o incluso en un laboratorio con Docker) nos preocupan:\n",
    "\n",
    "- **Latencia promedio** (p50): tiempo medio por petición.\n",
    "- **Latencia en la cola** (p95, p99): cuánto sufren los *tail users*.\n",
    "- **Throughput**: peticiones/segundo que somos capaces de sostener.\n",
    "- **Costo**: tiempo de CPU/GPU, energía y costo por token generado.\n",
    "- **Uso de memoria**:\n",
    "  - Memoria estática: pesos del modelo.\n",
    "  - Memoria dinámica: activaciones, cachés, buffers internos.\n",
    "\n",
    "En un LLM autoregresivo:\n",
    "\n",
    "- La complejidad de atención por token es normalmente `O(L * d)` por cabecera, donde:\n",
    "  - `L` = longitud de contexto (prompt + tokens ya generados).\n",
    "  - `d` = dimensión del embedding / head.\n",
    "- Cada token nuevo aumenta `L` y por tanto el cómputo crece con la longitud de la secuencia, a menos que reutilicemos resultados (K-V caching).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d246361",
   "metadata": {},
   "source": [
    "### **K-V caching**\n",
    "\n",
    "#### **Recordatorio de la atención**\n",
    "\n",
    "En una capa de atención, para cada token tenemos:\n",
    "\n",
    "- `Q` (query), `K` (key), `V` (value).\n",
    "- La atención se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "En un modelo autoregresivo, cuando generas el token `t`:\n",
    "\n",
    "- `Q_t` se calcula solo para la nueva posición.\n",
    "- `K_1,...,K_t` y `V_1,...,V_t` se usan para atender todo el contexto previo.\n",
    "\n",
    "Si **no** usamos K-V caching:\n",
    "\n",
    "- Para cada token nuevo, vuelves a calcular todas las `K` y `V` de todos los tokens anteriores.\n",
    "- Esto implica recomputar trabajo que ya hiciste.\n",
    "\n",
    "Con **K-V caching**:\n",
    "\n",
    "- Guardas `K_1,...,K_t` y `V_1,...,V_t` en un buffer (`past_key_values`).\n",
    "- Para el siguiente token solo calculas `K_{t+1}, V_{t+1}` y concatenas.\n",
    "- La complejidad por token se acerca a `O(t)` en vez de `O(t^2)` en la práctica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58070666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración más controlada de K-V caching con gpt2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "modelo = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "modelo.eval()\n",
    "\n",
    "prompt = \"Explain in one sentence what key-value caching is in transformer models.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "max_new_tokens = 30\n",
    "\n",
    "# 1) Generación directa con generate (usa cache internamente)\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    out_generate = modelo.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True\n",
    "    )\n",
    "t_generate = time.time() - start\n",
    "print(f\"Tiempo usando generate+use_cache=True: {t_generate:.3f}s\")\n",
    "\n",
    "# 2) Bucle manual SIN usar past_key_values explícito (no guardamos cache entre pasos)\n",
    "def autoregressive_no_cache(modelo, input_ids, max_steps):\n",
    "    ids = input_ids\n",
    "    for _ in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            outputs = modelo(input_ids=ids)  # recalcula todo\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        ids = torch.cat([ids, next_token], dim=-1)\n",
    "    return ids\n",
    "\n",
    "start = time.time()\n",
    "out_no_cache = autoregressive_no_cache(modelo, inputs[\"input_ids\"], max_new_tokens)\n",
    "t_no_cache = time.time() - start\n",
    "print(f\"Tiempo autoregresivo sin cache explícito: {t_no_cache:.3f}s\")\n",
    "\n",
    "print(\"\\nSalida (corta) con generate:\")\n",
    "print(tokenizer.decode(out_generate[0][: len(inputs['input_ids'][0]) + 40], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666a562",
   "metadata": {},
   "source": [
    "#### **Uso explícito de `past_key_values`**\n",
    "\n",
    "Podemos mostrar aún más claro el mecanismo:\n",
    "\n",
    "1. Primer *forward* sobre el *prompt* con `use_cache=True` -> obtenemos `past_key_values`.\n",
    "2. Luego, en cada paso:\n",
    "   - Pasamos solo el último token + `past_key_values`.\n",
    "   - El modelo devuelve logits y un nuevo `past_key_values` actualizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_with_kv_cache(model, input_ids, max_steps):\n",
    "    # Paso 1: procesar todo el prompt con cache\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, use_cache=True)\n",
    "    past_kv = outputs.past_key_values\n",
    "    generated = input_ids\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        last_token = generated[:, -1:]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=last_token, past_key_values=past_kv, use_cache=True)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        past_kv = outputs.past_key_values\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "    return generated\n",
    "\n",
    "start = time.time()\n",
    "out_kv = autoregressive_with_kv_cache(modelo, inputs[\"input_ids\"], max_new_tokens)\n",
    "t_kv = time.time() - start\n",
    "print(f\"Tiempo autoregresivo con past_key_values: {t_kv:.3f}s\")\n",
    "\n",
    "print(\"\\nSalida (corta) con KV cache explícito:\")\n",
    "print(tokenizer.decode(out_kv[0][: len(inputs['input_ids'][0]) + 40], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04319e01",
   "metadata": {},
   "source": [
    "### **Early Exit y Knowledge Distillation**\n",
    "\n",
    "#### **Early Exit**\n",
    "\n",
    "En un modelo profundo (muchas capas):\n",
    "\n",
    "- Las primeras capas capturan patrones locales.\n",
    "- Las capas intermedias capturan estructuras más abstractas.\n",
    "- Las últimas capas refinan la predicción.\n",
    "\n",
    "Idea de *early exit*:\n",
    "\n",
    "- Añadir \"clasificadores auxiliares\" en capas intermedias.\n",
    "- En tiempo de inferencia, en cada capa calculas:\n",
    "  - Probabilidades sobre el siguiente token (o clase).\n",
    "  - Una medida de confianza (por ejemplo, entropía o gap entre top-1 y top-2).\n",
    "- Si la confianza supera un umbral, puedes **detenerte** sin pasar por las capas restantes.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Para ejemplos \"fáciles\", reduces latencia porque no recorres todas las capas.\n",
    "- Para ejemplos difíciles, el modelo sigue usando la profundidad completa.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Arquitectura y código de inferencia más complejos.\n",
    "- Necesitas una fase de entrenamiento adicional para esos clasificadores intermedios.\n",
    "\n",
    "#### **Knowledge Distillation (KD) con temperatura**\n",
    "\n",
    "En KD, el *teacher* genera logits `z_teacher` y el *student* logits `z_student`.  \n",
    "Usamos una **temperatura `T`** para suavizar la distribución:\n",
    "\n",
    "$$\n",
    "p_i^{(teacher)} = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}, \\quad\n",
    "p_i^{(student)} = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\n",
    "$$\n",
    "\n",
    "Con `T > 1`, la distribución es más suave y revela \"conocimiento oscuro\" (dark knowledge) sobre clases secundarias.\n",
    "\n",
    "La loss típica es:\n",
    "\n",
    "$$\n",
    "L = T^2 \\cdot \\text{KL}(p^{(teacher)} \\,\\|\\, p^{(student)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedeb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo más explícito de KD con distintas temperaturas\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size = 10\n",
    "teacher_logits = torch.randn(vocab_size) * 3.0  # teacher más \"afilado\"\n",
    "student_logits = torch.randn(vocab_size, requires_grad=True)\n",
    "\n",
    "def distillation_step(teacher_logits, student_logits, temperature):\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    teacher_probs = softmax(teacher_logits / temperature).detach()\n",
    "    student_log_probs = log_softmax(student_logits / temperature)\n",
    "\n",
    "    loss = kl_div(student_log_probs, teacher_probs) * (temperature ** 2)\n",
    "    return loss, teacher_probs, torch.exp(student_log_probs)\n",
    "\n",
    "for T in [1.0, 2.0, 5.0]:\n",
    "    student_logits.grad = None\n",
    "    loss, p_teacher, p_student = distillation_step(teacher_logits, student_logits, T)\n",
    "    loss.backward()\n",
    "    print(f\"Temperature={T}: Loss={loss.item():.6f}, grad_norm={student_logits.grad.norm().item():.6f}\")\n",
    "    print(f\"  Teacher probs (top-3): {torch.topk(p_teacher, 3).values}\")\n",
    "    print(f\"  Student probs (top-3): {torch.topk(p_student, 3).values}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e2c49",
   "metadata": {},
   "source": [
    "### **Decoding avanzado: Speculative y Parallel Decoding**\n",
    "\n",
    "#### **Speculative Decoding (idea intuitiva)**\n",
    "\n",
    "Imagina:\n",
    "\n",
    "- Modelo pequeño y rápido (*draft model*).\n",
    "- Modelo grande y caro (*target model*).\n",
    "\n",
    "Pasos:\n",
    "\n",
    "1. El modelo pequeño propone `k` tokens (ejemplo: 4) de golpe.\n",
    "2. El modelo grande evalúa esos `k` tokens en bloque y calcula sus probabilidades reales.\n",
    "3. Si la secuencia propuesta es lo bastante \"compatible\" con el modelo grande:\n",
    "   - Aceptas varios tokens de una sola vez.\n",
    "   - Solo \"corriges\" cuando haya divergencias grandes.\n",
    "\n",
    "Resultado:\n",
    "\n",
    "- Menos llamadas *full forward* al modelo grande.\n",
    "- Ahorro de latencia, especialmente con contextos largos.\n",
    "\n",
    "#### **Parallel/Beam Decoding**\n",
    "\n",
    "- **Beam search** mantiene varias hipótesis en paralelo:\n",
    "  - Aumenta calidad a costa de cómputo.\n",
    "- **Parallel decoding** (investigación actual):\n",
    "  - Intenta predecir varios tokens futuros en paralelo y luego corregir.\n",
    "  - Más complicado de implementar correctamente, pero prometedor para latencias bajas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93098201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "\n",
    "# Asumimos que:\n",
    "# - modelo (draft_model y target_model) ya está cargado\n",
    "# - tokenizer ya está cargado\n",
    "# - device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def speculative_step(draft_model, target_model, tokenizer, prompt, max_new_tokens=30, draft_k=4):\n",
    "    # Tokenizamos el prompt una sola vez\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = inputs[\"input_ids\"]  # [1, L_inicial]\n",
    "\n",
    "    # Número de iteraciones: cada paso propone draft_k tokens\n",
    "    num_steps = max_new_tokens // draft_k\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # Atención coherente con la longitud actual de la secuencia\n",
    "        attention_mask = torch.ones_like(generated, device=device)\n",
    "\n",
    "        # 1) Draft model propone draft_k tokens adicionales\n",
    "        with torch.no_grad():\n",
    "            draft_out = draft_model.generate(\n",
    "                input_ids=generated,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=draft_k,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=tokenizer.eos_token_id,  # evita el warning\n",
    "            )\n",
    "\n",
    "        # Nos quedamos solo con los últimos draft_k tokens generados\n",
    "        proposed = draft_out[:, -draft_k:]  # [1, draft_k]\n",
    "\n",
    "        # 2) Target model los \"evalúa\" (en esta demo, simplemente los aceptamos)\n",
    "        # En una implementación real, aquí usarías target_model(**...) para\n",
    "        # comparar distribuciones y aceptar/rechazar token a token.\n",
    "        generated = torch.cat([generated, proposed], dim=-1)  # [1, L + draft_k]\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "#  Demostración rápida\n",
    "short_prompt = \"List three benefits of speculative decoding:\"\n",
    "start = time.time()\n",
    "out_spec = speculative_step(modelo, modelo, tokenizer, short_prompt, max_new_tokens=20, draft_k=4)\n",
    "t_spec = time.time() - start\n",
    "\n",
    "print(f\"Tiempo (toy speculative con modelo==draft==target): {t_spec:.3f}s\")\n",
    "print(tokenizer.decode(out_spec[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15035f4",
   "metadata": {},
   "source": [
    "### **Cuantización en detalle: simétrica y asimétrica**\n",
    "\n",
    "Queremos mapear valores reales `x` a enteros `q`.\n",
    "\n",
    "#### **Cuantización simétrica**\n",
    "\n",
    "- Escala `s`:\n",
    "  - Sea `a = max(|x|)` y el rango entero `[-Qmax, Qmax]` (ejemplo: `Qmax = 127` para 8 bits signed).\n",
    "  - Definimos `s = a / Qmax`.\n",
    "- Cuantizamos:\n",
    "\n",
    "$$\n",
    "q = \\text{round}(x / s)\n",
    "$$\n",
    "\n",
    "- De-cuantizamos:\n",
    "\n",
    "$$\n",
    "\\hat{x} = s \\cdot q\n",
    "$$\n",
    "\n",
    "#### **Cuantización asimétrica**\n",
    "\n",
    "- Para rangos que no están centrados en 0.\n",
    "\n",
    "Sea:\n",
    "\n",
    "- `x_min`, `x_max`.\n",
    "- `Qmin`, `Qmax` (por ejemplo, `[0, 255]` para 8 bits unsigned).\n",
    "\n",
    "Definimos:\n",
    "\n",
    "$$\n",
    "s = \\frac{x_{\\max} - x_{\\min}}{Q_{\\max} - Q_{\\min}}, \\quad\n",
    "z = \\text{round}\\left(Q_{\\min} - \\frac{x_{\\min}}{s}\\right)\n",
    "$$\n",
    "\n",
    "Cuantizamos:\n",
    "\n",
    "$$\n",
    "q = \\text{round}\\left(\\frac{x}{s}\\right) + z\n",
    "$$\n",
    "\n",
    "De-cuantizamos:\n",
    "\n",
    "$$\n",
    "\\hat{x} = s \\cdot (q - z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ee0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación sencilla de cuantización simétrica y asimétrica para un vector\n",
    "\n",
    "def quantize_symmetric(x, num_bits=8):\n",
    "    qmax = 2 ** (num_bits - 1) - 1  # ej. 127\n",
    "    a = np.max(np.abs(x))\n",
    "    if a == 0:\n",
    "        scale = 1.0\n",
    "        q = np.zeros_like(x, dtype=np.int8)\n",
    "    else:\n",
    "        scale = a / qmax\n",
    "        q = np.round(x / scale).astype(np.int8)\n",
    "        q = np.clip(q, -qmax-1, qmax)  # aseguramos rango\n",
    "    x_hat = scale * q.astype(np.float32)\n",
    "    return q, x_hat, scale\n",
    "\n",
    "def quantize_asymmetric(x, num_bits=8):\n",
    "    qmin = 0\n",
    "    qmax = 2 ** num_bits - 1  # ej. 255\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    if x_max == x_min:\n",
    "        scale = 1.0\n",
    "        zero_point = 0\n",
    "        q = np.zeros_like(x, dtype=np.uint8)\n",
    "    else:\n",
    "        scale = (x_max - x_min) / (qmax - qmin)\n",
    "        zero_point = int(round(qmin - x_min / scale))\n",
    "        zero_point = max(qmin, min(qmax, zero_point))\n",
    "        q = np.round(x / scale + zero_point).astype(np.int32)\n",
    "        q = np.clip(q, qmin, qmax).astype(np.uint8)\n",
    "    x_hat = scale * (q.astype(np.float32) - zero_point)\n",
    "    return q, x_hat, scale, zero_point\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(10).astype(np.float32) * 10 + 5  # distribución no centrada\n",
    "\n",
    "q_sym, x_sym, s_sym = quantize_symmetric(x)\n",
    "q_asym, x_asym, s_asym, z_asym = quantize_asymmetric(x)\n",
    "\n",
    "def mse(a, b):\n",
    "    return float(np.mean((a - b) ** 2))\n",
    "\n",
    "print(\"Vector original:\", x)\n",
    "print(\"\\n Simétrica\")\n",
    "print(\"q_sym:\", q_sym)\n",
    "print(\"x_hat_sym:\", x_sym)\n",
    "print(\"scale:\", s_sym)\n",
    "print(\"MSE sym:\", mse(x, x_sym))\n",
    "\n",
    "print(\"\\n Asimétrica \")\n",
    "print(\"q_asym:\", q_asym)\n",
    "print(\"x_hat_asym:\", x_asym)\n",
    "print(\"scale:\", s_asym, \"zero_point:\", z_asym)\n",
    "print(\"MSE asym:\", mse(x, x_asym))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0de39",
   "metadata": {},
   "source": [
    "#### **Cuantización por canal/por tensor**\n",
    "\n",
    "En modelos grandes:\n",
    "\n",
    "- Puedes cuantizar:\n",
    "  - **Por tensor**: un `scale` y `zero_point` global por tensor de pesos.\n",
    "  - **Por canal**: un `scale` por cada canal de salida (por ejemplo, por fila de una matriz de pesos de una capa lineal).\n",
    "- La cuantización por canal suele preservar mejor la calidad, al precio de más metadatos (más escalas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: cuantización simétrica por fila en una matriz de pesos\n",
    "\n",
    "W = np.random.randn(4, 8).astype(np.float32) * 0.5\n",
    "\n",
    "def quantize_per_row_symmetric(W, num_bits=8):\n",
    "    rows, cols = W.shape\n",
    "    qW = np.zeros_like(W, dtype=np.int8)\n",
    "    scales = np.zeros(rows, dtype=np.float32)\n",
    "    for i in range(rows):\n",
    "        q, x_hat, s = quantize_symmetric(W[i], num_bits=num_bits)\n",
    "        qW[i] = q\n",
    "        scales[i] = s\n",
    "    return qW, scales\n",
    "\n",
    "def dequantize_per_row_symmetric(qW, scales):\n",
    "    rows, cols = qW.shape\n",
    "    W_hat = np.zeros_like(qW, dtype=np.float32)\n",
    "    for i in range(rows):\n",
    "        W_hat[i] = scales[i] * qW[i].astype(np.float32)\n",
    "    return W_hat\n",
    "\n",
    "qW, scales = quantize_per_row_symmetric(W)\n",
    "W_hat = dequantize_per_row_symmetric(qW, scales)\n",
    "\n",
    "print(\"Matriz original W:\\n\", W)\n",
    "print(\"\\nMatriz cuantizada qW:\\n\", qW)\n",
    "print(\"\\nMatriz recosntruida W_hat:\\n\", W_hat)\n",
    "print(\"\\nMSE por fila:\", np.mean((W - W_hat) ** 2, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa06249",
   "metadata": {},
   "source": [
    "### **Juntando todo: K-V caching + cuantización**\n",
    "\n",
    "La idea general para un despliegue tipo \"Llama3 8-bit + KV cache\" es:\n",
    "\n",
    "1. **Modelo cuantizado** (por ejemplo, en 8 bits):\n",
    "   - Pesos almacenados en `int8` o formato comprimido.\n",
    "   - Matrices de proyección (`W_q, W_k, W_v, W_o`) y MLPs en bajo bit.\n",
    "2. **Inferencia autoregresiva con K-V cache**:\n",
    "   - Se guarda `past_key_values` en memoria (a menudo también comprimido/optimizados).\n",
    "   - Cada nuevo token reutiliza los K y V de pasos previos.\n",
    "3. **Pipeline típico**:\n",
    "   - Pre-procesas el *prompt*.\n",
    "   - Forward del *prompt* completo para inicializar cachés.\n",
    "   - Generación token a token:\n",
    "     - Cálculo del siguiente token usando pesos cuantizados.\n",
    "     - Reutilización de caché para atención.\n",
    "\n",
    "En este cuaderno no vamos a cargar un Llama3 real (su tamaño es demasiado grande para un contenedor estándar),\n",
    "pero sí podemos ilustrar el flujo usando modelos pequeños y, opcionalmente, cuantización dinámica en PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d48c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo \"mixto\": gpt2 + cuantización de capas lineales (CPU) + generate (que usa K-V cache internamente)\n",
    "# Nota: la cuantización dinámica de PyTorch solo funciona en CPU para módulos soportados.\n",
    "\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "small_model_name = \"gpt2\"\n",
    "tokenizer_small = AutoTokenizer.from_pretrained(small_model_name)\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(small_model_name)\n",
    "\n",
    "def count_parameters_mb(model):\n",
    "    total_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    return total_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Modelo FP32 en CPU: ~{count_parameters_mb(model_fp32):.2f} MB\")\n",
    "\n",
    "# Cuantización dinámica de capas lineales a int8\n",
    "quantized_model = quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(f\"Modelo cuantizado dinámicamente (int8 para Linear): ~{count_parameters_mb(quantized_model):.2f} MB\")\n",
    "\n",
    "prompt_prod = \"Explain why KV caching is important for large language models in production.\"\n",
    "inputs_prod = tokenizer_small(prompt_prod, return_tensors=\"pt\")\n",
    "\n",
    "# Medimos tiempos con generate (usa K-V cache) en FP32 y modelo cuantizado\n",
    "def measure_generate_time(modelo, inputs, max_new_tokens=40):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = modelo.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return time.time() - start\n",
    "\n",
    "t_fp32 = measure_generate_time(model_fp32, inputs_prod)\n",
    "t_q = measure_generate_time(quantized_model, inputs_prod)\n",
    "\n",
    "print(f\"Tiempo generate FP32: {t_fp32:.3f}s\")\n",
    "print(f\"Tiempo generate cuantizado dinámico: {t_q:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e52fe",
   "metadata": {},
   "source": [
    "#### **Blueprint para Llama3 8-bit + KV cache**\n",
    "\n",
    "En un entorno real con Llama3 y GPU (y librerías como `bitsandbytes`, `transformers`, etc.), el flujo sería similar a:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Requiere bitsandbytes y GPU\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,      # o load_in_4bit=True\n",
    "    device_map=\"auto\"       # distribuye en GPU(s)\n",
    ")\n",
    "\n",
    "prompt = \"Explain the impact of KV caching and 8-bit quantization on LLM inference latency.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model_8bit.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,      # KV cache activado\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "Puntos clave:\n",
    "\n",
    "- `load_in_8bit=True` carga los pesos en 8 bits usando kernels especializados.\n",
    "- `use_cache=True` activa K-V caching en el *decoder*.\n",
    "- Con esto, cada paso de generación usa:\n",
    "  - Matmuls más baratos (int8).\n",
    "  - Menos recomputación de atención (KV cache).\n",
    "- En despliegues reales, se ajustan:\n",
    "  - Tamaño de batch.\n",
    "  - Longitud máxima de contexto.\n",
    "  - Estrategias de batching dinámico, *streaming*, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68fee2",
   "metadata": {},
   "source": [
    "### **Experimento comparativo: FP32 sin caché vs FP32 con caché vs int8 con caché**\n",
    "\n",
    "En esta sección podemos **medir y comparar** tres modos de inferencia:\n",
    "\n",
    "1. **FP32 + sin K-V cache**  \n",
    "2. **FP32 + con K-V cache**  \n",
    "3. **Int8 (cuantización dinámica) + con K-V cache**\n",
    "\n",
    "El objetivo es observar:\n",
    "\n",
    "- Diferencias en **tamaño de modelo (MB)**.\n",
    "- Diferencias en **tiempo de generación**.\n",
    "- Una métrica simple de **tokens/segundo**.\n",
    "\n",
    "> Nota: el experimento está pensado para correr en **CPU** con un modelo pequeño (`gpt2`) usando cuantización dinámica de PyTorch (`torch.ao.quantization`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "torch.set_num_threads(max(1, torch.get_num_threads()))\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "def load_fp32_model(name: str):\n",
    "    modelo = AutoModelForCausalLM.from_pretrained(name)\n",
    "    modelo.to(device_cpu)\n",
    "    modelo.eval()\n",
    "    return modelo\n",
    "\n",
    "def count_parameters_mb(model: torch.nn.Module) -> float:\n",
    "    total_bytes = sum(p.numel() * p.element_size() for p in modelo.parameters())\n",
    "    return total_bytes / (1024 * 1024)\n",
    "\n",
    "def measure_generate_time(model, inputs, max_new_tokens=40, use_cache=True):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = modelo.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "    return time.time() - start\n",
    "\n",
    "prompt = \"Explain briefly why KV caching and quantization help LLM inference.\"\n",
    "inputs_cpu = tokenizer(prompt, return_tensors=\"pt\").to(device_cpu)\n",
    "\n",
    "max_new_tokens = 40\n",
    "\n",
    "model_fp32 = load_fp32_model(model_name)\n",
    "size_fp32 = count_parameters_mb(model_fp32)\n",
    "\n",
    "t_fp32_nocache = measure_generate_time(model_fp32, inputs_cpu, max_new_tokens=max_new_tokens, use_cache=False)\n",
    "t_fp32_cache   = measure_generate_time(model_fp32, inputs_cpu, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "\n",
    "model_int8 = quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "size_int8 = count_parameters_mb(model_int8)\n",
    "\n",
    "t_int8_cache = measure_generate_time(model_int8, inputs_cpu, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"mode\": \"FP32 - sin cache\",\n",
    "        \"dtype\": \"float32\",\n",
    "        \"use_cache\": False,\n",
    "        \"model_size_MB\": size_fp32,\n",
    "        \"time_s\": t_fp32_nocache,\n",
    "        \"tokens_generated\": max_new_tokens,\n",
    "        \"tokens_per_s\": max_new_tokens / t_fp32_nocache if t_fp32_nocache > 0 else np.nan,\n",
    "    },\n",
    "    {\n",
    "        \"mode\": \"FP32 - con cache\",\n",
    "        \"dtype\": \"float32\",\n",
    "        \"use_cache\": True,\n",
    "        \"model_size_MB\": size_fp32,\n",
    "        \"time_s\": t_fp32_cache,\n",
    "        \"tokens_generated\": max_new_tokens,\n",
    "        \"tokens_per_s\": max_new_tokens / t_fp32_cache if t_fp32_cache > 0 else np.nan,\n",
    "    },\n",
    "    {\n",
    "        \"mode\": \"Int8 - con cache\",\n",
    "        \"dtype\": \"int8 (Linear)\",\n",
    "        \"use_cache\": True,\n",
    "        \"model_size_MB\": size_int8,\n",
    "        \"time_s\": t_int8_cache,\n",
    "        \"tokens_generated\": max_new_tokens,\n",
    "        \"tokens_per_s\": max_new_tokens / t_int8_cache if t_int8_cache > 0 else np.nan,\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[[\"mode\", \"dtype\", \"use_cache\", \"model_size_MB\", \"time_s\", \"tokens_generated\", \"tokens_per_s\"]]\n",
    "\n",
    "print(\"Resumen comparativo de modos de inferencia:\\n\")\n",
    "print(df.to_string(index=False, float_format=lambda x: f\"{x:8.3f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb5bf9",
   "metadata": {},
   "source": [
    "#### **Preguntas y ejercicios**\n",
    "\n",
    "Algunas preguntas a responder a partir de esta tabla:\n",
    "\n",
    "1. ¿Cuánta diferencia hay en **model_size_MB** entre FP32 e int8?\n",
    "2. ¿Se observa una mejora clara en **time_s** entre:\n",
    "   - FP32 sin cache vs FP32 con cache?\n",
    "   - FP32 con cache vs int8 con cache?\n",
    "3. ¿Cuál de los tres modos tiene más **tokens_per_s**?\n",
    "4. ¿En qué escenarios reales tendría sentido:\n",
    "   - Aceptar una posible pequeña degradación de calidad por cuantización a cambio de esta mejora?\n",
    "   - Combinar además técnicas como *speculative decoding* o *batching*?\n",
    "\n",
    "5. Documenta: \n",
    "- Setup del experimento (CPU, número de threads, modelo, prompt).\n",
    "- Tabla de resultados.\n",
    "- Conclusiones y trade-offs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe0c9a-4b0f-4383-a5e1-d6a3bc2205f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
