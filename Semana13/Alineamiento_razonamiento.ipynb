{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8c2ada",
   "metadata": {},
   "source": [
    "### **Entrenamiento de alineación y razonamiento con RLHF**\n",
    "\n",
    "Este cuaderno está pensado para ejecutarse **dentro de Docker** con un entorno que ya tenga instalados:\n",
    "\n",
    "- `torch`\n",
    "- `transformers`\n",
    "- `datasets`\n",
    "- `trl`\n",
    "- (opcional) `accelerate` y soporte de GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65128002",
   "metadata": {},
   "source": [
    "#### **Preparación de entorno**\n",
    "\n",
    "En esta sección:\n",
    "\n",
    "- Elegimos dispositivo (CPU / GPU).\n",
    "- Verificamos versiones de librerías clave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749e2c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.2.0+cu121\n",
      "Transformers: 4.42.3\n",
      "Datasets: 2.20.0\n",
      "TRL: 0.9.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import trl\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"TRL:\", trl.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8012b4c",
   "metadata": {},
   "source": [
    "#### **Entrenamiento de alineación (Alignment Training)**\n",
    "\n",
    "**Entrenamiento de alineación** = conjunto de técnicas para que un modelo de lenguaje grande (LLM) sea:\n",
    "\n",
    "- **Útil (helpful)**: responde a lo que el usuario realmente necesita.\n",
    "- **Inofensivo (harmless)**: evita instrucciones dañinas, ilegales o poco éticas.\n",
    "- **Honesto (honest)**: evita *alucinaciones* y reconoce sus límites.\n",
    "\n",
    "Pipeline típico usual elemental:\n",
    "\n",
    "1. **Pre-entrenamiento** (*pre-training*):  \n",
    "   El modelo aprende estadística del lenguaje en grandes corpus, optimizando una tarea de modelado de lenguaje (predecir el siguiente token).\n",
    "\n",
    "2. **Supervised Fine-Tuning (SFT)**:  \n",
    "   Se ajusta el modelo con pares *(instrucción, respuesta deseada)*.  \n",
    "   - Datos generados por humanos (anotadores) o por otros modelos filtrados.\n",
    "   - Objetivo: que el modelo hable en \"modo asistente\".\n",
    "\n",
    "3. **Modelado de recompensas (Reward modeling)**:  \n",
    "   Se entrena un modelo que asigna un **score escalar** a una respuesta, dado un prompt.  \n",
    "   - Usa datos de **preferencias humanas**: pares *(chosen, rejected)*.\n",
    "   - El modelo aprende a puntuar más alto las respuestas alineadas.\n",
    "\n",
    "4. **RLHF (Reinforcement Learning from Human Feedback)**:  \n",
    "   El reward model actúa como **juez**. Con algoritmos como PPO:\n",
    "   - La *política (policy)* (modelo generador) produce respuestas.\n",
    "   - El modelo de recompensa las puntúa.\n",
    "   - La política  se actualiza para maximizar la recompensa (con regularización por KL para no alejarse mucho del modelo base).\n",
    "\n",
    "En este cuaderno trabajaremos con un subconjunto del dataset **`Anthropic/hh-rlhf`** para ilustrar el *reward modeling** y un esqueleto de RLHF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5511038",
   "metadata": {},
   "source": [
    "### **Tipos de retroalimentación humana y dataset `Anthropic/hh-rlhf`**\n",
    "\n",
    "##### **Tipos de human feedback**\n",
    "\n",
    "En la práctica, la retroalimentación humana puede tomar varias formas:\n",
    "\n",
    "1. **Etiquetas directas (supervisión estándar)**  \n",
    "   - Por ejemplo: *\"esta respuesta es correcta/incorrecta\"*, *\"esta respuesta es tóxica\"*, etc.\n",
    "\n",
    "2. **Preferencias pareadas (pairwise preferences)**  \n",
    "   - Dado un mismo prompt, el humano ve dos respuestas y elige cuál prefiere.\n",
    "   - Es justo el caso de `hh-rlhf`: `chosen` vs `rejected`.\n",
    "\n",
    "3. **Recompensas escalares**  \n",
    "   - El humano da un score (por ejemplo, entre 1 y 5).\n",
    "\n",
    "4. **Feedback estructurado**  \n",
    "   - Explicaciones, correcciones paso a paso, anotaciones de razonamiento, etc.\n",
    "\n",
    "En RLHF nos interesan especialmente las **preferencias pareadas**, porque permiten entrenar un modelo de recompensas robusto con relativamente pocos datos.\n",
    "\n",
    "#### **`Anthropic/hh-rlhf`**\n",
    "\n",
    "`Anthropic/hh-rlhf` contiene diálogos tipo chat con dos variantes para cada ejemplo:\n",
    "\n",
    "- `chosen`: respuesta preferida (más alineada y segura).\n",
    "- `rejected`: respuesta descartada (menos segura, menos útil).\n",
    "\n",
    "En un escenario de seguridad, por ejemplo, `chosen` suele ser la respuesta que:\n",
    "\n",
    "- **Rechaza** ayudar a hacer daño (ejemplo, hackear correos, fabricar explosivos).\n",
    "- Ofrece alternativas seguras o educativas.\n",
    "\n",
    "Mientras que `rejected` a menudo:\n",
    "\n",
    "- Da detalles de cómo ejecutar la acción dañina,\n",
    "- O al menos no la bloquea de forma clara.\n",
    "\n",
    "Nuestro reward model aprenderá a asignar **score mayor** a `chosen` que a `rejected`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa694a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargamos un subconjunto pequeño para exploración\n",
    "raw_ds = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:200]\")\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c101c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chosen', 'rejected'])\n",
      "\n",
      " Escogida (respuesta preferida) \n",
      "\n",
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\n",
      "\n",
      "Human: What's your favorite one?\n",
      "\n",
      "Assistant: I haven't even thought about it.\n",
      "\n",
      " Rechazada (respuesta no deseada)\n",
      "\n",
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\n",
      "\n",
      "Human: What's your favorite one?\n",
      "\n",
      "Assistant: Ass.\n"
     ]
    }
   ],
   "source": [
    "example = raw_ds[0]\n",
    "print(example.keys())\n",
    "\n",
    "print(\"\\n Escogida (respuesta preferida) \\n\")\n",
    "print(example[\"chosen\"][:600])\n",
    "\n",
    "print(\"\\n Rechazada (respuesta no deseada)\\n\")\n",
    "print(example[\"rejected\"][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b210d1a",
   "metadata": {},
   "source": [
    "#### **Preparar los datos para `RewardTrainer`**\n",
    "\n",
    "El `RewardTrainer` de TRL espera un dataset con, al menos, columnas como:\n",
    "\n",
    "- `input_ids_chosen`, `attention_mask_chosen`\n",
    "- `input_ids_rejected`, `attention_mask_rejected`\n",
    "\n",
    "Pasos:\n",
    "\n",
    "1. Elegimos un modelo base para el modelo de recompensas (aquí, **`gpt2-medium`**, mayor que `sshleifer/tiny-gpt2`).  \n",
    "2. Tokenizamos `chosen` y `rejected` con la misma longitud máxima.  \n",
    "3. Filtramos ejemplos demasiado largos.\n",
    "\n",
    "Esto nos dará un dataset listo para el entrenamiento del modelo de recompensa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65dbb428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reward_model_name = \"gpt2-medium\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def build_pair(example):\n",
    "    chosen_text = example[\"chosen\"]\n",
    "    rejected_text = example[\"rejected\"]\n",
    "\n",
    "    chosen_tokens = tokenizer(\n",
    "        chosen_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "    rejected_tokens = tokenizer(\n",
    "        rejected_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_tokens[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_tokens[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_tokens[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_tokens[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "processed_ds = raw_ds.map(build_pair, remove_columns=raw_ds.column_names)\n",
    "\n",
    "processed_ds = processed_ds.filter(\n",
    "    lambda x: len(x[\"input_ids_chosen\"]) <= max_length\n",
    "              and len(x[\"input_ids_rejected\"]) <= max_length\n",
    ")\n",
    "\n",
    "processed_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28aa9da",
   "metadata": {},
   "source": [
    "#### **Entrenamiento de un reward model con `RewardTrainer`**\n",
    "\n",
    "Usaremos:\n",
    "\n",
    "- `AutoModelForSequenceClassification` con `num_labels = 1`.\n",
    "- `RewardTrainer` de TRL con una configuración simple.\n",
    "\n",
    "La pérdida que se optimiza es de tipo **Bradley-Terry**:\n",
    "el modelo recibe scores $r_{chosen}, r_{rejected}$ y aprende que:\n",
    "\n",
    "- $r_{chosen} > r_{rejected}$\n",
    "\n",
    "De forma más formal, se maximiza algo como:\n",
    "\n",
    "$$\n",
    "\\log \\sigma(r_{chosen} - r_{rejected})\n",
    "$$\n",
    "\n",
    "donde $\\sigma$ es la sigmoide logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0688ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardTrainer inicializado.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    reward_model_name,\n",
    "    num_labels=1,\n",
    ")\n",
    "reward_model.to(device)\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "reward_model.config.use_cache = False\n",
    "\n",
    "reward_train_config = RewardConfig(\n",
    "    output_dir=\"./reward-model-hh-rlhf-demo\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,  # demo: 1 epoch\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=False,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_train_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=processed_ds,\n",
    ")\n",
    "\n",
    "print(\"RewardTrainer inicializado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1821fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El entrenamiento completo puede ser costoso.\n",
    "# Para un entorno de demo puedes reducir el número de ejemplos o epochs.\n",
    "#\n",
    "# Descomenta para entrenar de verdad:\n",
    "#\n",
    "# reward_trainer.train()\n",
    "# reward_trainer.save_model()\n",
    "# tokenizer.save_pretrained(\"./reward-model-hh-rlhf-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e5da8",
   "metadata": {},
   "source": [
    "#### **Esqueleto de RLHF con `PPOTrainer`**\n",
    "\n",
    "El ciclo básico de PPO (muy simplificado) es:\n",
    "\n",
    "1. Tomar un batch de **prompts** (queries).\n",
    "2. La política(modelo generador) produce respuestas.\n",
    "3. El modelo de recompensa asigna una recompensa a cada respuesta.\n",
    "4. PPO ajusta la política para maximizar la recompensa, penalizando desviarse\n",
    "   demasiado del modelo de referencia (regularización por KL).\n",
    "\n",
    "Aquí mostraremos un **esqueleto** donde la recompensa es *dummy* (por longitud)\n",
    "para ilustrar la API de `PPOTrainer`. En un pipeline real (no incluido aquí), se conectaría con el modelo de recompensas entrenado en la sección anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be3054eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/trl/trainer/ppo_config.py:154: UserWarning: Note that using `forward_batch_size` is deprecated, use `mini_batch_size` instead. By setting it you overwrite `mini_batch_size` which affects both the batch size during forward passes and also the mini batch size for PPO optimization.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPOTrainer inicializado con queries limpias.\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "policy_model_name = \"gpt2\"  # policy para PPO (más grande que tiny)\n",
    "ppo_tokenizer = AutoTokenizer.from_pretrained(policy_model_name)\n",
    "if ppo_tokenizer.pad_token is None:\n",
    "    ppo_tokenizer.pad_token = ppo_tokenizer.eos_token\n",
    "\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model_name)\n",
    "policy_model.to(device)\n",
    "ref_model = create_reference_model(policy_model)  # modelo de referencia fijo\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=policy_model_name,\n",
    "    batch_size=2,\n",
    "    forward_batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    mini_batch_size=1,\n",
    "    log_with=None,\n",
    ")\n",
    "\n",
    "def extract_prompt(example):\n",
    "    \"\"\"\n",
    "    Extrae una línea no vacía del campo 'chosen'.\n",
    "    Si no encuentra nada, usa un prompt por defecto.\n",
    "    \"\"\"\n",
    "    text = example[\"chosen\"] or \"\"\n",
    "    # nos quedamos solo con líneas que tengan contenido\n",
    "    lines = [l for l in text.splitlines() if l.strip()]\n",
    "    if lines:\n",
    "        first_line = lines[0].strip()\n",
    "    else:\n",
    "        # fallback seguro (no debe quedar vacío)\n",
    "        first_line = \"Explain briefly what RLHF (Reinforcement Learning from Human Feedback) is.\"\n",
    "    return {\"query\": first_line}\n",
    "\n",
    "# Construir dataset con queries no vacías\n",
    "ppo_ds = raw_ds.map(extract_prompt)\n",
    "\n",
    "# Filtrar cualquier ejemplo que aun así tenga query vacía\n",
    "ppo_ds = ppo_ds.filter(lambda ex: len(ex[\"query\"].strip()) > 0)\n",
    "\n",
    "# Usar un mini-dataset para la demo\n",
    "ppo_ds = ppo_ds.select(range(min(8, len(ppo_ds))))\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    config=ppo_config,\n",
    "    tokenizer=ppo_tokenizer,\n",
    "    dataset=ppo_ds,\n",
    ")\n",
    "\n",
    "print(\"PPOTrainer inicializado con queries limpias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10bda570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective/kl': 0.0,\n",
       " 'objective/kl_dist': array([0., 0.], dtype=float32),\n",
       " 'objective/logprobs': array([[-5.0454917e+00, -5.5828285e+00, -2.9713676e+00, -3.1973491e+00,\n",
       "         -1.0266676e+01, -5.1310158e+00, -3.9688346e-01, -3.0542512e+00,\n",
       "         -4.7354078e+00, -7.1069807e-01, -1.0574325e+00, -9.3989745e-03,\n",
       "         -4.8161550e+00, -3.1059086e+00, -2.6087179e+00, -4.6385756e+00,\n",
       "         -4.1160727e+00, -2.6365438e+00, -1.0174114e+00, -2.1193166e-04,\n",
       "         -4.1726003e+00, -3.3575612e-01, -8.4118974e-01, -5.4087420e+00,\n",
       "         -2.6615305e+00, -1.2992363e+00, -1.6907987e-01, -1.9953644e-04,\n",
       "         -1.3743496e+00, -1.9560654e-02, -1.9289745e-01, -4.5840139e+00,\n",
       "         -3.8360395e+00, -2.1989255e+00, -1.4569078e-01, -1.2038434e-01,\n",
       "         -2.3171601e-04, -3.2421359e-01, -1.9321332e-02, -1.1621184e-01,\n",
       "         -4.8667555e+00, -2.5975082e+00],\n",
       "        [-5.0454917e+00, -5.5828247e+00, -4.6213140e+00, -3.2080401e-02,\n",
       "         -1.0034083e+01, -4.2052121e+00, -9.1327219e+00, -5.2883238e-01,\n",
       "         -8.0593663e-01, -8.8505715e-01, -3.1641677e-03, -5.4833589e+00,\n",
       "         -1.5608938e+00, -3.5847640e+00, -4.4385834e+00, -9.5842493e-01,\n",
       "         -2.7715788e+00, -1.6824837e+00, -1.9659610e+00, -2.6658380e+00,\n",
       "         -3.0947163e+00, -1.9100156e+00, -2.5120306e+00, -6.1567044e-01,\n",
       "         -3.2092404e+00, -1.7369292e+00, -1.3434659e+00, -6.9483733e-01,\n",
       "         -1.9985029e+00, -2.2914839e+00, -3.0648537e+00, -1.1663741e+00,\n",
       "         -2.0696793e+00, -1.6211149e-04, -4.1982346e+00, -1.3735454e+00,\n",
       "         -2.5372062e+00, -2.9490254e+00, -2.4868286e+00, -1.4065721e+00,\n",
       "         -8.0390626e-01, -1.5348681e+01]], dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-5.0454917e+00, -5.5828285e+00, -2.9713676e+00, -3.1973491e+00,\n",
       "         -1.0266676e+01, -5.1310158e+00, -3.9688346e-01, -3.0542512e+00,\n",
       "         -4.7354078e+00, -7.1069807e-01, -1.0574325e+00, -9.3989745e-03,\n",
       "         -4.8161550e+00, -3.1059086e+00, -2.6087179e+00, -4.6385756e+00,\n",
       "         -4.1160727e+00, -2.6365438e+00, -1.0174114e+00, -2.1193166e-04,\n",
       "         -4.1726003e+00, -3.3575612e-01, -8.4118974e-01, -5.4087420e+00,\n",
       "         -2.6615305e+00, -1.2992363e+00, -1.6907987e-01, -1.9953644e-04,\n",
       "         -1.3743496e+00, -1.9560654e-02, -1.9289745e-01, -4.5840139e+00,\n",
       "         -3.8360395e+00, -2.1989255e+00, -1.4569078e-01, -1.2038434e-01,\n",
       "         -2.3171601e-04, -3.2421359e-01, -1.9321332e-02, -1.1621184e-01,\n",
       "         -4.8667555e+00, -2.5975082e+00],\n",
       "        [-5.0454917e+00, -5.5828247e+00, -4.6213140e+00, -3.2080401e-02,\n",
       "         -1.0034083e+01, -4.2052121e+00, -9.1327219e+00, -5.2883238e-01,\n",
       "         -8.0593663e-01, -8.8505715e-01, -3.1641677e-03, -5.4833589e+00,\n",
       "         -1.5608938e+00, -3.5847640e+00, -4.4385834e+00, -9.5842493e-01,\n",
       "         -2.7715788e+00, -1.6824837e+00, -1.9659610e+00, -2.6658380e+00,\n",
       "         -3.0947163e+00, -1.9100156e+00, -2.5120306e+00, -6.1567044e-01,\n",
       "         -3.2092404e+00, -1.7369292e+00, -1.3434659e+00, -6.9483733e-01,\n",
       "         -1.9985029e+00, -2.2914839e+00, -3.0648537e+00, -1.1663741e+00,\n",
       "         -2.0696793e+00, -1.6211149e-04, -4.1982346e+00, -1.3735454e+00,\n",
       "         -2.5372062e+00, -2.9490254e+00, -2.4868286e+00, -1.4065721e+00,\n",
       "         -8.0390626e-01, -1.5348681e+01]], dtype=float32),\n",
       " 'objective/kl_coef': 0.2,\n",
       " 'objective/entropy': 63.37712860107422,\n",
       " 'ppo/mean_non_score_reward': 0.0,\n",
       " 'ppo/mean_scores': 32.0,\n",
       " 'ppo/std_scores': 0.0,\n",
       " 'tokens/queries_len_mean': 10.5,\n",
       " 'tokens/queries_len_std': 0.7071067690849304,\n",
       " 'tokens/queries_dist': array([11., 10.], dtype=float32),\n",
       " 'tokens/responses_len_mean': 32.0,\n",
       " 'tokens/responses_len_std': 0.0,\n",
       " 'tokens/responses_dist': array([32., 32.], dtype=float32),\n",
       " 'ppo/loss/policy': -0.05850623548030853,\n",
       " 'ppo/loss/value': 133.3682098388672,\n",
       " 'ppo/loss/total': 13.278314590454102,\n",
       " 'ppo/policy/entropy': 3.8886876106262207,\n",
       " 'ppo/policy/approxkl': 0.05144844204187393,\n",
       " 'ppo/policy/policykl': 0.10327112674713135,\n",
       " 'ppo/policy/clipfrac': 0.30078125,\n",
       " 'ppo/policy/advantages': array([-1.0495962 , -1.0100722 , -0.96846825, -0.9246745 , -0.8785758 ,\n",
       "        -0.8300509 , -0.77897197, -0.72520477, -0.66860765, -1.0891347 ,\n",
       "        -1.4423944 , -1.1807457 , -1.066859  , -1.000117  , -1.0362481 ,\n",
       "        -0.6404414 , -0.6285907 , -0.8164262 , -0.65223134, -0.5480073 ,\n",
       "        -0.51547974, -0.5169714 , -0.22843759, -0.21408571, -0.05554017,\n",
       "        -0.07285734, -0.13100225, -0.0239701 ,  0.12978514,  0.33756515,\n",
       "         0.43631965,  0.33336332,  0.4728659 ,  0.65658534,  0.4165697 ,\n",
       "         1.0382496 ,  1.1590072 ,  1.205667  ,  1.63868   ,  1.6986798 ,\n",
       "         1.9922435 , -1.8005496 , -1.108822  , -1.0724152 , -1.0340924 ,\n",
       "        -0.9937525 , -0.9512894 , -0.9065915 , -0.85954106, -0.8100143 ,\n",
       "        -0.75788087, -0.70300347, -1.1622462 , -1.2797507 , -1.1753169 ,\n",
       "        -1.3113061 , -1.1352515 , -1.051591  , -1.2144451 , -0.82961106,\n",
       "        -0.92721474, -0.771847  , -0.8143073 , -0.13376357, -0.63343614,\n",
       "        -0.6644414 , -0.59093606, -0.40772656,  0.31490445, -0.17706208,\n",
       "        -0.40580973,  0.98115766,  0.67021793,  0.05908301,  0.05664023,\n",
       "         0.64128107,  0.7289877 ,  1.7268329 ,  0.89680886,  1.3002778 ,\n",
       "         2.4137492 ,  2.2162714 ,  1.4345336 ,  1.5892739 , -1.108822  ,\n",
       "        -1.0724152 , -1.0340924 , -0.9937525 , -0.9512894 , -0.9065915 ,\n",
       "        -0.85954106, -0.8100143 , -0.75788087, -0.70300347, -1.1622462 ,\n",
       "        -1.2797507 , -1.1753169 , -1.3113061 , -1.1352515 , -1.051591  ,\n",
       "        -1.2144451 , -0.82961106, -0.92721474, -0.771847  , -0.8143073 ,\n",
       "        -0.13376357, -0.63343614, -0.6644414 , -0.59093606, -0.40772656,\n",
       "         0.31490445, -0.17706208, -0.40580973,  0.98115766,  0.67021793,\n",
       "         0.05908301,  0.05664023,  0.64128107,  0.7289877 ,  1.7268329 ,\n",
       "         0.89680886,  1.3002778 ,  2.4137492 ,  2.2162714 ,  1.4345336 ,\n",
       "         1.5892739 , -1.0495962 , -1.0100722 , -0.96846825, -0.9246745 ,\n",
       "        -0.8785758 , -0.8300509 , -0.77897197, -0.72520477, -0.66860765,\n",
       "        -1.0891347 , -1.4423944 , -1.1807457 , -1.066859  , -1.000117  ,\n",
       "        -1.0362481 , -0.6404414 , -0.6285907 , -0.8164262 , -0.65223134,\n",
       "        -0.5480073 , -0.51547974, -0.5169714 , -0.22843759, -0.21408571,\n",
       "        -0.05554017, -0.07285734, -0.13100225, -0.0239701 ,  0.12978514,\n",
       "         0.33756515,  0.43631965,  0.33336332,  0.4728659 ,  0.65658534,\n",
       "         0.4165697 ,  1.0382496 ,  1.1590072 ,  1.205667  ,  1.63868   ,\n",
       "         1.6986798 ,  1.9922435 , -1.8005496 , -1.108822  , -1.0724152 ,\n",
       "        -1.0340924 , -0.9937525 , -0.9512894 , -0.9065915 , -0.85954106,\n",
       "        -0.8100143 , -0.75788087, -0.70300347, -1.1622462 , -1.2797507 ,\n",
       "        -1.1753169 , -1.3113061 , -1.1352515 , -1.051591  , -1.2144451 ,\n",
       "        -0.82961106, -0.92721474, -0.771847  , -0.8143073 , -0.13376357,\n",
       "        -0.63343614, -0.6644414 , -0.59093606, -0.40772656,  0.31490445,\n",
       "        -0.17706208, -0.40580973,  0.98115766,  0.67021793,  0.05908301,\n",
       "         0.05664023,  0.64128107,  0.7289877 ,  1.7268329 ,  0.89680886,\n",
       "         1.3002778 ,  2.4137492 ,  2.2162714 ,  1.4345336 ,  1.5892739 ,\n",
       "        -1.0495962 , -1.0100722 , -0.96846825, -0.9246745 , -0.8785758 ,\n",
       "        -0.8300509 , -0.77897197, -0.72520477, -0.66860765, -1.0891347 ,\n",
       "        -1.4423944 , -1.1807457 , -1.066859  , -1.000117  , -1.0362481 ,\n",
       "        -0.6404414 , -0.6285907 , -0.8164262 , -0.65223134, -0.5480073 ,\n",
       "        -0.51547974, -0.5169714 , -0.22843759, -0.21408571, -0.05554017,\n",
       "        -0.07285734, -0.13100225, -0.0239701 ,  0.12978514,  0.33756515,\n",
       "         0.43631965,  0.33336332,  0.4728659 ,  0.65658534,  0.4165697 ,\n",
       "         1.0382496 ,  1.1590072 ,  1.205667  ,  1.63868   ,  1.6986798 ,\n",
       "         1.9922435 , -1.8005496 , -1.0495962 , -1.0100722 , -0.96846825,\n",
       "        -0.9246745 , -0.8785758 , -0.8300509 , -0.77897197, -0.72520477,\n",
       "        -0.66860765, -1.0891347 , -1.4423944 , -1.1807457 , -1.066859  ,\n",
       "        -1.000117  , -1.0362481 , -0.6404414 , -0.6285907 , -0.8164262 ,\n",
       "        -0.65223134, -0.5480073 , -0.51547974, -0.5169714 , -0.22843759,\n",
       "        -0.21408571, -0.05554017, -0.07285734, -0.13100225, -0.0239701 ,\n",
       "         0.12978514,  0.33756515,  0.43631965,  0.33336332,  0.4728659 ,\n",
       "         0.65658534,  0.4165697 ,  1.0382496 ,  1.1590072 ,  1.205667  ,\n",
       "         1.63868   ,  1.6986798 ,  1.9922435 , -1.8005496 , -1.108822  ,\n",
       "        -1.0724152 , -1.0340924 , -0.9937525 , -0.9512894 , -0.9065915 ,\n",
       "        -0.85954106, -0.8100143 , -0.75788087, -0.70300347, -1.1622462 ,\n",
       "        -1.2797507 , -1.1753169 , -1.3113061 , -1.1352515 , -1.051591  ,\n",
       "        -1.2144451 , -0.82961106, -0.92721474, -0.771847  , -0.8143073 ,\n",
       "        -0.13376357, -0.63343614, -0.6644414 , -0.59093606, -0.40772656,\n",
       "         0.31490445, -0.17706208, -0.40580973,  0.98115766,  0.67021793,\n",
       "         0.05908301,  0.05664023,  0.64128107,  0.7289877 ,  1.7268329 ,\n",
       "         0.89680886,  1.3002778 ,  2.4137492 ,  2.2162714 ,  1.4345336 ,\n",
       "         1.5892739 ], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': -3.818422555923462e-08,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98823327, 0.9827465 , 0.9626357 ,\n",
       "        0.9752619 , 1.0413866 , 0.9238082 , 0.9905826 , 0.9932151 ,\n",
       "        1.1074529 , 1.0046504 , 0.95555615, 0.9957686 , 0.9155829 ,\n",
       "        0.9089823 , 0.8962128 , 0.96826315, 0.99993324, 0.9108269 ,\n",
       "        0.9976591 , 0.9999964 , 1.0758449 , 1.0470365 , 1.0456232 ,\n",
       "        0.9977062 , 1.007846  , 0.99895793, 0.99206173, 0.9999956 ,\n",
       "        0.93534446, 0.99252814, 0.9927879 , 0.9807153 , 1.007685  ,\n",
       "        1.0137668 , 1.001018  , 0.9922698 , 0.9999982 , 0.9647213 ,\n",
       "        0.9946923 , 0.99169457, 1.0064514 , 0.9893667 , 0.9823286 ,\n",
       "        0.9528908 , 0.98797625, 0.96784747, 1.0880871 , 0.9670146 ,\n",
       "        0.9712471 , 1.0530173 , 1.0989751 , 0.9757493 , 0.915541  ,\n",
       "        0.99728835, 0.863048  , 0.8692427 , 0.79742026, 0.9162795 ,\n",
       "        0.84234226, 0.81899446, 0.90476966, 0.99998856, 0.99677277,\n",
       "        0.8650697 , 0.75090337, 0.85029346, 0.9197837 , 0.83486557,\n",
       "        0.9425367 , 0.999985  , 0.97768444, 0.9888297 , 0.8679397 ,\n",
       "        0.93396664, 0.92517567, 1.190856  , 0.95706236, 0.9472864 ,\n",
       "        0.99998915, 0.9372232 , 0.9914206 , 0.88619304, 1.0238163 ,\n",
       "        1.1710345 , 0.9762497 , 0.9348792 , 1.124771  , 0.9904651 ,\n",
       "        1.0837747 , 0.9983691 , 0.98018885, 0.92498183, 0.95312417,\n",
       "        0.87886614, 0.9982812 , 0.84191537, 0.6935355 , 0.53929234,\n",
       "        0.61598814, 0.8339275 , 0.55560243, 1.0279319 , 0.84814686,\n",
       "        1.0453864 , 0.7312887 , 0.7827371 , 0.81516576, 0.971849  ,\n",
       "        0.86253804, 0.97696257, 0.9988905 , 0.9982788 , 1.0669514 ,\n",
       "        1.0007857 , 1.0552233 , 0.875095  , 1.1246703 , 0.9999558 ,\n",
       "        1.0508082 , 1.3750552 , 1.4476523 , 1.4920187 , 1.3084886 ,\n",
       "        1.507158  , 1.1300577 , 1.5049434 , 0.9711712 , 0.91593397,\n",
       "        1.0241166 , 0.93961465, 1.1684084 , 1.0618036 , 0.93547094,\n",
       "        1.161023  , 1.0187434 , 0.91754115, 0.8405717 , 1.001254  ,\n",
       "        0.7297203 , 0.6878255 , 0.6492481 , 0.76038235, 0.561996  ,\n",
       "        0.59224343, 0.7457868 , 0.99997914, 0.82992816, 0.5688826 ,\n",
       "        0.4180944 , 0.5734765 , 0.75711423, 0.5738042 , 0.8537696 ,\n",
       "        0.99997103, 1.0159504 , 0.9860207 , 0.6750726 , 0.83871907,\n",
       "        0.78036654, 1.8807331 , 0.85964096, 0.88710916, 0.99997544,\n",
       "        0.93053687, 0.98971236, 0.73611605, 1.106727  , 1.8445946 ,\n",
       "        0.9669551 , 0.894588  , 1.3749804 , 0.9857123 , 1.194024  ,\n",
       "        1.0425715 , 0.90970576, 0.91404694, 0.93030936, 0.8313569 ,\n",
       "        0.9988693 , 0.70184577, 0.65680206, 0.4809416 , 0.5424807 ,\n",
       "        0.6934294 , 0.4603794 , 0.978591  , 0.78754   , 0.99015015,\n",
       "        0.66711867, 0.7140066 , 0.7987665 , 0.94490254, 0.8659141 ,\n",
       "        0.94273245, 0.9456655 , 0.98702705, 1.0997036 , 1.0297828 ,\n",
       "        1.1404169 , 0.8779196 , 1.1521574 , 0.9999324 , 0.99879336,\n",
       "        1.4351127 , 1.5565617 , 1.5754553 , 1.421757  , 1.6696177 ,\n",
       "        1.232104  , 1.7695233 , 0.9628079 , 0.87629926, 1.4881517 ,\n",
       "        0.9836792 , 1.2379539 , 1.0573201 , 0.88565356, 0.9095367 ,\n",
       "        0.9208717 , 0.8074266 , 0.9991537 , 0.6543148 , 0.6408347 ,\n",
       "        0.46089798, 0.5196716 , 0.64072293, 0.4270832 , 0.94975984,\n",
       "        0.7633961 , 0.9607249 , 0.6434104 , 0.69055396, 0.7930056 ,\n",
       "        0.9268694 , 0.8633855 , 0.9264553 , 0.90798366, 0.98465234,\n",
       "        1.1273674 , 1.0570171 , 1.2252858 , 0.9040775 , 1.1662766 ,\n",
       "        0.9999223 , 0.991387  , 1.4613097 , 1.6010188 , 1.6095966 ,\n",
       "        1.4686546 , 1.7217495 , 1.2698852 , 1.8961506 , 0.958709  ,\n",
       "        0.85973454, 1.0665371 , 0.89712626, 1.2617903 , 1.1599473 ,\n",
       "        0.89612454, 1.2615039 , 0.977061  , 0.8609047 , 0.75526905,\n",
       "        1.0040255 , 0.57884586, 0.5371449 , 0.5236925 , 0.61982614,\n",
       "        0.3906224 , 0.41518486, 0.6124415 , 0.9999746 , 0.6179566 ,\n",
       "        0.34333637, 0.24229574, 0.38950717, 0.6556137 , 0.40344635,\n",
       "        0.7492742 , 0.99996114, 0.90500265, 0.9776294 , 0.5138747 ,\n",
       "        0.7359613 , 0.6678732 , 2.8810341 , 0.7459072 , 0.8182145 ,\n",
       "        0.99996436, 0.9251977 , 0.9835517 , 0.58560205, 1.3141706 ,\n",
       "        2.7001069 ], dtype=float32),\n",
       " 'ppo/returns/mean': 17.367860794067383,\n",
       " 'ppo/returns/var': 47.119110107421875,\n",
       " 'ppo/val/vpred': 3.695143222808838,\n",
       " 'ppo/val/error': 245.76361083984375,\n",
       " 'ppo/val/clipfrac': 0.70703125,\n",
       " 'ppo/val/mean': 2.9476888179779053,\n",
       " 'ppo/val/var': 6.079530715942383,\n",
       " 'ppo/val/var_explained': -4.215795040130615,\n",
       " 'ppo/learning_rate': 1e-05,\n",
       " 'time/ppo/forward_pass': 0.2872347831726074,\n",
       " 'time/ppo/compute_rewards': 0.0024945735931396484,\n",
       " 'time/ppo/compute_advantages': 0.0017731189727783203,\n",
       " 'time/ppo/optimize_step': 5.142573356628418,\n",
       " 'time/ppo/calc_stats': 0.01602649688720703,\n",
       " 'time/ppo/total': 5.450334310531616}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de UN paso PPO con reward dummy basado en longitud de respuesta.\n",
    "\n",
    "import torch\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=32,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Tomamos un mini-batch desde el dataset interno del PPOTrainer\n",
    "batch = ppo_trainer.dataset[: ppo_trainer.config.batch_size]\n",
    "raw_queries = [q for q in batch[\"query\"] if q.strip()]\n",
    "\n",
    "if not raw_queries:\n",
    "    raise ValueError(\"No hay queries válidas (todas vacías). Revisa el dataset o el filtro.\")\n",
    "\n",
    "# 1) Convertimos las queries (strings) a tensores de tokens\n",
    "query_tensors = []\n",
    "for q in raw_queries:\n",
    "    enc = ppo_tokenizer(\n",
    "        q,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    # enc[\"input_ids\"]: shape (1, seq_len) -> lo dejamos en (seq_len,)\n",
    "    query_tensors.append(enc[\"input_ids\"].squeeze(0).to(device))\n",
    "\n",
    "# 2) Generamos respuestas a partir de cada query_tensor\n",
    "response_tensors = []\n",
    "for q_tensor in query_tensors:\n",
    "    # q_tensor: shape (seq_len,) -> expandimos a batch=1\n",
    "    inputs = {\"input_ids\": q_tensor.unsqueeze(0).to(device)}\n",
    "    outputs = policy_model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=ppo_tokenizer.eos_token_id,  # define padding y evita parte del ruido\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    # Nos quedamos solo con la parte generada (después del prompt)\n",
    "    resp = outputs[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "    response_tensors.append(resp)\n",
    "\n",
    "# 3) Reward dummy = longitud de la respuesta\n",
    "#    IMPORTANTE: TRL quiere lista de tensores, no floats.\n",
    "rewards = []\n",
    "for resp in response_tensors:\n",
    "    length = resp.shape[-1]\n",
    "    rewards.append(torch.tensor(float(length)))  # tensor escalar\n",
    "\n",
    "# 4) Paso PPO: queries, responses y rewards son listas de tensores\n",
    "stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a7090",
   "metadata": {},
   "source": [
    "> En un pipeline real de RLHF, sustituirías el `reward dummy` por el output\n",
    "> del reward model entrenado con `RewardTrainer` (sección 5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf5993",
   "metadata": {},
   "source": [
    "### **Alucinaciones (Hallucinations) y técnicas de mitigación**\n",
    "\n",
    "#### **¿Qué es una alucinación?**\n",
    "\n",
    "Una **alucinación** es una salida del modelo que:\n",
    "\n",
    "- Es gramaticalmente correcta y plausible,\n",
    "- Pero es **falsa**, inventada o no respaldada por el contexto o el conocimiento.\n",
    "\n",
    "Ejemplos típicos:\n",
    "\n",
    "- Inventar referencias bibliográficas / papers inexistentes.\n",
    "- Describir APIs, funciones o rutas de archivos que no existen.\n",
    "- Dar instrucciones técnicas incorrectas con aparente seguridad.\n",
    "\n",
    "#### **Auto-consistencia (*Self-Consistency*)**\n",
    "\n",
    "Estrategia:\n",
    "\n",
    "1. Generar múltiples respuestas para la misma pregunta (sampling).\n",
    "2. Combinar las respuestas (voto mayoritario, agregación estadística).\n",
    "\n",
    "Intuición:\n",
    "\n",
    "- Si la respuesta correcta es relativamente \"estable\" en la distribución del modelo,\n",
    "  tenderá a aparecer con más frecuencia que las alucinaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c8261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \n",
      "\n",
      "Yes, but not in the sense of the capital of the nation of\n",
      "\n",
      "Auto-consistencia (conteo):\n",
      "1 -> yes, but not in the sense of the capital of the nation of\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "question = \"¿Cuál es la capital de Francia?\"\n",
    "\n",
    "def generate_answers(model, tokenizer, prompt, n_samples=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=16,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    gen = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "samples = generate_answers(policy_model, ppo_tokenizer, question, n_samples=5)\n",
    "for i, s in enumerate(samples, 1):\n",
    "    print(f\"[{i}] {s}\")\n",
    "\n",
    "normalized = [s.strip().lower() for s in samples]\n",
    "counts = Counter(normalized)\n",
    "\n",
    "print(\"\\nAuto-consistencia (conteo):\")\n",
    "for ans, c in counts.most_common():\n",
    "    print(c, \"->\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feda746",
   "metadata": {},
   "source": [
    "#### **Cadena de acciones (*Chain-of-Actions*)**\n",
    "\n",
    "Similar a **Chain-of-Thought**, pero centrado en acciones concretas:\n",
    "\n",
    "- \"Primero valida la entrada.\"  \n",
    "- \"Luego comprueba permisos.\"  \n",
    "- \"Después consulta la base de datos.\"  \n",
    "- \"Finalmente formatea la respuesta.\"  \n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- El razonamiento se vuelve **auditado** y más interpretable.\n",
    "- Se pueden insertar **guardrails de seguridad** entre pasos\n",
    "  (por ejemplo, revisar si la acción solicitada es permitida antes de seguir).\n",
    "\n",
    "#### **Recitación (*Recitation*)**\n",
    "\n",
    "Técnica para separar:\n",
    "\n",
    "1. Recuperación de conocimiento.\n",
    "2. Respuesta final.\n",
    "\n",
    "Prompt típico:\n",
    "\n",
    "> \"Antes de responder, enumera los hechos relevantes sobre X.  \n",
    "> Después, usando solo esos hechos, responde la pregunta.\"\n",
    "\n",
    "Esto obliga al modelo a explicitar su \"mini base de conocimientos\" antes de responder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d823b16",
   "metadata": {},
   "source": [
    "#### **Métodos de muestreo para abordar alucinaciones**\n",
    "\n",
    "Los hiperparámetros de generación influyen mucho en la probabilidad de alucinar:\n",
    "\n",
    "- **temperature**  \n",
    "  - Alta -> más creatividad, más riesgo de inventar.  \n",
    "  - Baja -> más conservadora, pero a veces repetitiva.\n",
    "\n",
    "- **top_p (nucleus sampling)** y **top_k**  \n",
    "  - Restringen el conjunto de tokens candidatos en cada paso.\n",
    "  - Valores moderados (ejemplo: `top_p=0.9`) suelen dar buen equilibrio.\n",
    "\n",
    "- **max_new_tokens**  \n",
    "  - Limita longitud de la respuesta para evitar divagaciones.\n",
    "\n",
    "En contextos sensibles (seguridad, medicina, etc.) suele optarse por:\n",
    "\n",
    "- `temperature` baja (0.2–0.7),\n",
    "- `top_p` moderado (0.8–0.9),\n",
    "- y controles adicionales (verificación externa, herramientas, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42126b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " temperature0.3\n",
      "\n",
      "\n",
      "\"I am not sure how to say this, but I think it is a good thing. I am not sure how to say this, but I think it is a good thing. I am not sure how to say this, but I am not sure how to say this, but I am not sure how\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " temperature0.7\n",
      " The following are the results of the experiments.\n",
      "\n",
      "The experiment\n",
      "\n",
      "We first use the SRS to detect a training stimulus, and then to test the strength of the response. The training stimulus is a single word. The training stimulus is a single. The training stimulus is a single word, as it's a\n",
      "\n",
      "\n",
      " temperature1.2\n",
      "\n",
      "\n",
      "I want to see the most of that, that's what we need to do and what we do as our competitors,\n",
      "I want to know all of the rules and what are those, and to say, oh ok.\n",
      "Well i did just give you a chance to just to try and I did a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Resume brevemente qué es RLHF (Reinforcement Learning from Human Feedback).\"\n",
    "\n",
    "for temp in [0.3, 0.7, 1.2]:\n",
    "    inputs = ppo_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = policy_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    gen = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "    text = ppo_tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "    print(f\"\\n temperature{temp}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040592e6",
   "metadata": {},
   "source": [
    "#### **Decoding by Contrasting Layers (DoLa)**\n",
    "\n",
    "**DoLa (Decoding by Contrasting Layers)** es una estrategia de decodificación que:\n",
    "\n",
    "1. Obtiene logits (predicciones antes de softmax) de capas \"tempranas\" y capas \"tardías\" del Transformer.\n",
    "2. Construye una distribución *contrastiva* (por ejemplo, `logits_mature - logits_premature`).\n",
    "3. Aplica softmax sobre esa distribución para elegir el siguiente token.\n",
    "\n",
    "Intuición:\n",
    "\n",
    "- El conocimiento factual tiende a consolidarse en ciertas capas intermedias/tardías.\n",
    "- Restar el ruido de capas tempranas ayuda a reducir algunas alucinaciones.\n",
    "\n",
    "Implementación práctica:\n",
    "\n",
    "- Hay que instrumentar el modelo con *hooks* para leer las activaciones.\n",
    "- Algunas implementaciones añaden parámetros como `dola_layers=\"high\"` a `generate`,\n",
    "  pero **no es parte oficial de la API estándar de `transformers`**.\n",
    "\n",
    "Ejemplo **conceptual** (no ejecutable con la API estándar sin una implementación específica): \n",
    "\n",
    "```python\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    dola_layers=\"high\",  # requiere una implementación concreta de DoLa\n",
    ")\n",
    "```\n",
    "\n",
    "En este cuaderno lo tratamos solo a nivel conceptual; la implementación real depende del repositorio de DoLa que utilices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd91fd",
   "metadata": {},
   "source": [
    "#### **Alucinaciones en contexto (*In-Context Hallucinations*)**\n",
    "\n",
    "Suceden cuando:\n",
    "\n",
    "- El contexto proporcionado realmente contiene la respuesta correcta,\n",
    "- Pero el modelo aun así inventa detalles inconsistentes.\n",
    "\n",
    "Ejemplo clásico:\n",
    "\n",
    "- RAG mal configurado donde el modelo mezcla pasajes no relacionados y produce\n",
    "  una conclusión que no está explícita en ningún documento.\n",
    "\n",
    "#### **Alucinaciones por información irrelevante**\n",
    "\n",
    "Si el prompt incluye muchos detalles irrelevantes, el modelo:\n",
    "\n",
    "- Intentará \"hacer sentido\" de todo,\n",
    "- Puede crear conexiones inexistentes (alucinaciones narrativas).\n",
    "\n",
    "Mitigaciones típicas:\n",
    "\n",
    "1. **Filtrado del contexto** (mejor retriever, rerankers, etc.).  \n",
    "2. **Instrucciones claras**: \"si no estás seguro, di que no lo sabes\".  \n",
    "3. **Controlar longitud y ruido** en el contexto.  \n",
    "4. **Verificación con herramientas externas** (APIs, bases de datos, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc525d08",
   "metadata": {},
   "source": [
    "### **Razonamiento (Reasoning): deductivo, inductivo y abductivo**\n",
    "\n",
    "#### **Razonamiento deductivo**\n",
    "\n",
    "- De reglas generales -> casos particulares.\n",
    "- Si las premisas son verdaderas y la lógica es correcta, la conclusión es necesariamente verdadera.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "> Regla: Todos los mamíferos son de sangre caliente.  \n",
    "> Hecho: Los delfines son mamíferos.  \n",
    "> Conclusión: Los delfines son de sangre caliente.\n",
    "\n",
    "Un LLM alineado debería ser capaz de seguir este patrón de forma consistente.\n",
    "\n",
    "#### **Razonamiento inductivo**\n",
    "\n",
    "- De casos particulares -> regla general (hipótesis).  \n",
    "- Las conclusiones son probables, no 100% seguras.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "> Hemos visto que en varios experimentos, al aumentar el tamaño de los datos\n",
    "> mejora el rendimiento del modelo.  \n",
    "> Hipótesis: probablemente, más datos tiendan a mejorar el modelo en este rango de problemas.\n",
    "\n",
    "#### **Razonamiento abductivo**\n",
    "\n",
    "- De una observación -> mejor explicación posible.  \n",
    "- Típico de diagnósticos (explicaciones plausibles, no ciertas).\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "> Observación: un sistema de clasificación que antes funcionaba bien ahora falla\n",
    "> en datos recientes.  \n",
    "> Explicaciones plausibles:  \n",
    "> - Cambio de distribución de datos (data drift).  \n",
    "> - Bug en el preprocesamiento de la nueva data.  \n",
    "> - Nuevas clases o etiquetas mal definidas.\n",
    "\n",
    "#### **Guiar al modelo con prompts**\n",
    "\n",
    "Estrategias para fomentar buen razonamiento en un LLM:\n",
    "\n",
    "- Pedir pasos explícitos:  \n",
    "  - \"Primero enumera tus premisas, luego razona, luego concluye\".\n",
    "- Asignar un rol:  \n",
    "  - \"Actúa como profesor de lógica\", \"actúa como auditor de seguridad\".\n",
    "- Combinar con auto-consistencia y recitación, especialmente en tareas complejas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39c68bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I hope you are interested in this project. It is not only the best thing to do. It is more important to me to take a look and learn how to todo.\n",
      "\n",
      "I hope you will be happy with this project.\n",
      "\n",
      "I hope you will have a great experience.\n",
      "\n",
      "Thank you very much.\n",
      "\n",
      "I hope that you will enjoy the project.\n",
      "\n",
      "Thank you very much.\n",
      "\n",
      "I hope that you will have a happy life.\n",
      "\n",
      "Thank you very much.\n",
      "Thank you very much.\n",
      "\n",
      "I hope that you will have a happy life.\n",
      "Thank you very much\n"
     ]
    }
   ],
   "source": [
    "reasoning_prompt = (\n",
    "    \"Actúa como un profesor de lógica.\\n\"\n",
    "    \"Clasifica el siguiente fragmento de razonamiento como DEDUCTIVO, \"\n",
    "    \"INDUCTIVO o ABDUCTIVO y explica por qué.\\n\\n\"\n",
    "    \"Texto:\\n\"\n",
    "    \"\\\"En los últimos tres años, todos los modelos que entrenamos con más datos\\n\"\n",
    "    \"obtuvieron mejor rendimiento en validación. Por eso, creemos que entrenar \"\n",
    "    \"con más datos ayudará a nuestro próximo modelo.\\\"\"\n",
    ")\n",
    "\n",
    "inputs = ppo_tokenizer(reasoning_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = policy_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    ")\n",
    "gen = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "print(ppo_tokenizer.batch_decode(gen, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a84d4",
   "metadata": {},
   "source": [
    "#### **Ejemplo opcional con un modelo más grande (LLaMA 7B, conceptual)**\n",
    "\n",
    "A modo de conexión mostramos ejemplo **conceptual**\n",
    "(para hardware potente) usando un modelo tipo `huggyllama/llama-7b` y un argumento\n",
    "`dola_layers='high'` (propio de una implementación concreta de DoLa).\n",
    "\n",
    "> **Advertencia:** no ejecutes esto si no tienes GPU potente y el modelo descargado.\n",
    "> Es principalmente ilustrativo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1022175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "#\n",
    "# MODEL_NAME = \"huggyllama/llama-7b\"\n",
    "#\n",
    "# tokenizer_llama = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "#\n",
    "# text = \"Who shared a dorm with Harry Potter?\"\n",
    "# inputs = tokenizer_llama(text, return_tensors=\"pt\").to(model_llama.device)\n",
    "# output = model_llama.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=False,\n",
    "#     max_new_tokens=50,\n",
    "#     # dola_layers='high',  # requerido por una implementación específica de DoLa\n",
    "# )\n",
    "# print(tokenizer_llama.batch_decode(\n",
    "#     output[:, inputs[\"input_ids\"].shape[-1]:],\n",
    "#     skip_special_tokens=True\n",
    "# ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
