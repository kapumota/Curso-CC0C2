{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8c2ada",
   "metadata": {},
   "source": [
    "### **Entrenamiento de alineación y razonamiento con RLHF**\n",
    "\n",
    "Este cuaderno está pensado para ejecutarse **dentro de Docker** con un entorno que ya tenga instalados:\n",
    "\n",
    "- `torch`\n",
    "- `transformers`\n",
    "- `datasets`\n",
    "- `trl`\n",
    "- (opcional) `accelerate` y soporte de GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65128002",
   "metadata": {},
   "source": [
    "#### **Preparación del entorno**\n",
    "\n",
    "En esta sección:\n",
    "\n",
    "- Elegimos dispositivo (CPU / GPU).\n",
    "- Verificamos versiones de librerías clave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import trl\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"TRL:\", trl.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8012b4c",
   "metadata": {},
   "source": [
    "#### **Entrenamiento de alineación (Alignment Training)**\n",
    "\n",
    "**Entrenamiento de alineación** = conjunto de técnicas para que un modelo de lenguaje grande (LLM) sea:\n",
    "\n",
    "- **Útil (helpful)**: responde a lo que el usuario realmente necesita.\n",
    "- **Inofensivo (harmless)**: evita instrucciones dañinas, ilegales o poco éticas.\n",
    "- **Honesto (honest)**: evita *alucinaciones* y reconoce sus límites.\n",
    "\n",
    "Pipeline típico usual elemental:\n",
    "\n",
    "1. **Pre-entrenamiento** (*pre-training*):  \n",
    "   El modelo aprende estadística del lenguaje en grandes corpus, optimizando una tarea de modelado de lenguaje (predecir el siguiente token).\n",
    "\n",
    "2. **Supervised Fine-Tuning (SFT)**:  \n",
    "   Se ajusta el modelo con pares *(instrucción, respuesta deseada)*.  \n",
    "   - Datos generados por humanos (anotadores) o por otros modelos filtrados.\n",
    "   - Objetivo: que el modelo hable en \"modo asistente\".\n",
    "\n",
    "3. **Modelado de recompensas (Reward modeling)**:  \n",
    "   Se entrena un modelo que asigna un **score escalar** a una respuesta, dado un prompt.  \n",
    "   - Usa datos de **preferencias humanas**: pares *(chosen, rejected)*.\n",
    "   - El modelo aprende a puntuar más alto las respuestas alineadas.\n",
    "\n",
    "4. **RLHF (Reinforcement Learning from Human Feedback)**:  \n",
    "   El reward model actúa como **juez**. Con algoritmos como PPO:\n",
    "   - La *política (policy)* (modelo generador) produce respuestas.\n",
    "   - El modelo de recompensa las puntúa.\n",
    "   - La política  se actualiza para maximizar la recompensa (con regularización por KL para no alejarse mucho del modelo base).\n",
    "\n",
    "En este cuaderno trabajaremos con un subconjunto del dataset **`Anthropic/hh-rlhf`** para ilustrar el *reward modeling** y un esqueleto de RLHF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5511038",
   "metadata": {},
   "source": [
    "### **Tipos de retroalimentación humana y dataset `Anthropic/hh-rlhf`**\n",
    "\n",
    "##### **Tipos de human feedback**\n",
    "\n",
    "En la práctica, la retroalimentación humana puede tomar varias formas:\n",
    "\n",
    "1. **Etiquetas directas (supervisión estándar)**  \n",
    "   - Por ejemplo: *\"esta respuesta es correcta/incorrecta\"*, *\"esta respuesta es tóxica\"*, etc.\n",
    "\n",
    "2. **Preferencias pareadas (pairwise preferences)**  \n",
    "   - Dado un mismo prompt, el humano ve dos respuestas y elige cuál prefiere.\n",
    "   - Es justo el caso de `hh-rlhf`: `chosen` vs `rejected`.\n",
    "\n",
    "3. **Recompensas escalares**  \n",
    "   - El humano da un score (por ejemplo, entre 1 y 5).\n",
    "\n",
    "4. **Feedback estructurado**  \n",
    "   - Explicaciones, correcciones paso a paso, anotaciones de razonamiento, etc.\n",
    "\n",
    "En RLHF nos interesan especialmente las **preferencias pareadas**, porque permiten entrenar un modelo de recompensas robusto con relativamente pocos datos.\n",
    "\n",
    "#### **`Anthropic/hh-rlhf`**\n",
    "\n",
    "`Anthropic/hh-rlhf` contiene diálogos tipo chat con dos variantes para cada ejemplo:\n",
    "\n",
    "- `chosen`: respuesta preferida (más alineada y segura).\n",
    "- `rejected`: respuesta descartada (menos segura, menos útil).\n",
    "\n",
    "En un escenario de seguridad, por ejemplo, `chosen` suele ser la respuesta que:\n",
    "\n",
    "- **Rechaza** ayudar a hacer daño (ejemplo, hackear correos, fabricar explosivos).\n",
    "- Ofrece alternativas seguras o educativas.\n",
    "\n",
    "Mientras que `rejected` a menudo:\n",
    "\n",
    "- Da detalles de cómo ejecutar la acción dañina,\n",
    "- O al menos no la bloquea de forma clara.\n",
    "\n",
    "Nuestro reward model aprenderá a asignar **score mayor** a `chosen` que a `rejected`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa694a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargamos un subconjunto pequeño para exploración\n",
    "raw_ds = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:200]\")\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c101c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = raw_ds[0]\n",
    "print(example.keys())\n",
    "\n",
    "print(\"\\n Escogida (respuesta preferida) \\n\")\n",
    "print(example[\"chosen\"][:600])\n",
    "\n",
    "print(\"\\n Rechazada (respuesta no deseada)\\n\")\n",
    "print(example[\"rejected\"][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b210d1a",
   "metadata": {},
   "source": [
    "#### **Preparar los datos para `RewardTrainer`**\n",
    "\n",
    "El `RewardTrainer` de TRL espera un dataset con, al menos, columnas como:\n",
    "\n",
    "- `input_ids_chosen`, `attention_mask_chosen`\n",
    "- `input_ids_rejected`, `attention_mask_rejected`\n",
    "\n",
    "Pasos:\n",
    "\n",
    "1. Elegimos un modelo base para el modelo de recompensas (aquí, **`gpt2-medium`**, mayor que `sshleifer/tiny-gpt2`).  \n",
    "2. Tokenizamos `chosen` y `rejected` con la misma longitud máxima.  \n",
    "3. Filtramos ejemplos demasiado largos.\n",
    "\n",
    "Esto nos dará un dataset listo para el entrenamiento del modelo de recompensa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reward_model_name = \"gpt2-medium\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def build_pair(example):\n",
    "    chosen_text = example[\"chosen\"]\n",
    "    rejected_text = example[\"rejected\"]\n",
    "\n",
    "    chosen_tokens = tokenizer(\n",
    "        chosen_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "    rejected_tokens = tokenizer(\n",
    "        rejected_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_tokens[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_tokens[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_tokens[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_tokens[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "processed_ds = raw_ds.map(build_pair, remove_columns=raw_ds.column_names)\n",
    "\n",
    "processed_ds = processed_ds.filter(\n",
    "    lambda x: len(x[\"input_ids_chosen\"]) <= max_length\n",
    "              and len(x[\"input_ids_rejected\"]) <= max_length\n",
    ")\n",
    "\n",
    "processed_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28aa9da",
   "metadata": {},
   "source": [
    "#### **Entrenamiento de un reward model con `RewardTrainer`**\n",
    "\n",
    "Usaremos:\n",
    "\n",
    "- `AutoModelForSequenceClassification` con `num_labels = 1`.\n",
    "- `RewardTrainer` de TRL con una configuración simple.\n",
    "\n",
    "La pérdida que se optimiza es de tipo **Bradley-Terry**:\n",
    "el modelo recibe scores $r_{chosen}, r_{rejected}$ y aprende que:\n",
    "\n",
    "- $r_{chosen} > r_{rejected}$\n",
    "\n",
    "De forma más formal, se maximiza algo como:\n",
    "\n",
    "$$\n",
    "\\log \\sigma(r_{chosen} - r_{rejected})\n",
    "$$\n",
    "\n",
    "donde $\\sigma$ es la sigmoide logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    reward_model_name,\n",
    "    num_labels=1,\n",
    ")\n",
    "reward_model.to(device)\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "reward_model.config.use_cache = False\n",
    "\n",
    "reward_train_config = RewardConfig(\n",
    "    output_dir=\"./reward-model-hh-rlhf-demo\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,  # demo: 1 epoch\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=False,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_train_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=processed_ds,\n",
    ")\n",
    "\n",
    "print(\"RewardTrainer inicializado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1821fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El entrenamiento completo puede ser costoso.\n",
    "# Para un entorno de demo puedes reducir el número de ejemplos o epochs.\n",
    "#\n",
    "# Descomenta para entrenar de verdad:\n",
    "#\n",
    "# reward_trainer.train()\n",
    "# reward_trainer.save_model()\n",
    "# tokenizer.save_pretrained(\"./reward-model-hh-rlhf-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e5da8",
   "metadata": {},
   "source": [
    "#### **Esqueleto de RLHF con `PPOTrainer`**\n",
    "\n",
    "El ciclo básico de PPO (muy simplificado) es:\n",
    "\n",
    "1. Tomar un batch de **prompts** (queries).\n",
    "2. La política(modelo generador) produce respuestas.\n",
    "3. El modelo de recompensa asigna una recompensa a cada respuesta.\n",
    "4. PPO ajusta la política para maximizar la recompensa, penalizando desviarse\n",
    "   demasiado del modelo de referencia (regularización por KL).\n",
    "\n",
    "Aquí mostraremos un **esqueleto** donde la recompensa es *dummy* (por longitud)\n",
    "para ilustrar la API de `PPOTrainer`. En un pipeline real (no incluido aquí), se conectaría con el modelo de recompensas entrenado en la sección anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3054eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "policy_model_name = \"gpt2\"  # policy para PPO (más grande que tiny)\n",
    "ppo_tokenizer = AutoTokenizer.from_pretrained(policy_model_name)\n",
    "if ppo_tokenizer.pad_token is None:\n",
    "    ppo_tokenizer.pad_token = ppo_tokenizer.eos_token\n",
    "\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model_name)\n",
    "policy_model.to(device)\n",
    "ref_model = create_reference_model(policy_model)  # modelo de referencia fijo\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=policy_model_name,\n",
    "    batch_size=2,\n",
    "    forward_batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    mini_batch_size=1,\n",
    "    log_with=None,\n",
    ")\n",
    "\n",
    "def extract_prompt(example):\n",
    "    \"\"\"\n",
    "    Extrae una línea no vacía del campo 'chosen'.\n",
    "    Si no encuentra nada, usa un prompt por defecto.\n",
    "    \"\"\"\n",
    "    text = example[\"chosen\"] or \"\"\n",
    "    # nos quedamos solo con líneas que tengan contenido\n",
    "    lines = [l for l in text.splitlines() if l.strip()]\n",
    "    if lines:\n",
    "        first_line = lines[0].strip()\n",
    "    else:\n",
    "        # fallback seguro (no debe quedar vacío)\n",
    "        first_line = \"Explain briefly what RLHF (Reinforcement Learning from Human Feedback) is.\"\n",
    "    return {\"query\": first_line}\n",
    "\n",
    "# Construir dataset con queries no vacías\n",
    "ppo_ds = raw_ds.map(extract_prompt)\n",
    "\n",
    "# Filtrar cualquier ejemplo que aun así tenga query vacía\n",
    "ppo_ds = ppo_ds.filter(lambda ex: len(ex[\"query\"].strip()) > 0)\n",
    "\n",
    "# Usar un mini-dataset para la demo\n",
    "ppo_ds = ppo_ds.select(range(min(8, len(ppo_ds))))\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    config=ppo_config,\n",
    "    tokenizer=ppo_tokenizer,\n",
    "    dataset=ppo_ds,\n",
    ")\n",
    "\n",
    "print(\"PPOTrainer inicializado con queries limpias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bda570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de UN paso PPO con reward dummy basado en longitud de respuesta.\n",
    "\n",
    "import torch\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=32,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Tomamos un mini-batch desde el dataset interno del PPOTrainer\n",
    "batch = ppo_trainer.dataset[: ppo_trainer.config.batch_size]\n",
    "raw_queries = [q for q in batch[\"query\"] if q.strip()]\n",
    "\n",
    "if not raw_queries:\n",
    "    raise ValueError(\"No hay queries válidas (todas vacías). Revisa el dataset o el filtro.\")\n",
    "\n",
    "# 1) Convertimos las queries (strings) a tensores de tokens\n",
    "query_tensors = []\n",
    "for q in raw_queries:\n",
    "    enc = ppo_tokenizer(\n",
    "        q,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    # enc[\"input_ids\"]: shape (1, seq_len) -> lo dejamos en (seq_len,)\n",
    "    query_tensors.append(enc[\"input_ids\"].squeeze(0).to(device))\n",
    "\n",
    "# 2) Generamos respuestas a partir de cada query_tensor\n",
    "response_tensors = []\n",
    "for q_tensor in query_tensors:\n",
    "    # q_tensor: shape (seq_len,) -> expandimos a batch=1\n",
    "    inputs = {\"input_ids\": q_tensor.unsqueeze(0).to(device)}\n",
    "    outputs = policy_model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=ppo_tokenizer.eos_token_id,  # define padding y evita parte del ruido\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    # Nos quedamos solo con la parte generada (después del prompt)\n",
    "    resp = outputs[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "    response_tensors.append(resp)\n",
    "\n",
    "# 3) Reward dummy = longitud de la respuesta\n",
    "#    IMPORTANTE: TRL quiere lista de tensores, no floats.\n",
    "rewards = []\n",
    "for resp in response_tensors:\n",
    "    length = resp.shape[-1]\n",
    "    rewards.append(torch.tensor(float(length)))  # tensor escalar\n",
    "\n",
    "# 4) Paso PPO: queries, responses y rewards son listas de tensores\n",
    "stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a7090",
   "metadata": {},
   "source": [
    "> En un pipeline real de RLHF, sustituirías el `reward dummy` por el output\n",
    "> del reward model entrenado con `RewardTrainer` (sección 5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf5993",
   "metadata": {},
   "source": [
    "### **Alucinaciones (Hallucinations) y técnicas de mitigación**\n",
    "\n",
    "#### **¿Qué es una alucinación?**\n",
    "\n",
    "Una **alucinación** es una salida del modelo que:\n",
    "\n",
    "- Es gramaticalmente correcta y plausible,\n",
    "- Pero es **falsa**, inventada o no respaldada por el contexto o el conocimiento.\n",
    "\n",
    "Ejemplos típicos:\n",
    "\n",
    "- Inventar referencias bibliográficas / papers inexistentes.\n",
    "- Describir APIs, funciones o rutas de archivos que no existen.\n",
    "- Dar instrucciones técnicas incorrectas con aparente seguridad.\n",
    "\n",
    "#### **Auto-consistencia (*Self-Consistency*)**\n",
    "\n",
    "Estrategia:\n",
    "\n",
    "1. Generar múltiples respuestas para la misma pregunta (sampling).\n",
    "2. Combinar las respuestas (voto mayoritario, agregación estadística).\n",
    "\n",
    "Intuición:\n",
    "\n",
    "- Si la respuesta correcta es relativamente \"estable\" en la distribución del modelo,\n",
    "  tenderá a aparecer con más frecuencia que las alucinaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "question = \"¿Cuál es la capital de Francia?\"\n",
    "\n",
    "def generate_answers(model, tokenizer, prompt, n_samples=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = modelo.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=16,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    gen = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "samples = generate_answers(policy_model, ppo_tokenizer, question, n_samples=5)\n",
    "for i, s in enumerate(samples, 1):\n",
    "    print(f\"[{i}] {s}\")\n",
    "\n",
    "normalized = [s.strip().lower() for s in samples]\n",
    "counts = Counter(normalized)\n",
    "\n",
    "print(\"\\nAuto-consistencia (conteo):\")\n",
    "for ans, c in counts.most_common():\n",
    "    print(c, \"->\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feda746",
   "metadata": {},
   "source": [
    "#### **Cadena de acciones (*Chain-of-Actions*)**\n",
    "\n",
    "Similar a **Chain-of-Thought**, pero centrado en acciones concretas:\n",
    "\n",
    "- \"Primero valida la entrada.\"  \n",
    "- \"Luego comprueba permisos.\"  \n",
    "- \"Después consulta la base de datos.\"  \n",
    "- \"Finalmente formatea la respuesta.\"  \n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- El razonamiento se vuelve **auditado** y más interpretable.\n",
    "- Se pueden insertar **guardrails de seguridad** entre pasos\n",
    "  (por ejemplo, revisar si la acción solicitada es permitida antes de seguir).\n",
    "\n",
    "#### **Recitación (*Recitation*)**\n",
    "\n",
    "Técnica para separar:\n",
    "\n",
    "1. Recuperación de conocimiento.\n",
    "2. Respuesta final.\n",
    "\n",
    "Prompt típico:\n",
    "\n",
    "> \"Antes de responder, enumera los hechos relevantes sobre X.  \n",
    "> Después, usando solo esos hechos, responde la pregunta.\"\n",
    "\n",
    "Esto obliga al modelo a explicitar su \"mini base de conocimientos\" antes de responder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d823b16",
   "metadata": {},
   "source": [
    "#### **Métodos de muestreo para abordar alucinaciones**\n",
    "\n",
    "Los hiperparámetros de generación influyen mucho en la probabilidad de alucinar:\n",
    "\n",
    "- **temperature**  \n",
    "  - Alta -> más creatividad, más riesgo de inventar.  \n",
    "  - Baja -> más conservadora, pero a veces repetitiva.\n",
    "\n",
    "- **top_p (nucleus sampling)** y **top_k**  \n",
    "  - Restringen el conjunto de tokens candidatos en cada paso.\n",
    "  - Valores moderados (ejemplo: `top_p=0.9`) suelen dar buen equilibrio.\n",
    "\n",
    "- **max_new_tokens**  \n",
    "  - Limita longitud de la respuesta para evitar divagaciones.\n",
    "\n",
    "En contextos sensibles (seguridad, medicina, etc.) suele optarse por:\n",
    "\n",
    "- `temperature` baja (0.2-0.7),\n",
    "- `top_p` moderado (0.8-0.9),\n",
    "- y controles adicionales (verificación externa, herramientas, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42126b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Resume brevemente qué es RLHF (Reinforcement Learning from Human Feedback).\"\n",
    "\n",
    "for temp in [0.3, 0.7, 1.2]:\n",
    "    inputs = ppo_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = policy_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    gen = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "    text = ppo_tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "    print(f\"\\n temperature{temp}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040592e6",
   "metadata": {},
   "source": [
    "#### **Decoding by Contrasting Layers (DoLa)**\n",
    "\n",
    "**DoLa (Decoding by Contrasting Layers)** es una estrategia de decodificación que:\n",
    "\n",
    "1. Obtiene logits (predicciones antes de softmax) de capas \"tempranas\" y capas \"tardías\" del Transformer.\n",
    "2. Construye una distribución *contrastiva* (por ejemplo, `logits_mature - logits_premature`).\n",
    "3. Aplica softmax sobre esa distribución para elegir el siguiente token.\n",
    "\n",
    "Intuición:\n",
    "\n",
    "- El conocimiento factual tiende a consolidarse en ciertas capas intermedias/tardías.\n",
    "- Restar el ruido de capas tempranas ayuda a reducir algunas alucinaciones.\n",
    "\n",
    "Implementación práctica:\n",
    "\n",
    "- Hay que instrumentar el modelo con *hooks* para leer las activaciones.\n",
    "- Algunas implementaciones añaden parámetros como `dola_layers=\"high\"` a `generate`,\n",
    "  pero **no es parte oficial de la API estándar de `transformers`**.\n",
    "\n",
    "Ejemplo **conceptual** (no ejecutable con la API estándar sin una implementación específica): \n",
    "\n",
    "```python\n",
    "outputs = modelo.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    dola_layers=\"high\",  # requiere una implementación concreta de DoLa\n",
    ")\n",
    "```\n",
    "\n",
    "En este cuaderno lo tratamos solo a nivel conceptual; la implementación real depende del repositorio de DoLa que utilices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd91fd",
   "metadata": {},
   "source": [
    "#### **Alucinaciones en contexto (*In-Context Hallucinations*)**\n",
    "\n",
    "Suceden cuando:\n",
    "\n",
    "- El contexto proporcionado realmente contiene la respuesta correcta,\n",
    "- Pero el modelo aun así inventa detalles inconsistentes.\n",
    "\n",
    "Ejemplo clásico:\n",
    "\n",
    "- RAG mal configurado donde el modelo mezcla pasajes no relacionados y produce\n",
    "  una conclusión que no está explícita en ningún documento.\n",
    "\n",
    "#### **Alucinaciones por información irrelevante**\n",
    "\n",
    "Si el prompt incluye muchos detalles irrelevantes, el modelo:\n",
    "\n",
    "- Intentará \"hacer sentido\" de todo,\n",
    "- Puede crear conexiones inexistentes (alucinaciones narrativas).\n",
    "\n",
    "Mitigaciones típicas:\n",
    "\n",
    "1. **Filtrado del contexto** (mejor retriever, rerankers, etc.).  \n",
    "2. **Instrucciones claras**: \"si no estás seguro, di que no lo sabes\".  \n",
    "3. **Controlar longitud y ruido** en el contexto.  \n",
    "4. **Verificación con herramientas externas** (APIs, bases de datos, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc525d08",
   "metadata": {},
   "source": [
    "### **Razonamiento (Reasoning): deductivo, inductivo y abductivo**\n",
    "\n",
    "#### **Razonamiento deductivo**\n",
    "\n",
    "- De reglas generales -> casos particulares.\n",
    "- Si las premisas son verdaderas y la lógica es correcta, la conclusión es necesariamente verdadera.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "> Regla: Todos los mamíferos son de sangre caliente.  \n",
    "> Hecho: Los delfines son mamíferos.  \n",
    "> Conclusión: Los delfines son de sangre caliente.\n",
    "\n",
    "Un LLM alineado debería ser capaz de seguir este patrón de forma consistente.\n",
    "\n",
    "#### **Razonamiento inductivo**\n",
    "\n",
    "- De casos particulares -> regla general (hipótesis).  \n",
    "- Las conclusiones son probables, no 100% seguras.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "> Hemos visto que en varios experimentos, al aumentar el tamaño de los datos\n",
    "> mejora el rendimiento del modelo.  \n",
    "> Hipótesis: probablemente, más datos tiendan a mejorar el modelo en este rango de problemas.\n",
    "\n",
    "#### **Razonamiento abductivo**\n",
    "\n",
    "- De una observación -> mejor explicación posible.  \n",
    "- Típico de diagnósticos (explicaciones plausibles, no ciertas).\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "> Observación: un sistema de clasificación que antes funcionaba bien ahora falla\n",
    "> en datos recientes.  \n",
    "> Explicaciones plausibles:  \n",
    "> - Cambio de distribución de datos (data drift).  \n",
    "> - Bug en el preprocesamiento de la nueva data.  \n",
    "> - Nuevas clases o etiquetas mal definidas.\n",
    "\n",
    "#### **Guiar al modelo con prompts**\n",
    "\n",
    "Estrategias para fomentar buen razonamiento en un LLM:\n",
    "\n",
    "- Pedir pasos explícitos:  \n",
    "  - \"Primero enumera tus premisas, luego razona, luego concluye\".\n",
    "- Asignar un rol:  \n",
    "  - \"Actúa como profesor de lógica\", \"actúa como auditor de seguridad\".\n",
    "- Combinar con auto-consistencia y recitación, especialmente en tareas complejas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt = (\n",
    "    \"Actúa como un profesor de lógica.\\n\"\n",
    "    \"Clasifica el siguiente fragmento de razonamiento como DEDUCTIVO, \"\n",
    "    \"INDUCTIVO o ABDUCTIVO y explica por qué.\\n\\n\"\n",
    "    \"Texto:\\n\"\n",
    "    \"\\\"En los últimos tres años, todos los modelos que entrenamos con más datos\\n\"\n",
    "    \"obtuvieron mejor rendimiento en validación. Por eso, creemos que entrenar \"\n",
    "    \"con más datos ayudará a nuestro próximo modelo.\\\"\"\n",
    ")\n",
    "\n",
    "inputs = ppo_tokenizer(reasoning_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = policy_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    ")\n",
    "gen = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "print(ppo_tokenizer.batch_decode(gen, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a84d4",
   "metadata": {},
   "source": [
    "#### **Ejemplo opcional con un modelo más grande (LLaMA 7B, conceptual)**\n",
    "\n",
    "A modo de conexión mostramos ejemplo **conceptual**\n",
    "(para hardware potente) usando un modelo tipo `huggyllama/llama-7b` y un argumento\n",
    "`dola_layers='high'` (propio de una implementación concreta de DoLa).\n",
    "\n",
    "> **Advertencia:** no ejecutes esto si no tienes GPU potente y el modelo descargado.\n",
    "> Es principalmente ilustrativo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "#\n",
    "# MODEL_NAME = \"huggyllama/llama-7b\"\n",
    "#\n",
    "# tokenizer_llama = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "#\n",
    "# text = \"Who shared a dorm with Harry Potter?\"\n",
    "# inputs = tokenizer_llama(text, return_tensors=\"pt\").to(model_llama.device)\n",
    "# output = model_llama.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=False,\n",
    "#     max_new_tokens=50,\n",
    "#     # dola_layers='high',  # requerido por una implementación específica de DoLa\n",
    "# )\n",
    "# print(tokenizer_llama.batch_decode(\n",
    "#     output[:, inputs[\"input_ids\"].shape[-1]:],\n",
    "#     skip_special_tokens=True\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206112fc-8077-4c44-8332-75268446783a",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "**Ejercicio 1 - Pipeline de alineación (Alignment Pipeline)**  \n",
    "Explica con tus palabras el pipeline completo de alineación para un LLM:\n",
    "\n",
    "1. Pre-entrenamiento (pre-training).  \n",
    "2. Supervised Fine-Tuning (SFT).  \n",
    "3. Reward modeling.  \n",
    "4. RLHF (PPO u otro método).\n",
    "\n",
    "Para cada etapa, responde:\n",
    "\n",
    "- ¿Cuál es el objetivo principal de esa etapa?\n",
    "- ¿Qué tipo de datos se necesitan?\n",
    "- ¿Qué pasaría si esa etapa se hiciera mal o se omitiera?\n",
    "\n",
    "**Ejercicio 2 - Tipos de retroalimentación humana (Types of Human Feedback)**  \n",
    "Enumera y describe al menos **tres** tipos de feedback humano que se pueden usar para alinear un modelo:\n",
    "\n",
    "- Etiquetas directas (clasificación).\n",
    "- Preferencias pareadas (pairwise preferences).\n",
    "- Recompensas escalares, etc.\n",
    "\n",
    "Para cada tipo de feedback, discute:\n",
    "\n",
    "- Una ventaja.\n",
    "- Una desventaja.\n",
    "- Un ejemplo concreto de cómo se usaría con un LLM.\n",
    "\n",
    "**Ejercicio 3 - `chosen` vs `rejected` en `hh-rlhf`**  \n",
    "Escoge manualmente **un ejemplo** del dataset `Anthropic/hh-rlhf` (puedes usar la celda donde se imprime `example[\"chosen\"]` y `example[\"rejected\"]`):\n",
    "\n",
    "1. Resume el contexto del diálogo.  \n",
    "2. Explica por qué la respuesta `chosen` es más alineada que la `rejected`, en términos de:\n",
    "   - Seguridad (harmlessness).\n",
    "   - Utilidad (helpfulness).\n",
    "   - Honestidad (honesty).\n",
    "\n",
    "\n",
    "**Ejercicio 4 - Alucinaciones y mitigación (Hallucinations and Mitigation)**  \n",
    "Define qué es una *alucinación* en el contexto de LLMs y describe al menos **tres** de las siguientes técnicas, explicando cómo ayudan a reducirlas:\n",
    "\n",
    "- Auto-consistencia (Self-Consistency).\n",
    "- Cadena de acciones (Chain-of-Actions).\n",
    "- Recitación (Recitation).\n",
    "- Control de sampling (`temperature`, `top_p`, `top_k`).\n",
    "- Decoding by Contrasting Layers (DoLa).\n",
    "\n",
    "Para cada técnica, da un ejemplo de cómo se podría aplicar en tu propio sistema (aunque sea de forma conceptual).\n",
    "\n",
    "**Ejercicio 5 - Razonamiento: deductivo, inductivo, abductivo**  \n",
    "Da un ejemplo propio (no el del cuaderno) de cada tipo de razonamiento:\n",
    "\n",
    "1. Deductivo.  \n",
    "2. Inductivo.  \n",
    "3. Abductivo.  \n",
    "\n",
    "Para cada ejemplo, explica por qué corresponde a ese tipo de razonamiento y cómo un LLM podría equivocarse al intentar reproducirlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3ae14-db28-44e1-a2e9-4eb7090e3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
