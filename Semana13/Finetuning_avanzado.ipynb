{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f538a3db-44ce-4d96-818a-1cf57b4f9aa7",
   "metadata": {},
   "source": [
    "### **Técnicas avanzadas de ajuste fino (versión elemental)**\n",
    "\n",
    "En este cuaderno veremos, de forma introductoria y práctica, varias técnicas avanzadas de ajuste fino de modelos de lenguaje:\n",
    "\n",
    " - Preentrenamiento continuo (*Continual Pre-Training*)\n",
    " - Repetición/Memoria de reproducción (*Replay / Memory Replay*)\n",
    " - Expansión de parámetros (*Parameter Expansion*)\n",
    " - Ajuste fino eficiente en parámetro (*Parameter-Efficient Fine-Tuning*)\n",
    " - Adición de nuevos parámetros (*Adding New Parameters*)\n",
    " - Métodos de subconjuntos (*Subset Methods*)\n",
    " \n",
    "Tambien,  usaremos la combinación de múltiples modelos (*Combining Multiple Models*), que incluye:\n",
    "\n",
    "   - **Ensamble de modelos** (*Model Ensembling*)\n",
    "   - **Fusión de modelos** (*Model Fusion*)\n",
    "   -  **Fusión de adaptadores** (*Adapter Merging*)\n",
    "\n",
    "Usaremos un modelo pequeño de Hugging Face para que todo pueda ejecutarse en CPU dentro del contenedor Docker, usando solo las librerías de tu `requirements.txt` (sin APIs externas ni credenciales).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb521c9",
   "metadata": {},
   "source": [
    "#### **Setup: modelo base elemental**\n",
    "\n",
    "Trabajaremos con un modelo **tiny GPT-2** como *language model* causal.  No es un modelo fuerte, pero es suficiente para ilustrar ideas de fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2aada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import textwrap\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "modelo = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "modelo.eval()\n",
    "\n",
    "def tokenize_batch(texts, max_length=64):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "def simple_generate(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 40,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    use_greedy: bool = False,\n",
    ") -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    if use_greedy:\n",
    "        # Decodificación determinista (menos creativa, menos caótica)\n",
    "        gen_kwargs[\"do_sample\"] = False\n",
    "    else:\n",
    "        # Muestreo controlado\n",
    "        gen_kwargs.update(\n",
    "            dict(\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                # si quieres, puedes quitar top_k para no restringir demasiado\n",
    "                # top_k=50,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = modelo.generate(\n",
    "            **inputs,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "\n",
    "    # Sólo decodificamos la continuación (sin repetir el prompt)\n",
    "    generated_ids = out_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Opcional: envolver el texto para que se vea más bonito\n",
    "    return textwrap.fill(text, width=80)\n",
    "\n",
    "\n",
    "print(\"Greedy\")\n",
    "print(simple_generate(\"The transformer model is\", max_new_tokens=20, use_greedy=True))\n",
    "\n",
    "print(\"\\Muestreo controlado\")\n",
    "print(simple_generate(\"The transformer model is\", max_new_tokens=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec3d09",
   "metadata": {},
   "source": [
    "#### **Preentrenamiento continuo**\n",
    "\n",
    "**Idea:** seguir pre-entrenando un modelo ya preentrenado sobre un nuevo corpus (por ejemplo, un dominio específico).\n",
    "\n",
    "- Típicamente se mantiene el **mismo objetivo** de pre-entrenamiento (MLM, causal LM, etc.).  \n",
    "- El modelo adapta su distribución interna al nuevo dominio sin \"olvidar\" completamente lo anterior (si se hace bien).\n",
    "\n",
    "Ejemplo típico: un modelo general de lenguaje → se continúa el pre-entrenamiento sobre artículos médicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_base = [\n",
    "    \"Transformers are a powerful neural network architecture.\",\n",
    "    \"Language models can generate coherent text.\",\n",
    "    \"Deep learning has transformed natural language processing.\",\n",
    "]\n",
    "\n",
    "texts_new_domain = [\n",
    "    \"Reinforcement learning with human feedback is used to align large language models.\",\n",
    "    \"Parameter-efficient fine-tuning allows adapting models with fewer trainable parameters.\",\n",
    "    \"Continual pre-training on domain-specific data can improve downstream performance.\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_ct = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model_ct.resize_token_embeddings(len(tokenizer))\n",
    "model_ct.config.pad_token_id = tokenizer.pad_token_id\n",
    "model_ct = model_ct.to(device)\n",
    "\n",
    "\n",
    "def tokenize_batch(texts, max_length=64):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_lm_batch(texts):\n",
    "    enc = tokenize_batch(texts)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    attn_mask = enc[\"attention_mask\"]\n",
    "    labels = input_ids.clone()\n",
    "    labels[attn_mask == 0] = -100\n",
    "    return input_ids.to(device), attn_mask.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "# Un solo paso de \"continual pre-training\" sobre el dominio nuevo (demo)\n",
    "optimizer = torch.optim.AdamW(model_ct.parameters(), lr=5e-5)\n",
    "\n",
    "model_ct.train()\n",
    "input_ids, attn_mask, labels = build_lm_batch(texts_new_domain)\n",
    "\n",
    "loss = model_ct(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    labels=labels,\n",
    ").loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Loss (un solo paso de continual pre-training):\", float(loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbb4d9",
   "metadata": {},
   "source": [
    "#### **Repetición/Memoria de reproducción**\n",
    "\n",
    "Problema clásico en continual learning: **catastrophic forgetting**.  \n",
    "Si solo entrenas en el nuevo dominio, el modelo puede olvidar el dominio original.\n",
    "\n",
    "**Idea**:\n",
    "\n",
    "- Mantienes una pequeña memoria (buffer) con ejemplos de dominios anteriores.  \n",
    "- En cada paso de entrenamiento sobre el nuevo dominio, mezclas algunos ejemplos del buffer.\n",
    "\n",
    "Esto ayuda a que el modelo \"repase\" el conocimiento previo y no lo olvide tan rápido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Memoria de ejemplos antiguos (dominio base)\n",
    "replay_buffer = list(texts_base)\n",
    "\n",
    "def sample_mixed_batch(new_texts, replay_buffer, replay_ratio=0.5, batch_size=4):\n",
    "    num_replay = int(batch_size * replay_ratio)\n",
    "    num_new = batch_size - num_replay\n",
    "\n",
    "    new_samples = (new_texts * ((num_new // len(new_texts)) + 1))[:num_new]\n",
    "    replay_samples = (replay_buffer * ((num_replay // len(replay_buffer)) + 1))[:num_replay]\n",
    "\n",
    "    batch = new_samples + replay_samples\n",
    "    shuffle(batch)\n",
    "    return batch\n",
    "\n",
    "mixed_batch = sample_mixed_batch(texts_new_domain, replay_buffer, replay_ratio=0.5, batch_size=4)\n",
    "print(\"Ejemplo de batch mixto (nuevo + replay):\")\n",
    "for t in mixed_batch:\n",
    "    print(\"-\", t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb193c29",
   "metadata": {},
   "source": [
    "#### **Expansión de parámetros**\n",
    "\n",
    "En lugar de solo ajustar los parámetros existentes, podemos **expandir el modelo**:\n",
    "\n",
    "- Añadir capas nuevas.  \n",
    "- Aumentar el tamaño de algunas capas.  \n",
    "- Introducir módulos adicionales (por ejemplo, adapters, MLPs).  \n",
    "\n",
    "Ejemplo sencillo: un wrapper que añade una pequeña MLP encima de las salidas del modelo base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf53e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedLM(nn.Module):\n",
    "    # Envuelve un modelo base de lenguaje y añade una capa MLP adicional\n",
    "    # sobre las representaciones internas antes de predecir logits.\n",
    "    def __init__(self, base_model, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        d_model = base_model.config.n_embd\n",
    "        self.extra_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.base_model.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state  # (batch, seq, d_model)\n",
    "        hidden_expanded = hidden_states + self.extra_mlp(hidden_states)\n",
    "        logits = self.base_model.lm_head(hidden_expanded)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "            )\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "expanded_model = ExpandedLM(modelo)\n",
    "print(expanded_model.__class__.__name__, \"creado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b526b",
   "metadata": {},
   "source": [
    "#### **Ajuste fino eficiente en parámetro (PEFT)**\n",
    "\n",
    "**Objetivo:** adaptar un modelo grande entrenando solo una **pequeña fracción** de parámetros.\n",
    "\n",
    "Ejemplos de PEFT:\n",
    "\n",
    "- Adapters  \n",
    "- LoRA (Low-Rank Adaptation)  \n",
    "- Prefix tuning, prompt tuning, etc.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Menor costo de entrenamiento.  \n",
    "- Menor almacenamiento (solo guardas los deltas o módulos adicionales).  \n",
    "- Posibilidad de tener muchos adaptadores para distintos tasks/dominos sobre un mismo modelo base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56169760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],  # módulos objetivo en GPT-2 pequeño\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model_peft = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model_peft = get_peft_model(model_peft, lora_config)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_peft.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_peft.parameters())\n",
    "\n",
    "print(f\"Parámetros totales: {total_params}\")\n",
    "print(f\"Parámetros entrenables (LoRA): {trainable_params}\")\n",
    "print(f\"Porcentaje entrenable: {100 * trainable_params / total_params:.4f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010f46b",
   "metadata": {},
   "source": [
    "#### **Adición de nuevos parámetros**\n",
    "\n",
    "A veces añadimos parámetros específicos sin tocar el resto:\n",
    "\n",
    "- **Nuevos tokens** en el vocabulario (por ejemplo, tokens especiales para un dominio).  \n",
    "- Nuevos *heads* de clasificación encima de un encoder.  \n",
    "\n",
    "Ejemplo: añadir un token especial `<DOMAIN>` y redimensionar las embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_add = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_add = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Vocab size original:\", len(tokenizer_add))\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<DOMAIN>\"]}\n",
    "num_added = tokenizer_add.add_special_tokens(special_tokens)\n",
    "print(\"Tokens añadidos:\", num_added)\n",
    "\n",
    "model_add.resize_token_embeddings(len(tokenizer_add))\n",
    "print(\"Nuevo vocab size:\", model_add.get_input_embeddings().weight.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d87ab",
   "metadata": {},
   "source": [
    "#### **Métodos de subconjuntos**\n",
    "\n",
    "En muchos escenarios no puedes usar **todo** el dataset para fine-tuning:\n",
    "\n",
    "- Coste computacional.  \n",
    "- Datos redundantes.  \n",
    "\n",
    "Estos métodos, seleccionan un subconjunto representativo o informativo:\n",
    "\n",
    "- Muestreo aleatorio estratificado.  \n",
    "- Selección por diversidad.  \n",
    "- Selección basada en importancia de ejemplo (más avanzado).\n",
    "\n",
    "Ejemplo: seleccionar textos de longitud media (ni muy cortos ni muy largos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = texts_base + texts_new_domain + [\n",
    "    \"Short.\",\n",
    "    \"This is a much longer sentence that might not be ideal for quick training loops in a demo notebook.\",\n",
    "]\n",
    "\n",
    "def select_by_length(texts, min_len=10, max_len=80):\n",
    "    selected = []\n",
    "    for t in texts:\n",
    "        n = len(t)\n",
    "        if min_len <= n <= max_len:\n",
    "            selected.append((t, n))\n",
    "    return selected\n",
    "\n",
    "subset = select_by_length(all_texts, min_len=20, max_len=100)\n",
    "print(\"Textos seleccionados por longitud (min=20, max=100):\")\n",
    "for t, n in subset:\n",
    "    print(f\"- ({n} chars) {t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd9f91",
   "metadata": {},
   "source": [
    "### **Combinación de múltiples modelos**\n",
    "\n",
    "En lugar de depender de un solo modelo, podemos combinar varios:\n",
    "\n",
    "- **Ensamble de modelos**: promediar predicciones.  \n",
    "- **Fusión de modelos**: combinar pesos.  \n",
    "- **Fusión de adaptadores**: fusionar adaptadores (por ejemplo, LoRA) entrenados para distintos tasks o dominios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430cce6",
   "metadata": {},
   "source": [
    "#### **Ensamblado de modelos (promedio de logits)**\n",
    "\n",
    "Imagina que tienes dos modelos (quizás entrenados con semillas distintas o datasets ligeramente diferentes).  \n",
    "Puedes promediar sus logits antes del softmax para obtener una predicción más robusta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model_ens_1 = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model_ens_2 = copy.deepcopy(model_ens_1)\n",
    "\n",
    "def ensemble_generate(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attn_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out1 = model_ens_1(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        out2 = model_ens_2(input_ids=input_ids, attention_mask=attn_mask)\n",
    "\n",
    "    logits1 = out1.logits[:, -1, :]\n",
    "    logits2 = out2.logits[:, -1, :]\n",
    "    avg_logits = (logits1 + logits2) / 2.0\n",
    "\n",
    "    probs = torch.softmax(avg_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    new_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    text = tokenizer.decode(new_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "print(ensemble_generate(\"The transformer model is\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcbd7d-661d-45df-9f2a-af161aa52851",
   "metadata": {},
   "source": [
    "**Versión ampliada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b99c94-f619-4ed6-9cdb-e3e5593d6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Por si el modelo no tiene pad_token definido\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargamos dos instancias separadas (sin deepcopy)\n",
    "model_ens_1 = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model_ens_2 = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "model_ens_1.eval()\n",
    "model_ens_2.eval()\n",
    "\n",
    "def ensemble_generate(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 30,\n",
    "    temperature: float = 0.8,\n",
    "    break_on_eos: bool = True,\n",
    ") -> str:\n",
    "    # Tokenizamos el prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attn_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generamos token a token\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            out1 = model_ens_1(input_ids=input_ids, attention_mask=attn_mask)\n",
    "            out2 = model_ens_2(input_ids=input_ids, attention_mask=attn_mask)\n",
    "\n",
    "        # Tomamos los logits del último paso de cada modelo\n",
    "        logits1 = out1.logits[:, -1, :]\n",
    "        logits2 = out2.logits[:, -1, :]\n",
    "\n",
    "        # Ensamble simple: promedio de logits\n",
    "        avg_logits = (logits1 + logits2) / 2.0\n",
    "\n",
    "        # Opción: escalamos por temperatura\n",
    "        avg_logits = avg_logits / temperature\n",
    "\n",
    "        # Convertimos a probabilidades y muestreamos el siguiente token\n",
    "        probs = torch.softmax(avg_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Si queremos cortar en EOS\n",
    "        if break_on_eos and next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Añadimos el nuevo token a la secuencia\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        # Actualizamos la máscara de atención (todo son tokens válidos)\n",
    "        attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Decodificamos todo (prompt + continuación)\n",
    "    text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Ejemplo\n",
    "print(ensemble_generate(\"The transformer model is\", max_new_tokens=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47157e",
   "metadata": {},
   "source": [
    "####  **Fusión de modelos (promedio de pesos)**\n",
    "\n",
    "Otra idea: combinar directamente los **pesos** de dos modelos en un modelo fusionado:\n",
    "\n",
    "`model_fused = alpha * model_A + (1 - alpha) * model_B`\n",
    "\n",
    "En la práctica, puede hacerse con distintos pesos o solo en algunas capas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_models(model_a, model_b, alpha=0.5):\n",
    "    fused = copy.deepcopy(model_a)\n",
    "    state_a = model_a.state_dict()\n",
    "    state_b = model_b.state_dict()\n",
    "    fused_state = fused.state_dict()\n",
    "\n",
    "    for k in fused_state.keys():\n",
    "        fused_state[k] = alpha * state_a[k] + (1 - alpha) * state_b[k]\n",
    "    fused.load_state_dict(fused_state)\n",
    "    return fused\n",
    "\n",
    "model_fused = fuse_models(model_ens_1, model_ens_2, alpha=0.5)\n",
    "print(\"Modelo fusionado creado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcfe21-71d2-45aa-bda1-fe9fd1956b66",
   "metadata": {},
   "source": [
    "**Versión ampliada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe66336-234f-4591-9b8a-fa542155c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Configuración y carga del modelo base\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", device)\n",
    "\n",
    "# Modelo pequeño basado en GPT-2 (en inglés, pero lo usamos como ejemplo didáctico)\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Nos aseguramos de tener un token de padding definido\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargamos dos instancias independientes del mismo modelo para el ensamble\n",
    "model_ens_1 = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model_ens_2 = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "model_ens_1.eval()\n",
    "model_ens_2.eval()\n",
    "\n",
    "# Fijar la semilla para tener resultados más reproducibles\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Función auxiliar: generación autoregresiva con un solo modelo\n",
    "\n",
    "def generate_with_model(\n",
    "    modelo,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 30,\n",
    "    temperature: float = 0.8,\n",
    "    break_on_eos: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generación autoregresiva usando un solo modelo.\n",
    "    - model: instancia de AutoModelForCausalLM.\n",
    "    - prompt: texto inicial.\n",
    "    - max_new_tokens: número máximo de tokens nuevos a generar.\n",
    "    - temperature: controla la aleatoriedad (valores más bajos → más determinista).\n",
    "    - break_on_eos: si es True, se detiene al generar el token EOS.\n",
    "    \"\"\"\n",
    "    modelo.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attn_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = modelo(input_ids=input_ids, attention_mask=attn_mask)\n",
    "\n",
    "        # Tomamos los logits del último paso de la secuencia\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Escalamos por temperatura para controlar la aleatoriedad\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Convertimos a probabilidades y muestreamos el siguiente token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Si queremos cortar cuando aparezca el token de fin de secuencia (EOS)\n",
    "        if break_on_eos and next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Añadimos el nuevo token generado a la secuencia\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        # Actualizamos la máscara de atención (todos los tokens son válidos)\n",
    "        attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Decodificamos toda la secuencia (prompt + continuación)\n",
    "    text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Ensamble de modelos: promedio de logits en cada paso\n",
    "\n",
    "\n",
    "def ensemble_generate(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 30,\n",
    "    temperature: float = 0.8,\n",
    "    break_on_eos: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generación autoregresiva usando un ensamble simple de dos modelos:\n",
    "    en cada paso, promediamos los logits de model_ens_1 y model_ens_2.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attn_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            out1 = model_ens_1(input_ids=input_ids, attention_mask=attn_mask)\n",
    "            out2 = model_ens_2(input_ids=input_ids, attention_mask=attn_mask)\n",
    "\n",
    "        # Logits del último token para cada modelo\n",
    "        logits1 = out1.logits[:, -1, :]\n",
    "        logits2 = out2.logits[:, -1, :]\n",
    "\n",
    "        # Ensamble: promedio de logits\n",
    "        avg_logits = (logits1 + logits2) / 2.0\n",
    "\n",
    "        # Escalamos por temperatura\n",
    "        avg_logits = avg_logits / temperature\n",
    "\n",
    "        # Muestreo del siguiente token a partir de las probabilidades\n",
    "        probs = torch.softmax(avg_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Condición de parada si aparece EOS\n",
    "        if break_on_eos and next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Construimos la nueva secuencia\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Fusión de modelos: interpolación lineal de pesos\n",
    "\n",
    "def fuse_models(model_a, model_b, alpha: float = 0.5, device=None):\n",
    "    \"\"\"\n",
    "    Fusión lineal de dos modelos con la misma arquitectura:\n",
    "        fused_weights = alpha * weights_a + (1 - alpha) * weights_b\n",
    "\n",
    "    - model_a, model_b: deben tener la misma arquitectura y las mismas claves en state_dict().\n",
    "    - alpha: controla el peso relativo de cada modelo en la mezcla.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model_a.parameters()).device\n",
    "\n",
    "    # Creamos un nuevo modelo a partir de la misma configuración (sin usar deepcopy)\n",
    "    fused_model = model_a.__class__(model_a.config).to(device)\n",
    "\n",
    "    state_a = model_a.state_dict()\n",
    "    state_b = model_b.state_dict()\n",
    "\n",
    "    fused_state = {}\n",
    "\n",
    "    # Interpolamos todos los parámetros\n",
    "    for k in state_a.keys():\n",
    "        fused_state[k] = alpha * state_a[k] + (1.0 - alpha) * state_b[k]\n",
    "\n",
    "    fused_model.load_state_dict(fused_state)\n",
    "    fused_model.eval()\n",
    "    return fused_model\n",
    "\n",
    "# Creamos el modelo fusionado a partir de los dos modelos del ensamble\n",
    "model_fused = fuse_models(model_ens_1, model_ens_2, alpha=0.5, device=device)\n",
    "print(\"Modelo fusionado creado correctamente.\")\n",
    "\n",
    "# Pequeña demostración\n",
    "\n",
    "prompt_1 = \"Un modelo Transformer es\"\n",
    "prompt_2 = \"Un modelo de lenguaje grande es\"\n",
    "\n",
    "print(\"\\n Modelo individual (model_ens_1)\")\n",
    "print(generate_with_model(model_ens_1, prompt_1, max_new_tokens=30))\n",
    "\n",
    "print(\"\\n Ensamble de dos modelos (promedio de logits)\")\n",
    "print(ensemble_generate(prompt_1, max_new_tokens=30))\n",
    "\n",
    "print(\"\\n Modelo fusionado (interpolación de pesos)\")\n",
    "print(generate_with_model(model_fused, prompt_1, max_new_tokens=30))\n",
    "\n",
    "print(\"\\n Modelo fusionado con otro prompt en español\")\n",
    "print(generate_with_model(model_fused, prompt_2, max_new_tokens=40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bdb2a",
   "metadata": {},
   "source": [
    "#### **Fusión de adaptadores (LoRA / Adapters)**\n",
    "\n",
    "Con PEFT, es común tener múltiples adaptadores:\n",
    "\n",
    "- Un adaptador para estilo A.  \n",
    "- Otro adaptador para dominio B.  \n",
    "\n",
    "La **fusión de adaptadores** busca combinar estos adaptadores en un solo conjunto de parámetros.\n",
    "\n",
    "Flujo conceptual típico:\n",
    "\n",
    "1. Tienes un modelo base.  \n",
    "2. Entrenas adaptador A -> guardas adapter A.  \n",
    "3. Entrenas adaptador B -> guardas adapter B.  \n",
    "4. Cargas ambos adaptadores y combinas sus deltas (por ejemplo, promedio) o usas funciones de merge de la librería.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_for_adapters = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Esqueleto conceptual (no entrenamos adaptadores distintos aquí):\n",
    "# peft_model_a = PeftModel.from_pretrained(base_for_adapters, \"path/to/adapter_A\")\n",
    "# peft_model_b = PeftModel.from_pretrained(base_for_adapters, \"path/to/adapter_B\")\n",
    "\n",
    "print(\"Adapter merging: flujo conceptual descrito en la celda anterior.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d79f8-04d2-4824-a69c-90d701730679",
   "metadata": {},
   "source": [
    "**Versión ampliada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde93495-75da-4fb4-a8b3-28ee6daa886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Crear adaptadores LoRA \"dummy\" y guardarlos en disco\n",
    "\n",
    "# Modelo base sobre el que se montarán los adaptadores\n",
    "base_for_adapters = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Por si hace falta ajustar el tamaño del embedding a las dimensiones del tokenizer\n",
    "base_for_adapters.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def create_dummy_lora_adapter(\n",
    "    base_model,\n",
    "    adapter_dir: str,\n",
    "    r: int = 8,\n",
    "    alpha: int = 16,\n",
    "    scaling_factor: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea un adaptador LoRA \"de juguete\" para el modelo base y lo guarda en disco.\n",
    "    No entrenamos realmente; sólo escalamos los pesos LoRA para que A y B sean distintos.\n",
    "\n",
    "    - base_model: modelo base (sin PEFT).\n",
    "    - adapter_dir: carpeta donde se guardará el adaptador.\n",
    "    - r, alpha: hiperparámetros estándar de LoRA.\n",
    "    - scaling_factor: factor para escalar los pesos LoRA, creando adaptadores diferentes.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(adapter_dir):\n",
    "        os.makedirs(adapter_dir, exist_ok=True)\n",
    "\n",
    "    # Configuración LoRA básica para atención causal\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"c_attn\"],  # típico en GPT-2/DistilGPT2 (puede variar según arquitectura)\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # Creamos un modelo con LoRA \"encima\" del base_model\n",
    "    peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    # Escalamos los pesos LoRA para tener adaptadores diferentes sin entrenamiento real\n",
    "    with torch.no_grad():\n",
    "        for name, param in peft_model.named_parameters():\n",
    "            # \"lora_\" suele aparecer en los parámetros de los módulos LoRA\n",
    "            if \"lora_\" in name and param.requires_grad:\n",
    "                param.mul_(scaling_factor)\n",
    "\n",
    "    # Guardamos solo el adaptador (no el modelo completo)\n",
    "    peft_model.save_pretrained(adapter_dir)\n",
    "    print(f\"Adaptador LoRA guardado en: {adapter_dir}\")\n",
    "\n",
    "\n",
    "# Creamos dos adaptadores distintos a partir del mismo modelo base\n",
    "create_dummy_lora_adapter(base_for_adapters, \"adapter_A\", scaling_factor=1.0)\n",
    "create_dummy_lora_adapter(base_for_adapters, \"adapter_B\", scaling_factor=1.5)\n",
    "\n",
    "\n",
    "# 2. Cargar los adaptadores A y B sobre nuevos modelos base\n",
    "\n",
    "# Volvemos a cargar el modelo base para no reutilizar el que modificamos antes\n",
    "base_for_adapters_A = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "base_for_adapters_B = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "peft_model_a = PeftModel.from_pretrained(base_for_adapters_A, \"adapter_A\")\n",
    "peft_model_b = PeftModel.from_pretrained(base_for_adapters_B, \"adapter_B\")\n",
    "\n",
    "print(\"Adaptadores A y B cargados correctamente.\")\n",
    "\n",
    "# 3. Función para fusionar (merge) adaptadores LoRA\n",
    "\n",
    "def merge_lora_adapters(\n",
    "    model_a: PeftModel,\n",
    "    model_b: PeftModel,\n",
    "    alpha: float = 0.5,\n",
    "    device=None,\n",
    ") -> PeftModel:\n",
    "    \"\"\"\n",
    "    Fusión de dos adaptadores LoRA:\n",
    "        LoRA_fusionado = alpha * LoRA_A + (1 - alpha) * LoRA_B\n",
    "\n",
    "    Asume:\n",
    "    - model_a y model_b son PeftModel con la misma configuración LoRA.\n",
    "    - Se fusionan sólo los pesos de los módulos LoRA (no el modelo base completo).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model_a.parameters()).device\n",
    "\n",
    "    # Tomamos la configuración LoRA del primer modelo\n",
    "    peft_config = model_a.peft_config[\"default\"]\n",
    "\n",
    "    # Creamos un nuevo modelo base limpio\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "    # Construimos un nuevo PeftModel con la misma config LoRA\n",
    "    fused_model = PeftModel(base_model, peft_config)\n",
    "\n",
    "    # Convertimos parámetros a diccionarios para acceder por nombre\n",
    "    params_a = dict(model_a.named_parameters())\n",
    "    params_b = dict(model_b.named_parameters())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, param_fused in fused_model.named_parameters():\n",
    "            # Sólo fusionamos los parámetros LoRA; el resto se deja como en el base_model\n",
    "            if \"lora_\" in name and name in params_a and name in params_b:\n",
    "                w_a = params_a[name].data\n",
    "                w_b = params_b[name].data\n",
    "                param_fused.copy_(alpha * w_a + (1.0 - alpha) * w_b)\n",
    "\n",
    "    fused_model.eval()\n",
    "    return fused_model\n",
    "\n",
    "\n",
    "adapter_fused_model = merge_lora_adapters(peft_model_a, peft_model_b, alpha=0.5, device=device)\n",
    "print(\"Adapter fusionado creado correctamente (adapter merging).\")\n",
    "\n",
    "# 4. Demostración: generación con el modelo base + adaptador fusionado\n",
    "\n",
    "def generate_with_peft_model(\n",
    "    modelo: PeftModel,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 30,\n",
    "    temperature: float = 0.8,\n",
    "    break_on_eos: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generación autoregresiva usando un modelo con adaptadores PEFT (LoRA).\n",
    "    \"\"\"\n",
    "    modelo.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attn_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = modelo(input_ids=input_ids, attention_mask=attn_mask)\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        logits = logits / temperature\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if break_on_eos and next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "prompt = \"En pocas palabras, un modelo Transformer adaptado con LoRA es\"\n",
    "\n",
    "print(\"\\nGeneración con adaptador fusionado (adapter merging)\")\n",
    "print(generate_with_peft_model(adapter_fused_model, prompt, max_new_tokens=40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ac6fd",
   "metadata": {},
   "source": [
    "#### **Ejercicios prácticos**\n",
    "\n",
    "Los siguientes ejercicios están pensados para que puedas **experimentar** con las técnicas del cuaderno en un entorno controlado (modelo tiny, pocos pasos de entrenamiento).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ee95c",
   "metadata": {},
   "source": [
    "**Ejercicio 1 - Pre-entrenamiento continuo vs modelo base**\n",
    "\n",
    "**Objetivo:** comparar el efecto de un pequeño pre-entrenamiento continuo sobre un dominio específico.\n",
    "\n",
    "1. Copia el modelo base en dos instancias:\n",
    "   - `model_base_eval`: sin entrenamiento adicional.  \n",
    "   - `model_ct_eval`: al que aplicarás algunos pasos del pre-entrenamiento continuo sobre `texts_new_domain`.\n",
    "\n",
    "2. Para cada modelo, genera texto a partir de prompts como:\n",
    "   - `\"Reinforcement learning with human\"`  \n",
    "   - `\"Parameter-efficient fine-tuning\"`  \n",
    "   - `\"Continual pre-training\"`  \n",
    "\n",
    "3. Compara las salidas **antes** y **después** del entrenamiento:\n",
    "   - ¿Notas algún cambio en el tipo de completaciones?  \n",
    "   - ¿El modelo adaptado parece \"más técnico\" o más alineado con el dominio?  \n",
    "\n",
    "4. (Opcional) Define una pequeña métrica de evaluación automática (por ejemplo, contar cuántas palabras clave del dominio aparecen en las generaciones).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar Ejercicio 1\n",
    "\n",
    "# 1) Crear copias del modelo\n",
    "#    Sugerencia:\n",
    "#    - Usa AutoModelForCausalLM.from_pretrained(MODEL_NAME) para model_base_eval.\n",
    "#    - Duplica para model_ct_eval y aplica algunos pasos de entrenamiento sobre texts_new_domain.\n",
    "\n",
    "# 2) Definir prompts de prueba\n",
    "prompts_domain = [\n",
    "    \"Reinforcement learning with human\",\n",
    "    \"Parameter-efficient fine-tuning\",\n",
    "    \"Continual pre-training\",\n",
    "]\n",
    "\n",
    "# 3) Escribir funciones helper para generar texto con cada modelo y comparar.\n",
    "#    Ejemplo de estructura:\n",
    "\n",
    "def generate_with_model(modelo, prompt, max_new_tokens=30):\n",
    "    modelo.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 4) COMPLETAR:\n",
    "# - Entrenar model_ct_eval algunos pasos sobre texts_new_domain.\n",
    "# - Generar salidas con ambos modelos y analizarlas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7206a",
   "metadata": {},
   "source": [
    "**Ejercicio 2 - Efecto de mezclar ejemplos de memoria**\n",
    "\n",
    "**Objetivo:** ver el efecto de mezclar ejemplos de memoria en el entrenamiento del pre-entrenamiento continuo\n",
    "\n",
    "1. Crea dos copias del modelo:\n",
    "   - `model_no_replay`  \n",
    "   - `model_with_replay`  \n",
    "\n",
    "2. Entrena ambos modelos durante el mismo número de pasos:\n",
    "   - `model_no_replay`: entrena solo con ejemplos de `texts_new_domain`.  \n",
    "   - `model_with_replay`: en cada paso usa `sample_mixed_batch(...)` para mezclar datos nuevos + `replay_buffer`.\n",
    "\n",
    "3. Define un pequeño conjunto de evaluación para el dominio base, por ejemplo `texts_base_eval`\n",
    "   (puede ser igual a `texts_base` u otras frases similares).\n",
    "\n",
    "4. Tras el entrenamiento:\n",
    "   - Calcula la *loss* de LM en `texts_base_eval` para ambos modelos.  \n",
    "   - Compara: ¿cuál modelo olvidó más el dominio original?  \n",
    "   - (Opcional) Haz lo mismo para una evaluación de dominio nuevo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ea677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar Ejercicio 2\n",
    "\n",
    "# 1) Crear model_no_replay y model_with_replay desde el modelo base\n",
    "#    Sugerencia: usa AutoModelForCausalLM.from_pretrained(MODEL_NAME) dos veces.\n",
    "\n",
    "# 2) Definir textos de evaluación para el dominio base\n",
    "texts_base_eval = texts_base  # puedes extender esta lista si quieres\n",
    "\n",
    "def lm_loss_on_texts(modelo, texts):\n",
    "    modelo.eval()\n",
    "    input_ids, attn_mask, labels = build_lm_batch(texts)\n",
    "    with torch.no_grad():\n",
    "        loss = modelo(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "    return float(loss.item())\n",
    "\n",
    "# 3) COMPLETAR:\n",
    "# - Entrenar model_no_replay con solo texts_new_domain.\n",
    "# - Entrenar model_with_replay con batches mezclados (sample_mixed_batch).\n",
    "# - Comparar lm_loss_on_texts(model_no_replay, texts_base_eval)\n",
    "#   vs lm_loss_on_texts(model_with_replay, texts_base_eval).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7812f6",
   "metadata": {},
   "source": [
    "**Ejercicio 3 - PEFT (LoRA) vs fine-tuning completo**\n",
    "\n",
    "**Objetivo:** comparar full fine-tuning vs LoRA en términos de:\n",
    "\n",
    "- número de parámetros entrenables,  \n",
    "- tiempo aproximado por paso (en tu máquina),\n",
    "- pérdida en un mini-dataset sintético.\n",
    "\n",
    "Pasos sugeridos:\n",
    "\n",
    "1. Define un mini-dataset `texts_task` con frases simples sobre un tema (por ejemplo, `\"security in devops\"`, `\"kubernetes policies\"`, etc.).  \n",
    "2. Crea dos modelos:\n",
    "   - `model_full`: todos los parámetros entrenables.  \n",
    "   - `model_lora`: usando `get_peft_model` (ya vimos cómo crear `model_peft`).  \n",
    "3. Entrena ambos modelos durante el mismo número de pasos sobre `texts_task` (muy pocos pasos para que sea rápido).  \n",
    "4. Calcula la pérdida final en `texts_task` para cada uno y compara tiempos y resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar Ejercicio 3\n",
    "\n",
    "# 1) Definir un mini-dataset de tarea\n",
    "texts_task = [\n",
    "    \"DevSecOps integrates security into the DevOps lifecycle.\",\n",
    "    \"Kubernetes NetworkPolicies control traffic between pods.\",\n",
    "    \"GitHub Actions can implement CI/CD pipelines.\",\n",
    "]\n",
    "\n",
    "# 2) Crear modelo full fine-tuning (model_full) y modelo LoRA (model_lora)\n",
    "#    Sugerencia:\n",
    "#    - model_full = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "#    - model_lora = get_peft_model(...)\n",
    "\n",
    "# 3) Entrenar unos pocos pasos en texts_task para ambos modelos.\n",
    "# 4) Comparar lm_loss_on_texts(model_full, texts_task) vs lm_loss_on_texts(model_lora, texts_task).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72035ddc",
   "metadata": {},
   "source": [
    "**Ejercicio 4 - Métodos de subconjuntos: selección de datos para fine-tuning**\n",
    "\n",
    "**Objetivo:** diseñar una política simple de selección de subconjunto de datos y analizar su impacto potencial.\n",
    "\n",
    "1. Construye una lista más grande de textos combinando:\n",
    "   - `texts_base`  \n",
    "   - `texts_new_domain`  \n",
    "   - Frases adicionales de longitud y contenido variado (puedes generarlas a mano).  \n",
    "\n",
    "2. Implementa al menos **dos estrategias de selección**:\n",
    "   - Por longitud (como el ejemplo `select_by_length`).  \n",
    "   - Por presencia de ciertas palabras clave (por ejemplo, `\"Transformer\"`, `\"fine-tuning\"`, `\"DevOps\"`).  \n",
    "\n",
    "3. Para cada subconjunto seleccionado:\n",
    "   - Calcula cuántos ejemplos contiene.  \n",
    "   - Muestra algunas frases de ejemplo.  \n",
    "\n",
    "4. Discute (en texto, no hace falta código):\n",
    "   - ¿Qué subconjunto crees que sería mejor para un fine-tuning orientado a “LLMs+DevOps”?  \n",
    "   - ¿Qué riesgos ves si seleccionas datos solo por palabra clave?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62159177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar Ejercicio 4\n",
    "\n",
    "# 1) Construir una lista de textos más grande\n",
    "extra_texts = [\n",
    "    \"Docker containers isolate applications and their dependencies.\",\n",
    "    \"Infrastructure as Code tools like Terraform manage cloud resources.\",\n",
    "    \"Prompt engineering is important for large language models.\",\n",
    "    \"Security policies should be automated whenever possible.\",\n",
    "]\n",
    "all_texts_extended = texts_base + texts_new_domain + extra_texts\n",
    "\n",
    "# 2) Estrategia 1: por longitud (puedes reutilizar select_by_length)\n",
    "# 3) Estrategia 2: por palabra clave\n",
    "keywords = [\"Transformer\", \"fine-tuning\", \"DevOps\", \"security\", \"Docker\"]\n",
    "\n",
    "def select_by_keywords(texts, keywords):\n",
    "    selected = []\n",
    "    for t in texts:\n",
    "        if any(kw.lower() in t.lower() for kw in keywords):\n",
    "            selected.append(t)\n",
    "    return selected\n",
    "\n",
    "subset_length = select_by_length(all_texts_extended, min_len=20, max_len=120)\n",
    "subset_keywords = select_by_keywords(all_texts_extended, keywords)\n",
    "\n",
    "print(\"Subconjunto por longitud:\")\n",
    "for t, n in subset_length:\n",
    "    print(f\"- ({n} chars) {t}\")\n",
    "\n",
    "print(\"\\nSubconjunto por keywords:\")\n",
    "for t in subset_keywords:\n",
    "    print(\"-\", t)\n",
    "\n",
    "# 4) Añadir tus propias reflexiones en una celda Markdown aparte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb82049",
   "metadata": {},
   "source": [
    "**Ejercicio 5 - Ensamblado de modelos y fusión de modelos\n",
    "\n",
    "**Objetivo:** ver cómo cambian las predicciones al combinar modelos.\n",
    "\n",
    "1. Usa `model_ens_1` y `model_ens_2` para generar texto con:\n",
    "   - Modelo individual (`simple_generate` con cada modelo).  \n",
    "   - Ensemble (`ensemble_generate`).  \n",
    "\n",
    "2. Compara manualmente:\n",
    "   - ¿Cambian las palabras más probables?  \n",
    "   - ¿Alguna generación te parece \"más estable\" o \"más neutra\"?  \n",
    "\n",
    "3. Usa el `model_fused` para generar texto desde los mismos prompts.  \n",
    "4. (Opcional) Define una mini-métrica simple (por ejemplo, cuántos tokens coinciden entre distintas ejecuciones) para cuantificar \"estabilidad\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6eb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar Ejercicio 5\n",
    "\n",
    "prompts_test = [\n",
    "    \"The transformer model is\",\n",
    "    \"Fine-tuning allows a model to\",\n",
    "]\n",
    "\n",
    "# 1) Generar texto con model_ens_1 y model_ens_2 por separado\n",
    "#    Sugerencia: reutiliza simple_generate pero pasando explícitamente el modelo que quieres usar\n",
    "def generate_with_specific_model(modelo, prompt, max_new_tokens=20):\n",
    "    modelo.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = modelo.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 2) COMPLETAR: para cada prompt en prompts_test,\n",
    "#    - imprimir salida con model_ens_1\n",
    "#    - imprimir salida con model_ens_2\n",
    "#    - imprimir salida con ensemble_generate\n",
    "#    - imprimir salida con model_fused\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
